Abstract,Response
"Lifelong learning represents an emerging machine learning paradigm that aims at designing new methods providing accurate analyses in complex and dynamic real-world environments. Although a significant amount of research has been conducted in image classification and reinforcement learning, very limited work has been done to solve lifelong anomaly detection problems. In this context, a successful method has to detect anomalies while adapting to changing environments and preserving knowledge to avoid catastrophic forgetting. While state-of-the-art online anomaly detection methods are able to detect anomalies and adapt to a changing environment, they are not designed to preserve past knowledge. On the other hand, while lifelong learning methods are focused on adapting to changing environments and preserving knowledge, they are not tailored for detecting anomalies, and often require task labels or task boundaries which are not available in task-agnostic lifelong anomaly detection scenarios. This paper proposes VLAD, a novel VAE-based Lifelong Anomaly Detection method addressing all these challenges simultaneously in complex task-agnostic scenarios. VLAD leverages the combination of lifelong change point detection and an effective model update strategy supported by experience replay with a hierarchical memory maintained by means of consolidation and summarization. An extensive quantitative evaluation showcases the merit of the proposed method in a variety of applied settings. VLAD outperforms state-of-the-art methods for anomaly detection, presenting increased robustness and performance in complex lifelong settings.
Elsevier",High
"Effective anomaly detection in multivariate time series data is critical to ensuring the security of Internet of Things (IoT) devices and systems. However, building a high precision and low false positive rate anomaly detection model for the complex and volatile IoT environment is a challenging task. This is often due to issues such as a lack of anomaly labeling, high data volatility, and the complexity of device mechanisms. Traditional machine learning algorithms and sequence models frequently fail to account for feature correlation and temporal dependency in anomaly detection. Although deep learning-based anomaly detection methods have progressed, there is still room for improvement in precision, recall, and generalization ability. In this paper, we propose an anomaly detection model called Meta-MWDG to address these issues. The model is based on a multi-scale discrete wavelet decomposition and a dual graph attention network, which can effectively extract feature correlation and temporal dependency in multivariate time series data. Additionally, model-agnostic meta-learning (MAML) is introduced to improve the modelâ€™s generalization performance, enabling it to perform well on new tasks even with a few samples. A gated recurrent unit (GRU) is combined with a multi-head self-attention network to output both prediction and reconstruction results in a joint optimization strategy, improving the precision of anomaly detection. Extensive experimental studies demonstrate that Meta-MWDG outperforms the state-of-the-art methods in anomaly detection.
Elsevier",Medium
"As reducing carbon emissions can relieve environmental concerns, networks-supported renewable power plants are being built more and more. Inevitable network attacks have become a serious threat in increasing and distributed power plants. Leveraging federated learning for training the joint model to detect network attacks in distributed power plants is efficient, but two malicious behaviors of cheating and free-riding are unavoidable. To this end, we design a new SDN based federated security architecture and propose a carbon-credit-rewarded consensus verification mechanism in this architecture to deal with malicious behaviors. For this architecture, on the one hand, considering geographical conditions of renewable power plants, multi-controller SDN is adopted in network to solve some security problems at root and to avoid single point of failure. On the other hand, the segmentation of collaborative zones reduces communication cost effectively. The proposed mechanism establishes consensus bearer and realizes the election of consensus bearer by cross-validation of client detection models. Only the excellent models are aggregated to mitigate cheating of malicious clients. Carbon emissions credit is introduced as an incentive in the proposed mechanism. The redistribution of carbon emissions credit improves the performance of global detection model and avoids free-riding. Moreover, the economic nature of carbon emissions credit enhances the spillover effect of carbon emissions trading market on the reduction of carbon emissions. The experimental results revealed that the proposed architecture has excellent performance, and can handle malicious behaviors effectively.
Elsevier",Low
"Host-based Intrusion Detection System (HIDS) is an effective last line of defense for defending against cyber security attacks after perimeter defenses (e.g., Network-based Intrusion Detection System and Firewall) have failed or been bypassed. HIDS is widely adopted in the industry as HIDS is ranked among the top two most used security tools by Security Operation Centers (SOC) of organizations. Although effective and efficient HIDS is highly desirable for industrial organizations, the evolution of increasingly complex attack patterns causes several challenges resulting in performance degradation of HIDS (e.g., high false alert rate creating alert fatigue for SOC staff). Since Natural Language Processing (NLP) methods are better suited for identifying complex attack patterns, an increasing number of HIDS are leveraging the advances in NLP that have shown effective and efficient performance in precisely detecting low footprint, zero-day attacks and predicting an attackerâ€™s next steps. This active research trend of using NLP in HIDS demands a synthesized and comprehensive body of knowledge of NLP-based HIDS. Despite the drastically growing adoption of NLP in HIDS development, there has been relatively little effort allocated to systematically analyze and synthesize the available peer review literature to understand how NLP is used in HIDS development. The lack of a synthesized and comprehensive body of knowledge on such an important topic motivated us to conduct a Systematic Literature Review (SLR) of the papers on the end-to-end pipeline of the use of NLP in HIDS development. For the end-to-end NLP-based HIDS development pipeline, we identify, taxonomically categorize and systematically compare the state-of-the-art of NLP methods usage in HIDS, attacks detected by these NLP methods, datasets and evaluation metrics which are used to evaluate the NLP-based HIDS. We highlight the relevant prevalent practices, considerations, advantages and limitations to support the HIDS developers. We also outline the future research directions for the NLP-based HIDS development.
Elsevier",Low
"AIoT applications often encounter challenges such as terminal resource constraints, data drift, and data heterogeneity in real world, leading to problems such as catastrophic forgetting, low generalization ability, and low accuracy during model training. To address these challenges, we proposed CoLLaRS, a cloudâ€“edgeâ€“terminal collaborative lifelong learning framework for AIoT applications. In the CoLLaRS framework, we alleviate the problem of terminal resource constraints by uploading terminal tasks at the edge. CoLLaRS uses continuous training at the edge to achieve lifelong learning training of the model and solve the problem of catastrophic forgetting. CoLLaRS employs federated optimization in the cloud to perform personalized aggregation of different edge models and solve the problem of weak model generalization ability. Finally, the model is fine-tuned at the terminal to further optimize its accuracy in local tasks. Our experiments on real-world datasets showed that CoLLaRS has an 8% improvement in accuracy and a 5% improvement in backward transfer(BWT) and forward transfer(FWT) compared to other baseline algorithms. The results of the ablation experiments further confirmed the effectiveness of CoLLaRS.
Elsevier",High
"Recent theoretical and practical studies have revealed that malware is one of the most harmful threats to the digital world. Malware mitigation techniques have evolved over the years to ensure security. Earlier, several classical methods were used for detecting malware embedded with various features like the signature, heuristic, and others. Traditional malware detection techniques were unable to defeat new generations of malware and their sophisticated obfuscation tactics. Deep Learning is increasingly used in malware detection as DL-based systems outperform conventional malware detection approaches at finding new malware variants. Furthermore, DL-based techniques provide rapid malware prediction with excellent detection rates and analysis of different malware types. Investigating recently proposed Deep Learning-based malware detection systems and their evolution is hence of interest to this work. It offers a thorough analysis of the recently developed DL-based malware detection techniques. Furthermore, current trending malwares are studied and detection techniques of Mobile malware (both Android and iOS), Windows malware, IoT malware, Advanced Persistent Threats (APTs), and Ransomware are precisely reviewed.",Low
"The advent of connected devices and omnipresence of Internet have paved way for intruders to attack networks, which leads to cyber-attack, financial loss, information theft in healthcare, and cyber war. Hence, network security analytics has become an important area of concern and has gained intensive attention among researchers, off late, specifically in the domain of anomaly detection in network, which is considered crucial for network security. However, preliminary investigations have revealed that the existing approaches to detect anomalies in network are not effective enough, particularly to detect them in real time. The reason for the inefficacy of current approaches is mainly due the amassment of massive volumes of data though the connected devices. Therefore, it is crucial to propose a framework that effectively handles real time big data processing and detect anomalies in networks. In this regard, this paper attempts to address the issue of detecting anomalies in real time. Respectively, this paper has surveyed the state-of-the-art real-time big data processing technologies related to anomaly detection and the vital characteristics of associated machine learning algorithms. This paper begins with the explanation of essential contexts and taxonomy of real-time big data processing, anomalous detection, and machine learning algorithms, followed by the review of big data processing technologies. Finally, the identified research challenges of real-time big data processing in anomaly detection are discussed.
Elsevier",Low
"Space anomaly detection plays a critical role in safeguarding the integrity and reliability of space systems amid the rising tide of threats. This survey aims to deepen comprehension of space cyber threats through space threat modeling, and meticulously examine the unique challenges of space anomaly detection. The survey identifies scalability, real-time detection, limited labeled data availability, concept drift, and adversarial attacks as key challenges based on thorough literature analysis and synthesis. By extensively exploring state-of-the-art anomaly detection techniques, the study evaluates their applicability, strengths, and limitations within space networks. Going beyond analysis, a notable contribution of this work involves integrating stream-based and graph-based methods, tailored to capture the intricate temporal and structural relationships inherent in space networks. This innovative hybrid approach holds promise for heightened detection accuracy and sets the stage for future research endeavors. As space threats continue evolving in both number and sophistication, this survey timely provides insights, recommendations, and a clear roadmap for researchers, engineers, and practitioners to fortify space anomaly detection mechanisms.
Elsevier",Medium
"With the advantage of analyzing data of multiple work sites comprehensively while ensuring data privacy, federated learning-based intrusion detection systems (IDS) are emerging as a distributed intrusion detection paradigm. Most of these IDS are assumed to work on static data. However, in the actual network environment, the practice of setting data as static will lead to the phenomenon known as catastrophic forgetting, where old classes that have already appeared would be forgotten. In this paper, we propose a novel IDS framework called FL-IIDS to effectively address the catastrophic forgetting problem. Firstly, a new loss function is synthetically designed for local model training. With the new function, the class gradient balance loss function assigns different learning weights to data of the new and old classes so that the learning rate of the new classes would decrease and the memory of the overall old classes would be deepened. Moreover, the sample label smoothing loss function leverages the knowledge distillation method to enhance the local model memory for every specific class of old classes. Secondly, the relay client fusing sample reconstruction is employed to mitigate the spread of catastrophic forgetting globally without compromising data privacy. Extensive experimental results on the UNSW-NB15 dataset and the CICIDS2018 dataset show that our proposed framework improves the memory capability for old classes substantially without affecting the detection effectiveness of the IDS for new classes.
Elsevier",High
"Most existing log-driven anomaly detection methods assume that logs are static and unchanged, which is often impractical. To address this, we propose a log anomaly detection model called DualAttlog. This model includes word-level and sequence-level semantic encoding modules, as well as a context-aware dual attention module. Specifically, The word-level semantic encoding module utilizes a self-matching attention mechanism to explore the interactive properties between words in log sequences. By performing word embedding and semantic encoding, it captures the associations and evolution processes between words, extracting local-level semantic information. while The sequence-level semantic encoding module encoding the entire log sequence using a pre-trained model. This extracts global semantic information, capturing overall patterns and trends in the logs. The context-aware dual attention module integrates these two levels of encoding, utilizing contextual information to reduce redundancy and enhance detection accuracy. Experimental results show that the DualAttlog model achieves an F1-Score of over 95% on 7 public datasets. Impressively, it achieves an F1-Score of 82.35% on the Real-Industrial W dataset and 83.54% on the Real-Industrial Q dataset. It outperforms existing baseline techniques on 9 datasets, demonstrating its significant advantages.
Elsevier",Medium
"Supervised learning algorithms have shown limited use in the field of anomaly detection due to the unpredictability and difficulty in acquiring abnormal samples. In recent years, unsupervised or semi-supervised anomaly-detection algorithms have become more widely used in anomaly-detection tasks. As a form of unsupervised learning algorithm, generative adversarial networks (GAN/GANs) have been widely used in anomaly detection because GAN can make abnormal inferences using adversarial learning of the representation of samples. To provide inspiration for the research of GAN-based anomaly detection, this review reconsiders the concept of anomaly, provides three criteria for discussing the anomaly detection task, and discusses the current challenges of anomaly detection. For the existing works, this review focuses on the theoretical and technological evolution, theoretical basis, applicable tasks, and practical application of GAN-based anomaly detection. This review also addresses the current internal and external outstanding issues encountered by GAN-based anomaly detection and predicts and analyzes several future research directions in detail. This review summarizes more than 330 references related to GAN-based anomaly detection and provides detailed technical information for researchers who are interested in GANs and want to apply them to anomaly-detection tasks.
Elsevier",Low
"Nowadays, cybersecurity challenges and their ever-growing complexity are the main concerns for various information technology-driven organizations and companies. Although several intrusion detection systems have been introduced in an attempt to deal with zero-day cybersecurity attacks, computer systems are still highly vulnerable to various types of distributed denial of service (DDoS) attacks. This complicated cyber-attack caused many system failures and service disruptions, resulting in billions of dollars of financial loss and irrecoverable reputation damage in recent years. Considering the nonnegligible importance of business continuity in the Industry 4.0 era, this paper presents a comprehensive, systematic survey of DDoS attacks. It also proposes a hierarchy for this severe cyber threat, besides conducting deep comparisons from various perspectives between the studies published by reputed venues in this area. Furthermore, this paper recommends the most effective defensive strategies, with a focus on recently offered fuzzy-based detection methods, to mitigate such threats and bridge the gaps existing in the current intrusion detection systems and related works. The outcomes and key findings of this survey paper are highly advantageous for private companies, enterprises, and government agencies to be implemented in their local or global businesses to significantly improve business sustainability.
Elsevier",Low
"Intrusion Detection Systems (IDS) have become pivotal in safeguarding information systems against evolving threats. Concurrently, Concept Drift presents a significant challenge in machine learning, affecting the adaptability and accuracy of predictive models in dynamic environments. Understanding the synergy between IDS and Concept Drift is crucial for developing robust security systems. The motivation behind this survey is driven by the emerging complexities in cyber threats and the dynamic nature of data streams, which necessitate advanced IDS capable of adapting to Concept and Feature Drift. Our analysis reveals a glaring omission in the existing literatureâ€”the integration of Concept Drift and Feature Drift within IDS. Most studies have focused on Concept Drift in a general context or on IDS but have yet to comprehensively consider the implications of data dynamics. This oversight has led to a fragmented understanding and suboptimal approaches to tackling modern cyber threats. To address this, we propose a comprehensive review that delves into the role of machine learning in IDS, explicitly focusing on Concept and Feature Drift. We have proposed a framework that includes all the necessary components for a drift-aware IDS. The framework incorporates dynamic feature selection, adaptive learning algorithms, and continuous monitoring techniques to handle Concept Drift and Feature Drift effectively. The survey highlights state-of-the-art methodologies and current challenges in integrating these concepts. The methodology involves an exhaustive analysis of published works from 2019 to 2024, comparing and contrasting various models and approaches. This includes a detailed examination of Concept Drift-aware IDS methods, dynamic feature selection techniques, and the impact of high dimensionality in IDS. These quantitative improvements underscore the necessity for developing adaptive and resilient IDS. The survey uncovers under-represented areas in current research, paving the way for future investigations. By highlighting these gaps and providing comparative data, the survey sets a clear direction for upcoming research efforts to foster the development of more dynamic and adaptable IDS solutions. The quantitative experimental evaluation of the proposed framework is planned to be conducted in a future article, where we will assess its effectiveness and performance in real-world scenarios.
Elsevier",Medium
"Deep learning (DL), as one of the most active machine learning research fields, has achieved great success in numerous scientific and technological disciplines, including speech recognition, image classification, language processing, big data analytics, and many more. Big data analytics (BDA), where raw data is often unlabeled or uncategorized, can greatly benefit from DL because of its ability to analyze and learn from enormous amounts of unstructured data. This survey paper tackles a comprehensive overview of state-of-the-art DL techniques applied in BDA. The main target of this survey is intended to illustrate the significance of DL and its taxonomy and detail the basic techniques used in BDA. It also explains the DL techniques used in big IoT data applications as well as their various complexities and challenges. The survey presents various real-world data-intensive applications where DL techniques can be applied. In particular, it concentrates on the DL techniques in accordance with the BDA type for each application domain. Additionally, the survey examines DL benchmarked frameworks used in BDA and reviews the available benchmarked datasets, besides analyzing the strengths and limitations of each DL technique and their suitable applications. Further, a comparative analysis is also presented by comparing existing approaches to the DL methods used in BDA. Finally, the challenges of DL modeling and future directions are discussed",Low
"Network security has always been a concern because it remains to be an unresolved problem. Unlike signature-based methods, anomaly-based methods can detect novel attacks and thus have gained increasing attention over the past decades. However, as the huge and unbounded network data samples continuously arrive at an unprecedented rate and always evolve and change, building a precise network normal pattern has become extremely difficult. In this study, an evolving anomaly detection method for network streaming data is proposed. Clusters are incrementally updated as the new network samples arrive at the incremental updating phase. The outliers, which include not only the global outliers but also the local outliers, are detected using the local density and global density thresholds at the anomaly detection phase. Meanwhile, a buffer is used to store temporary outliers, which may subsequently become normal samples, to avoid normal network samples being deleted as outliers.
Three prominent streaming data (packet-based KDDCUPâ€™99, NSL_KDD, and flow-based CIDDS-001) are used to validate the proposed algorithm. The detection rate of the proposed algorithm can achieve the best result. The result is nearly 100% on KDDCUPâ€™99 and CIDDS-001. The false positive rate and accuracy are 0.0125 and 0.9886 on CIDDS-001, respectively. Experimental results indicate that the proposed algorithm can process real-time network anomaly detection with a much lower time and memory computational cost, and it outperforms other unsupervised anomaly detection methods and most supervised anomaly detection methods reported in the literature in terms of detection rate, false-positive rate, and detection accuracy.
Elsevier",Medium
"The Internet of Things (IoT) envisions a smart environment powered by connectivity and heterogeneity where ensuring reliable services and communications across multiple industries, from financial fields to healthcare and fault detection systems, is a top priority. In such fields, data is being collected and broadcast at high speed on a continuous and real-time scale, including IoT in the streaming processing paradigm. Intrusion Detection Systems (IDS) rely on manually defined security policies and signatures that fail to design a real-time solution or prevent zero-day attacks. Therefore, anomaly detection appears as a prominent solution capable of recognizing patterns, learning from experience, and detecting abnormal behavior. However, most approaches do not fit the urged requirements, often evaluated on deprecated datasets not representative of the working environment. As a result, our contributions address an overview of cybersecurity threats in IoT, important recommendations for a real-time IDS, and a real-time dataset setting to evaluate a security system covering multiple cyber threats. The dataset used to evaluate current host-based IDS approaches is publicly available and can be used as a benchmark by the community.
Elsevier",Medium
"Utilizing machine learning methods to detect intrusion into computer networks is a trending topic in information security research. The limitation of labeled samples is one of the challenges in this area. This challenge makes it difficult to build accurate learning models for intrusion detection. Transfer learning is one of the methods to counter such a challenge in machine learning topics. On the other hand, the emergence of new technologies and applications might bring new vulnerabilities to computer networks. Therefore, the learning process cannot occur all at once. Incremental learning is a practical standpoint to confront this challenge. This research presents a new framework for intrusion detection systems called ITL-IDS that can potentially start learning in a network without prior knowledge. It begins with an incremental clustering algorithm to detect clustersâ€™ numbers and shape without prior assumptions about the attacks. The outcomes are candidates to transfer knowledge between other instances of ITL-IDS. In each iteration, transfer learning provides target environments with incremental knowledge. Our evaluation shows that this method can combine incremental and transfer learning to identify new attacks.
Elsevier",Medium
"Intrusion Detection Systems (IDSs) and Web Application Firewalls (WAFs) offer a crucial layer of defense that allows organizations to detect cyberattacks on their web servers. Academic research overwhelmingly suggests using anomaly detection techniques to improve the performance of these defensive systems. However, analyzing and comparing the wide range of solutions in the scientific literature is challenging since they are typically presented as isolated (unrelated) contributions, and their results cannot be generalized. We believe that this impairs the industryâ€™s adoption of academic results and the advancement of research in this field. This paper aims to shed light on the literature on anomaly-based detection of attacks that use HTTP request messages. We define a novel framework for anomaly detection based on six data processing steps grouped into two sequential phases: preprocessing and classification. Based on this framework, we provide a taxonomy and critical review of the techniques surveyed, emphasizing their limitations and applicability. Future approaches should take advantage of the syntax and semantics of the Uniform Resource Locator (URL), be scalable, and address their obsolescence. These aspects are frequently overlooked in the literature and pose a significant challenge in the current era of web services. For better comparability, authors should use adequate public datasets, follow a thorough methodology, and use appropriate metrics that fully show the pros and cons of the approach.
Elsevier",Low
"Networks and industrial systems play a pivotal role in modern society, and their security has garnered increasing attention. Anomalies within industrial equipment may propagate through fault transmission, leading to a cascade of failures. Additionally, cyberattacks on equipment can result in significant losses. Therefore, in the realm of industrial and cyberspace domains, an effective multivariate time series anomaly detection system for monitoring equipment is instrumental in ensuring the healthy operation of the machinery. Nevertheless, detecting anomalies in numerous time series remains challenging, stemming from the absence of anomaly labels and the complexity of the data patterns. Existing algorithms predominantly concentrate on modeling within the time domain, falling short in fully leveraging the informative features present in frequency domain data, resulting in diminished detection performance. This paper introduces STFT-TCAN, a model for anomaly detection in time series that seamlessly integrates information from both time and frequency domains for extracting data features. Sliding windows and the Short Time Fourier Transform (STFT) are utilized to construct a frequency matrix, effectively amalgamating the characteristics of both time and frequency domains within the time series. Furthermore, the model employs Temporal Convolutional Networks (TCN) and Transformer attention mechanisms (which combined to form the TCAN module) to capture the features of multivariate time series, thereby resulting in heightened detection accuracy. The proposed model undergoes validation on six publicly available datasets, showcasing the superior performance of the STFT-TCAN model in comparison to current baseline methods. It adeptly extracts features from both frequency and time domains in sequential data, thereby achieving state-of-the-art performance in tasks related to anomaly detection in multivariate time series.
Elsevier",Low
"Nowadays, the Internet of Things (IoT) environments are evolving and becoming popular. The number of devices connected to the Internet continues to raise. IoT is an interrelated network of numerous devices in which data is automatically gathered from the environment by the sensors and transferred over the internet without human support and intervention. The IoT eases individuals interacting with real-world applications over the internet in the IoT environment. Modern innovations in IoT have added computers, sensors, streets, buildings, and even communities to the impression of smartness. IoT appliances function in distinct environments to fulfill several purposes; result in the variety of computational devices and communication technologies employed in healthcare, education, military, agriculture, and commerce. Thus, IoT holds a lot of promise for enhancing social and corporate life. Nevertheless, IoT equipment are a soft target and prone to attacks due substantially to their resource limitations, and the nature of their networks. There are many approaches and technologies utilized to preclude IoT from varied attacks and assaults, Intrusion Detection System (IDS) and Intrusion Preventions System (IPS) are some of them, which can ensure the security, privacy, and reliability of the IoT. In this paper, we provide a deep study of many recent and pertinent IDS/IPS proposed between 2019 and 2022 for IoT networks, giving their key specifics, strengths, shortcomings, and challenges in order to spot the issues that still require to be handled. The paper also lines the mainstream research direction and opens the way for new avenues of research for forthcoming researchers.
Elsevier",Low
"The exponential growth in adopting Internet of Things (IoT) applications and services has rendered IoT security an essential concern that must be handled promptly. Multi-vector Distributed Denial of Service (DDoS) attacks are more intensified forms of DDoS attacks, and the anomaly-based Intrusion Detection System (IDS) schemes are the best suitable for detecting and mitigating them. However, deploying anomaly-based IDS frameworks in healthcare systems is particularly difficult since it involves longer processing times, increased complexity, and the need to preserve temporal features. This study presents a novel anomaly-based IDS framework that utilizes proposed stacked modified Gated Recurrent Units (mGRU) to detect and identify the Multi-vector DDoS attacks in mobile healthcare informatics systems. In order to generate user-specific results, we have developed two instances of IDS, namely the Binary Classification Engine (BCE) and the Multi-label Classification Engine (MCE). The empirical results demonstrate that the proposed mGRU-based IDS models outperform the standard GRU-based IDS models, with a reduction in time consumption of around 2% on the CICIoT2023 and CICDDoS2019 datasets. The proposed IDS instances provide leading-edge metrics, lightweight features, and user-specific results, making them suitable for effective deployment in time-critical healthcare applications and services.
Elsevier",Low
"Network Function Virtualization (NFV) is an emerging technology that allows network operators to deploy their Virtualized Network Functions (VNFs) on low-cost commodity servers in the cloud data center. The VNFs, such as virtual routers, firewalls etc., that typically control and transmit critical network packages, require strong security guarantees. However, detecting malicious or malfunctioning VNFs are challenging, as the behaviors of VNFs are dynamic and complex due to the changing network traffics in the cloud. In this paper, we propose a smart and efficient Hidden Markov Model based anomaly detection system (named vGuard) to protect online VNF services in the cloud. A general multivariate HMM model is proposed to profile the normal VNF behavior patterns. Using the VNF behavior model trained with normal observation sequences, vGuard can effectively detect abnormal behaviors online. vGuard is a general framework that can train different types of VNF behavior models. We implement the vGuard prototype in the OpenStack platform. Two types of VNF models, virtual router and virtual firewall, are trained using real normal network traffics in our experiment evaluation. A collection of abnormal attack cases are tested on the VNFs that showed the effectiveness of vGuard in detecting VNF behavior anomalies.
Elsevier",Low
"Todayâ€™s internet data primarily consists of streamed data from various applications like sensor networks, banking data and telecommunication data networks. A new field of study, data stream mining has been gaining popularity to study streamed data behavior. Detection of anomalies in the network traffic also finds its applicability in this context. However traditional machine learning algorithms suffer in providing consistent high accuracy values and give high false alarms. This is due to the presence of concept drift in the captured data streams. Concept drift describes unknown changes in the characteristics of network data over time. Therefore, to handle presence concept drift new methodologies and techniques for drift detection, understanding and adaptation are required. In this paper, we have proposed two techniques, an Error Rate Based Concept Drift Detection and Data Distribution Based Concept Drift Detection and studied their impact. Furthermore, sliding window based data capturing and drift analyzing combined with K-Means Clustering has been used for reducing data size and upgrading training dataset. We have used the Support Vector Machine (SVM) classifier for anomaly detection and retraining of the model has been initiated based on statistical tests. The experiments have been performed on three datasets, namely, generated Testbed Dataset, NSL-KDD and CIDDS-2017. Detection accuracy, KL-Divergence and Kappa Statistics have been used to study the severity of the concept drift in the datasets. After applying the proposed approach, the SVM has been shown to have a better classification accuracy of 93.52%, 99.80% and 91.33% respectively. We achieved a precision rate of 91.84%, 99.1% and 88.3%, a recall rate of 94.30%, 99.2% and 91.7% with an F1 score of 92.9%, 99.15% and 89.6% respectively.
Elsevier",Medium
"Intrusion detection methods are crucial means to mitigate network security issues. However, the challenges posed by large-scale complex network environments include local information islands, regional privacy leaks, communication burdens, difficulties in handling heterogeneous data, and storage resource bottlenecks. Federated learning has the potential to address these challenges by leveraging widely distributed and heterogeneous data, achieving load balancing of storage and computing resources across multiple nodes, and reducing the risks of privacy leaks and bandwidth resource demands. This paper reviews the process of constructing federated learning based intrusion detection system from the perspective of intrusion detection. Specifically, it outlines six main aspects: application scenario analysis, federated learning methods, privacy and security protection, selection of classification models, data sources and client data distribution, and evaluation metrics, establishing them as key research content. Subsequently, six research topics are extracted based on these aspects. These topics include expanding application scenarios, enhancing aggregation algorithm, enhancing security, enhancing classification models, personalizing model and utilizing unlabeled data. Furthermore, the paper delves into research content related to each of these topics through in-depth investigation and analysis. Finally, the paper discusses the current challenges faced by research, and suggests promising directions for future exploration.",Medium
"With the constant updating of applications and the emergence of various encryption technologies, it is important to achieve continuous learning of encrypted traffic. Traditional encrypted traffic classification techniques can only handle a fixed number of traffic classes and require all traffic data to be available at the same time, which consume a huge amount of memory to save the traffic data. In this paper, we propose an incremental learning method for encrypted traffic classification, called ILETC, to achieve continuous learning of encrypted traffic. ILETC can learn new encrypted traffic classes while avoiding forgetting knowledge of old encrypted traffic, with limited memory resources. It uses WGAN-GP to model the data distribution of encrypted traffic and design an exemplar set to select and store representative real traffic data. When learning from a new class of encrypted traffic, ILETC replays the knowledge of the old classes with the generated samples and exemplars to mitigate catastrophic forgetting. The ISCX VPN-nonVPN dataset and self-collected dataset are used to test the performance of ILETC. The results show that ILETC is superior to the state-of-the-art methods with an accuracy of over 98% on the ISCX dataset and over 94% in the two datasets together.
Elsevier",High
"Nowadays, with an increasing expansion of the internet of things (IoT) that has created massive data streams, online attack detection via stream processing has become a matter of extensive attention. Current encryption and authentication methods cannot satisfy the security requirements of IoT critical infrastructures because of many heterogeneous connected devices, a wide network geographical scope, rapid software development, the possibility of security bugs, and the emergence of new attacks. Therefore, an extended method based on machine learning, which can process data stream to detect the fog layer's IoT attacks and prevent the spread of intrusion to other network segments, was proposed in this paper. The IFogLearn++ uses a fog layer to facilitate fast data stream processing in the fog layer. Using a fog layer helps the network tolerate more attacks due to the annexation of an extra security layer before the cloud layer. Based on the results, the IFogLearn++ has similar accuracy to competitors and 18 and 8 times faster performance compared to SVM and Learn++. This characteristic helps secure massive data streams in IoT.
Elsevier",Medium
"Microwave links are widely used in cellular networks for large-scale data transmission. From the network operatorsâ€™ perspective, it is critical to quickly and accurately detect microwave link failures before they actually happen, thereby maintaining the robustness of the data transmissions. We present PMADS, a machine-learning-based proactive microwave link anomaly detection system that exploits both performance data and network topological information to detect microwave link anomalies that may eventually lead to actual failures. Our key observation is that anomalous links often share similar network topological properties, thereby enabling us to further improve the detection accuracy. To this end, PMADS adopts a network-embedding-based approach to encode topological information into features. It further adopts a novel active learning algorithm, ADAL, to continuously update the detection model at low cost by first applying unsupervised learning to separate anomalies as outliers from the training set. We evaluate PMADS on a real-world dataset collected from 2142 microwave links in a production LTE network during a one-month period, and show that PMADS achieves a precision of 94.4% and a recall of 87.1%. Furthermore, using the active learning feedback loop, only 7% of the training data is required to achieve comparable results. PMADS is currently deployed in a metropolitan LTE network that serves around four million subscribers in a Middle Eastern country.
Elsevier",Medium
"As a promising paradigm, Federated Learning (FL)-based distributed intrusion detection offers potent protection for the network security of Industrial Internet of Things (IIoT) systems. Nonetheless, the practical deployment of IIoT systems occurs in a highly-complex and dynamic distributed environment. The ever-growing and dynamically-evolving cyber attacks will render the FL-based intrusion detection model inefficient, since FL cannot gracefully learn from the dynamic traffic data streams to identify new attacks. To address this issue, we propose the evolutionary distributed intrusion detection system based on federated continual representation learning, designed to continually capture effective feature representations of emerging attacks from dynamic traffic data streams. Specifically, we develop the supervised contrastive loss and the global information-aware regularization loss to alleviate the catastrophic forgetting on the previously observed attacks and mitigate the data heterogeneity across clients. Besides, we propose the prototype variance-based memory update strategy to ensure the effective memory replay data. Extensive experimental results demonstrate our proposed method outperforms the state-of-the-art methods by 13.3%â€“31.5% in terms of average accuracy on a real energy intrusion detection dataset.
Elsevier",High
"The ongoing battle between malware distributors and those seeking to prevent the onslaught of malicious code has, so far, favored the former. Anti-virus methods are faltering with the rapid evolution and distribution of new malware, with obfuscation and detection evasion techniques exacerbating the issue. Recent research has monitored low-level opcodes to detect malware. Such dynamic analysis reveals the code at runtime, allowing the true behaviour to be examined. While previous research uses machine learning techniques to accurately detect malware using dynamic runtime opcodes, underpinning datasets have been poorly sampled and inadequate in size. Further, the datasets are always fixed size and no attempt, to our knowledge, has been made to examine the cost of retraining malware classification models on datasets which grow continually. In the literature, researchers discuss the explosion of malware, yet opcode analyses have used fixed-size datasets, with no deference to how this model will cope with retraining on escalating datasets. The research presented here examines this problem, and makes several novel contributions to the current body of knowledge.
First, the performance of 23 machine learning algorithms are investigated with respect to the largest run trace dataset in the literature. Second, following an extensive hyperparameter selection process, the performance of each classifier is compared, on both accuracy and computational costs (CPU time). Lastly, the cost of retraining and testing updatable and non-updatable classifiers, both parallelized and non-parallelized, is examined with simulated escalating datasets. This provides insight into how implemented malware classifiers would perform, given simulated dataset escalation. We find that parallelized RandomForest, using 4 cores, provides the optimal performance, with high accuracy and low training and testing times.
Elsevier",Medium
"Despite extensive academic research in anomaly detection within the cybersecurity domain, its successful adoption in real-world settings remains limited. This paper addresses the challenges of applying outlier detection techniques for threat detection within the context of Security Information and Event Management (SIEM) systems. It particularly highlights the significance of contextualization and explainability, while challenging the assumption that outliers invariably indicate malicious activity. It proposes a simple yet effective outlier detection technique designed to mimic a Security Operation Center (SOC) analystâ€™s reasoning process in finding anomalies/outliers and deciding maliciousness. The approach emphasizes explainability and simplicity, achieved by combining the output of simple, context-aware univariate submodels that calculate an outlier score for each entry.
The proposed technique is first evaluated on a public dataset, demonstrating its ability to achieve high performance in detecting outliers compared to other well-known algorithms. Furthermore, to assess the practicality in a real-world scenario, the approach is deployed in production alongside the SIEM of a large international enterprise with over 100,000 assets, utilizing 20 terabytes of Endpoint Detection and Response (EDR) logs to detect Living-off-the-Land Binaries (LOLBins). The proposed framework can empower SOC analysts in developing scalable, effective, and interpretable outlier-based threat detection use cases.
Elsevier",Low
"With the advent of big data, encrypted traffic is widely used, and it is gradually becoming a new challenge for network security and management. Many researchers have obtained good results by converting encrypted traffic into images and feeding them to deep learning-based classification models. However, these methods have some limitations in that they cannot do sustainable learning. They have to retrain a new classifier when new traffic is encountered. To tackle this issue, we propose an extended encrypted traffic classification algorithm based on incremental learning. This approach extracts content and statistical information from encrypted traffic and supports multi-label prediction for VPN channels and applications. We can add new classes to the model without completely retraining and accelerate the update cycle of the model. The results show that the proposed model performs well in classification experiments, which can achieve 98.1% and 96.0% classification accuracy on ISCXVPN2016 and self-collected dataset respectively. In addition, our method can retain high accuracy under the situation of limited memory space while the number of new classes of data increases gradually. It demonstrates the superiority of constructing a generic unforgetting basis for classifying encrypted communication.
Elsevier",Medium
"The increasing Internet of Things (IoT) network complexity and sophisticated Distributed Denial of Service (DDoS) attacks at machine speeds make accurate and timely detection and mitigation of these attacks a challenging activity. This study presents a multi-agent-based system design (MAS-IoT) against DDoS attacks in the IoT network. The MAS-IoT consists of different types of agents which communicate using Advanced Encryption Standard (AES). In the context of the study, DDoS attacks were detected using a long-short-term memory (LSTM)-based model (LSTM-IoT) developed based on the CIC-IoT-2022 dataset with a 99.48% accuracy rate. The detection time of the LSTM-IoT and its time complexity were calculated and compared using similar methods mentioned in the literature. The results demonstrate the effectiveness of the LSTM-IoT in accurately detecting DDoS attacks. However, the ability to use it effectively and on time is vital to counter real-time attacks. The MAS-IoT system enables this with minimum human intervention.
Elsevier",Medium
"The fast evolutions of Internet of Things (IoT) technologies have been accelerating their applicability in different sectors of life and becoming a pillar for sustainable development. However, this revolutionary expansion led to a substantial increase in attack surface, raising many concerns about security threats and their possible consequences. Machine learning has significantly contributed to designing intrusion detection systems (IDS) but suffers from critical limitations such as data privacy and sovereignty, data imbalance, concept drift, and catastrophic forgetting. This collectively makes existing IDSs an improper choice for securing IoT environments. This paper presents a federated learning approach called FIDWATCH to continuously monitor and detect a broad range of IoT security threats. The local side of FIDWATCH introduces contrastive focal loss to enhance the ability of the local model (teacher) to discriminate between diverse types of IoT security threats while putting an increased emphasis on hard-to-classify samples. A fine-grained Knowledge Distillation (KD) is introduced to allow the client to distill the required teacher's knowledge into a lighter, more compact model termed the pupil model. This greatly assists the competence and flexibility of the model in resource-constrained scenarios. Furthermore, an adaptive incremental updating method is introduced in FIDWATCH to allow the global model to exploit the distilled knowledge and refine the shared dataset. This helps generate global anchors for improving the robustness of the mode against the distributional shift, thereby improving model alignment and compliance with the dynamics of IoT security threats. Proof-of-concept simulations are performed on data from two public datasets (BoT-IoT and ToN-IoT), demonstrating the superiority of FIDWATCH over cutting-edge performance with an average f1-score of 97.07% and 95.63%, respectively.
Elsevier",High
"The industrial internet of things (IIoT) is an evolutionary extension of the traditional Internet of Things (IoT) into processes and machines for applications in the industrial sector. The IIoT systems generate a large amount of private and sensitive data i.e., stored and processed somewhere on the cloud-edge continuum. The IIoT devices, and the IIoT networks are subject to security mechanisms such as intelligent Intrusion Detection and Prevention Systems (IDS/IPS) systems, that can detect and respond unseen malicious network attacks. The adoption of centralized machine learning methods for IDS has become impractical due to the high computational cost and privacy concerns associated with storing large amounts of data on a single server along the cloud-edge continuum. The combination of federated learning and Blockchain has emerged as a promising advancement in addressing the challenge. Federated learning distributes learning to individual IIoT devices without compromising data privacy, while Blockchain enhances privacy and security. Many academic and industrial efforts outline IDS mechanisms using machine learning, deep learning, federated learning, and Blockchain technologies. The utilization of federated learning-based IDS has become increasingly popular and is now being applied to various tasks including IDS/IPS systems. However, existing intrusion detection systems (IDSs) survey are limited to the scope of classical machine learning and deep learning. To address this limitation, we analyze the IIoT literature that integrates Blockchain and federated learning to enhance IDSs and improve its threat detection capabilities. This survey explores the role of Blockchain and federated learning in addressing security and privacy issues, particularly those associated with IDS/IPS in IIoT networks. Insights on the possibilities of machine learning, federated learning, and Blockchain in supporting IDS to monitor IIoT network traffic for anomaly detection are discussed in detail through state of the art. Furthermore, we provide a set of recommendation based on our literature for the effective implementation of a Blockchain and federated learning-based network intrusion detection system. Finally, we summarize the study and highlight challenges as future research directions for Blockchain and Federated Learning-based technologies for cybersecurity and intrusion detection in IIoT.
Elsevier",Medium
"The Internet of Medical Things (IoMT) is a transformative fusion of medical sensors, equipment, and the Internet of Things, positioned to transform healthcare. However, security and privacy concerns hinder widespread IoMT adoption, intensified by the scarcity of high-quality datasets for developing effective security solutions. Addressing these challenges, we propose a novel framework for cyberattack detection in dynamic IoMT networks. This framework integrates Federated Learning with Meta-learning, employing a multi-phase architecture for identifying known attacks, and incorporates advanced clustering and biased classifiers to address zero-day attacks. The framework's deployment is adaptable to dynamic and diverse environments, utilizing an Infrastructure-as-a-Service (IaaS) model on the cloud and a Software-as-a-Service (SaaS) model on the fog end. To reflect real-world scenarios, we introduce a specialized IoMT dataset. Our experimental results indicate high accuracy and low misclassification rates, demonstrating the framework's capability in detecting cyber threats in complex IoMT environments. This approach shows significant promise in bolstering cybersecurity in advanced healthcare technologies.",Medium
"With the vigorous development of Internet technology, the scale of systems in the network has increased sharply, which provides a great opportunity for potential attacks, especially the Distributed Denial of Service (DDoS) attack. In this case, detecting DDoS attacks is critical to system security. However, current detection methods exhibit limitations, leading to compromises in accuracy and efficiency. To cope with it, three key strategies are implemented in this paper: (i) Using tensors to model large-scale and heterogeneous data in complex networks; (ii) Proposing a denoising algorithm based on the improved and distributed tensor train (IDTT) decomposition, which optimizes the tensor train(TT) decomposition in terms of parallel computation and low-rank estimation; (iii) Combining (i), (ii) and Light Gradient Boosting Machine (LightGBM) classification model, an efficient DDoS attack detection framework is proposed. Datasets CIC-DDoS2019 and NSL-KDD are used to evaluate the framework, and results demonstrate that accuracy can reach 99.19% while having the characteristics of low storage consumption and well speedup ratio.
Elsevier",Low
"Anomaly detection methods based on machine learning assist in identifying attacker behavior concealed in critical infrastructureâ€™s high-speed network traffic. However, these methods generally experience problems including a lack of labeled data and poor performance. We suggest a detection method based on staged frequency domain features to address these issues. A small-step sliding window is used in the training phase to fully understand the frequency domain features of the traffic. We suggest SOM-Kmeans, an integrated clustering technique that can accurately distinguish between malicious and benign flows. We evaluate the SOM-Kmeans accuracy using open datasets and assess its effectiveness in a real network environment. The experimental results demonstrate that our method can detect anomaly traffic at high speed without sacrificing detection accuracy.
Elsevier",Low
"The focus on privacy protection has brought much-encrypted network traffic. However, attackers always abuse traffic encryption to conceal malicious behaviors. Although researchers have proposed several enlightening detection methods, they must enhance the generalization ability or improve detection performance. Our inspiration is that the packet header fields, as do the underlying grammatical rules for constructing sentences, have a strict order. We consider the original packet as text and devise a robust approach with natural language processing and a deep learning model to improve the generalization ability and detection performance. We capture the critical keywords as characteristic representations of the traffic and design an adaptive domain generalization algorithm with a new loss function. It is robust against various datasets by generating more malicious samples to augment the minority of malicious samples. Simultaneously, we design an efficient feature selection algorithm, which obtains an optimal feature subset and reduces feature dimensions by 75.3%. To evaluate our work, we conducted extensive experiments with open-source datasets (CICIDS 2017, CICDDoS 2019, and USTC-TFC 2016), the synthetic dataset from IoT-23, and Internet backbone traffic (CERNET). Experimental results demonstrate that our proposal improves detection accuracy by up to 22.8% compared to others not using domain generalization algorithms and achieves an average detection latency of 0.67 seconds in the backbone. Besides, our work applies to the Industrial Internet of Things (IIoT) environment. It can be deployed at edge nodes to provide network security support for IIoT devices.
Elsevier",Low
"Network intrusion detection systems (NIDS) play a crucial role in maintaining network security. However, current NIDS techniques tend to neglect the topological structures of network traffic to varying degrees. This fundamental oversight leads to challenges in handling class-imbalanced and highly dynamic network traffic. In this paper, we propose a novel dynamic multi-scale topological representation (DMTR) method for improving network intrusion detection performance. Our DMTR method achieves the perception of multi-scale topology and exhibits strong robustness. It provides accurate and stable representations even in the presence of data distribution shifts and class imbalance problems. The multi-scale topology is obtained through multiple topology lenses, which reveal topological structures from different dimensional aspects. Furthermore, to address the limitations of existing detection models based on static network traffic, the DMTR method also achieves dynamic topological representation through our proposed group shuffle operation (GSO) strategy. When new traffic data arrives, the topological representation is updated by preserving a portion of the original information without reprocessing all data. Experiments on four publicly available network traffic datasets demonstrate the feasibility and effectiveness of the proposed DMTR method in handling class imbalanced and highly dynamic network traffic.
Elsevier",Medium
"Existing log anomaly diagnosis methods still face challenges in the lack of statistical features of log messages and insufficient exploitation of textual semantic features. In order to tackle this issue, we propose a novel approach called Dynamic Semantic Gating Network (DSGN). The core idea of DSGN is to enrich the semantic representation of log texts by selectively utilizing statistical information, thus achieving an organic integration of statistical and semantic features. Specifically, DSGN incorporates a variational encoding module to encode statistical features, and a log content-aware graph convolutional network module to capture semantic features from the log context. Furthermore, DSGN introduces a dynamic semantic threshold mechanism that dynamically adjusts the information flow based on the confidence level of semantic features and feeds it into the classifier. This design not only helps train a more robust classifier, but also leverages the advantages of both statistical and semantic features while avoiding overfitting caused by using statistical features. Experimental results show that the DSGN model achieves significant performance improvements on seven public datasets, with a macro-average F1 score exceeding 83% and a micro-average F1 score exceeding 81%, outperforming existing baseline techniques and demonstrating its substantial advantages.
Elsevier",Medium
"With the rapid development of network environment, cyber attacks have become one of the major threats to network security, and maintaining network security requires accurate detection of malicious traffic generated by cyber attacks. However, due to the dynamic nature of network behavior, data distribution in network traffic may change over time, i.e., appearing concept drift phenomenon, and the emergence of concept drift causes existing malicious traffic detection models to suffer from the problem of decreased detection efficiency. To address this challenge, we propose a Concept Drift Detection and Adaptation-based Malicious traffic Detection method called CDDA-MD. Firstly, the network traffic is segmented using sliding window technique and the data samples are analyzed on the basis of each window. And then, a long short-term memory network (LSTM) is utilized to capture the long-term dependencies in the time-series features of network traffic; At the same time, a multi-head self-attention mechanism is introduced to provide larger weights for the important features. Moreover, we replace the ReLU activation function in LSTM with Tanh to overcome the neuron â€œdeathâ€ problem, and replace the Adam optimizer with Nadam to accelerate convergence, thereby improving the detection performance. Next, the concept drift is detected based on the idea of error rate, and the detected concept drift data is used for incremental learning to make the model adapt to current network environment. Finally, based on the detected concept drift, malicious traffic detection operations are performed to effectively maintain the security of cyberspace. Experiments on four network traffic show that compared with existing state-of-the-art methods, the proposed CDDA-MD method improves 0.3%, 1.2% , 1.16% and 1.9% in F1-measure, 0.25%, 1.1%, 1.44% and 1.72% in TPR, respectively; It also has better stability.
Elsevier",Medium
"Currently, the world is witnessing a mounting avalanche of data due to the increasing number of mobile network subscribers, Internet websites, and online services. This trend is continuing to develop in a quick and diverse manner in the form of big data. Big data analytics can process large amounts of raw data and extract useful, smaller-sized information, which can be used by different parties to make reliable decisions.
In this paper, we conduct a survey on the role that big data analytics can play in the design of data communication networks. Integrating the latest advances that employ big data analytics with the networksâ€™ control/traffic layers might be the best way to build robust data communication networks with refined performance and intelligent features. First, the survey starts with the introduction of the big data basic concepts, framework, and characteristics. Second, we illustrate the main network design cycle employing big data analytics. This cycle represents the umbrella concept that unifies the surveyed topics. Third, there is a detailed review of the current academic and industrial efforts toward network design using big data analytics. Forth, we identify the challenges confronting the utilization of big data analytics in network design. Finally, we highlight several future research directions. To the best of our knowledge, this is the first survey that addresses the use of big data analytics techniques for the design of a broad range of networks.
Elsevier",Low
"Classifying encrypted network traffic into distinct categories, also known as encrypted traffic classification (ETC), is a crucial step in maintaining network security. There are numerous applications in the area of network security, such as quality of service. The rapid development of network applications makes tackling the classification of encrypted traffic in an incremental learning environment attractive. However, the ambiguity and imbalance of traffic restrict existing incremental approaches from achieving satisfactory results. To overcome these obstacles, we provide (1) a unique clustering-based exemplar selection approach for storing the most representative data relevant to each learned traffic scenario, as well as filtering ambiguous traffic for knowledge replay. Furthermore, (2) we ensure consistent model performance across all trained classes while addressing traffic imbalance issues via an incremental contrastive distillation approach. Using real encrypted network datasets, we validate the efficacy of our technique. The experimental findings prove the efficacy of the suggested strategy, our method surpasses the state-of-the-art model with micro F1 score improvements of 5.0%, 8.6%, and 4.8% on the ISCX-VPN, 18 Apps, and TLS1.3 benchmarks, respectively.
Elsevier",High
"Controller Area Network (CAN) serves as the neural system of modern cars, connecting and coordinating various electronic control units (ECUs) responsible for vehicle operation. However, the inherent features of CAN, such as broadcast communication and lack of authentication, make it increasingly vulnerable to cyberattacks. Although existing intrusion detection systems (IDSs) perform well in detecting malicious attacks, they often lack the ability to accurately locate the senders of these malicious messages. In this paper, we propose an efficient sender identification method called Voltage Inspector, which leverages physical voltage signal slice to accurately identify the source of messages for CAN bus. We start by extracting voltage slices from the raw physical signals of the CAN bus. Next, we leverage clustering technology to infer the ECU mapping information, which is typically considered confidential. This mapping information, combined with a machine learning classifier, is then utilized to construct an identification model capable of accurately identifying the sender of each message. To validate the effectiveness of our proposed method, we conducted extensive experiments using a publicly available voltage dataset collected from ten real vehicles. The experimental results demonstrate the remarkable accuracy of our approach, achieving a minimum identification accuracy of 99%. Furthermore, our method significantly reduces the data volume by half and reduces the identification time by a quarter when compared to state-of-the-art methods. Our research reveals that even a small portion of the voltage signal can be used to uniquely fingerprint an ECU. We emphasize that our method serves as an alternative identification approach and can complement existing works in the field.
Elsevier",Low
"Wireless Sensor Networks (WSNs) are susceptible to various security threats owing to its deployment in hostile environments. Intrusion detection system (IDS) contributes a critical role on securing WSNs by identifying malevolent activities and ensuring data integrity. Traditional IDS techniques often struggle with the dynamic and resource-constrained nature of WSNs. In this paper, Dynamically Stabilized Recurrent Neural Network Optimized with Intensified Sand Cat Swarm Optimization for Wireless Sensor Network Intrusion identification (DSRNN-ISCOA-ID-WSN) is proposed. Initially, the input data is amassed from WSN-DS dataset. After that, the pre-processing segment receives the data. In pre-processing stage, redundant and biased records are removed from input data with the help of Adaptive multi-scale improved differential filter (AMSIDF). Then the optimal are selected by utilizing Wolf-Bird Optimization Algorithm (WBOA). DSRNN is used to classify the data as Normal, Grey hole, Black hole, Time division multiple access (TDMA), and Flooding attacks. Then Intensified Sand Cat Swarm Optimization (ISCOA) is employed to optimize the weight parameters of DSRNN for accuracte classification. The proposed DSRNN-ISCOA-ID-WSN technique is implemented Python. The performance of the proposed DSRNN-ISCOA-ID-WSN approach attains 29.24 %, 33.45 %, and 28.73 % high accuracy; 30.53 %, 27.64 %, and 26.25 % higher precision when compared with existing method such as Machine Learning-Powered Stochastic Gradient Descent Intrusions Detection System for WSN Attacks (SGDA-ID-WSN), An updated dataset to identify threats in WSN (CNN-ID-WSN) and Denial-of-Service attack detection in WSN: a Low-Complexity Machine Learning Model (DTA-ID-WSN) respectively.
Elsevier",Low
"The Internet of Things (IoT) devices have been integrated into almost all everyday applications of human life such as healthcare, transportation and agriculture. This widespread adoption of IoT has opened a large threat landscape to computer networks, leaving security gaps in IoT-enabled networks. These resource-constrained devices lack sufficient security mechanisms and become the weakest link in our in computer networks and jeopardize systems and data. To address this issue, Intrusion Detection Systems (IDS) have been proposed as one of many tools to mitigate IoT related intrusions. While IDS have proven to be a crucial tools for threat detection, their dependence on labeled data and their high computational costs have become obstacles to real life adoption. In this work, we present IoT-PRIDS, a new framework equipped with a host-based anomaly-based intrusion detection system that leverages â€œpacket representationsâ€ to understand the typical behavior of devices, focusing on their communications, services, and packet header values. It is a lightweight non-ML model that relies solely on benign network traffic for intrusion detection and offers a practical way for securing IoT environments. Our results show that this model can detect the majority of abnormal flows while keeping false alarms at a minimum and is promising to be used in real-world applications.
Elsevier",Low
"The evolution of technology has raised concerns regarding cybersecurity for intelligent connected vehicles (ICVs). In-vehicle network in ICVs lacks robust protection mechanisms, making it vulnerable to cyber threats. In response, intrusion detection systems (IDSs) for ICVs have been developed to protect vehicles from malicious cyber attacks. However, current IDS methods solely rely on independent features, limiting their learning capabilities and increasing the number of false detections. Moreover, many IDSs require the knowledge of mapping between network messages and contents, which restricts their application. To address these limitations, we propose the Multi-order Feature Interaction-aware Intrusion Detection (MIFI) scheme for ICVs. Feature attention cross network is designed to address higher-order feature interactions, while factorization machine is used for second-order interactions. Then a discriminator is utilized to detect the attacks. MIFI expands the feature space through features interaction, thereby enhancing its ability to detect attacks. Moreover, it perceives the relationships of vehicle messages, facilitating intrusion detection without knowing the corresponding rules of vehicle messages. The performance of the proposed method is evaluated on two real-vehicle datasets, affirming its effectiveness and robustness. MIFI achieves an accuracy of over 99% in detecting different attacks. The proposed method can improve the accuracy of traditional IDS to a maximum of 99.99%, and increase the highest F1-score to 97.18%, demonstrating the modelâ€™s ability of achieving multi-order feature interactions. Ultimately, MIFI is suitable for intrusion detection in different types of ICV networks, significantly contributing to the cybersecurity of ICVs.
Elsevier",Low
"The security analysis has become the hotspot concern in Industrial Control Systems (ICSs) that grabs the research attention in todayâ€™s era. Owing to the rapid admittance of the Industrial Internet of Things (IIoT), the superimposition of fog and cloud computing plays a pivotal role for rectifying such issues as fewer computation resources and more latency in the network. In addition to this, security issue comes first while developing the ICS in the sector of IIoT. In general, ICS is applied for various processes, such as electric utilization and water supply management, that are related to every individualâ€™s life. Though it is interlinked with the Internet, it can easily expose to different threats. To defend the model, Intrusion Detection Systems (IDS) are employed. In signature-based detection, anomaly detection is used to find out the unknown attacks in the network. Yet, the ICS is structured with many software and hardware tools. It also seems in the openness nature of the industrial environment that, it becomes vulnerable to various attacks. Hence, detecting unknown attacks is a complex task in the openness nature of the industrial environment. It causes failure to protect such systems from malicious entities that could result in more complications for humans. Hence, the detection of malicious activities is essential. In order to provide an efficient model, a new hybrid deep learning is proposed in ICS. Initially, the original data is to be collected based on the ICS industry. Secondly, the gathered data is preprocessed to clean the artifacts and clean the data. Thirdly, the optimal features are chosen directly from the gathered data with the help of a hybrid algorithm called the Hybrid Honey Badger-World Cup Algorithm (HHB-WCA). The chosen optimal features are fed to the hybrid deep learning termed as Autoencoder-Bi-directional Long Short-Term Memory (A-Bi-LSTM), where the encoded features from Autoencoder are extracted and encoded features are subjected to the Bi-LSTM classifier. Finally, the simulation outcome has shown that the developed method is compared with the other traditional algorithms to detect the unknown classes in the network.
Elsevier",Low
"Suspicious action recognition is a captivating and testing task in the realm of surveillance. An anomaly recognition framework recognizes abnormal happenings uniquely in contrast to existing examples because any anomaly is an example that is not the same as a bunch of standard examples. Security is a fundamental need in each space, whether it is public or private. The utilization of feature extraction techniques, both from hand-crafted and deep learning methods, significantly influences the comprehensive methodology discussed in detail within this paper. This survey paper comprehensively covers multiple areas of advancements in surveillance. Starting with the importance and application of anomaly recognition in surveillance which leads to a comparison of different survey papers is also presented for reference which also includes the areas that are covered in this survey paper. Available datasets in the realm of surveillance are also explored in this survey paper leading to feature extraction methods of both handcrafted and deep learning. This paper also summarizes different methods available for suspicious action recognition in surveillance. The paper delves into the challenges faced when addressing this vital issue, presents valuable findings, and outlines limitations associated with the topic. It provides extensive analysis and ends by outlining potential future trends.
Elsevier",Low
"Anomaly intrusion detection systems are a class of intrusion detection systems that do not rely on the security attacks' signatures and focus on finding unknown malicious behaviors and attacks. In this context, some of the anomaly detection schemes benefit from various fuzzy data mining and statistical methods to deal with ambiguity in the intrusion detection process. The main objective of this article is to put forward an extensive and structured survey of the fuzzy logic-based network traffic anomaly and Distributed Denial of Service (DDoS) attack detection approaches. It groups the investigated scheme concerning the fuzzy techniques applied to deal with network anomalies and DDoS attacks. It illuminates how the fuzzy network anomaly detection approaches have integrated various techniques such as classifiers, feature selection/extraction methods, and statistical and clustering algorithms to find anomalous traffic. Besides, the significant challenges, issues, and ideas in network anomaly detection are discussed. Lastly, several future research topics are provided to better lead the subsequent studies in this context.
Elsevier",Low
"Incremental learning algorithms have been developed as an efficient solution for fast remodeling in Broad Learning Systems (BLS) without a retraining process. Even though the structure and performance of broad learning are gradually showing superiority, private data leakage in broad learning systems is still a problem that needs to be solved. Recently, Multiparty Secure Broad Learning System (MSBLS) is proposed to allow two clients to participate training. However, privacy-preserving broad learning across multiple clients has received limited attention. In this paper, we propose a Self-Balancing Incremental Broad Learning System (SIBLS) with privacy protection by considering the effect of different data sample sizes from clients, which allows multiple clients to be involved in the incremental learning. Specifically, we design a client selection strategy to select two clients in each round by reducing the gap in the number of data samples in the incremental updating process. To ensure the security under the participation of multiple clients, we introduce a mediator in the data encryption and feature mapping process. Three classical datasets are used to validate the effectiveness of our proposed SIBLS, including MNIST, Fashion and NORB datasets. Experimental results show that our proposed SIBLS can have comparable performance with MSBLS while achieving better performance than federated learning in terms of accuracy and running time.
Elsevier",Medium
"The Social Internet of Things (Social IoT) introduces a fresh approach to promote the usability of IoT networks and enhance service discovery by incorporating social contexts. However, this approach encounters various challenges that impact its performance and reliability. One of the most prominent challenges is trust, specifically trust-related attacks, where certain users engage in malicious behaviors and launch attacks to spread harmful services. To ensure a trustworthy experience for end-users and prevent such attacks in real-time, it is highly significant to incorporate a trust management mechanism within the Social IoT network. To address this challenge, we propose a novel trust management mechanism that leverages blockchain technology. By integrating this technology, we aim to prevent trust-related attacks and create a secure environment. Additionally, we introduce a new consensus protocol for the blockchain called Spark-based Proof of Trust-related Attacks (SPoTA). This protocol is designed to process stream transactions in real-time using Apache Spark, a distributed stream processing engine. To implement SPoTA, we have developed a new classifier utilizing Spark Libraries. This classifier is capable of accurately categorizing transactions as either malicious or secure. As new transaction streams are read, the classifier is employed to classify and assign a label to each stream. This label assists the SPoTA protocol in making informed decisions regarding the validation or rejection of transactions. Our research findings demonstrate the effectiveness of our classifier in predicting malicious transactions, outstripping our previous works and other approaches reported in the literature. Additionally, our new protocol exhibits improved transaction processing times.
Elsevier",Medium
"The proliferation of rumors on social networks poses a serious threat to cybersecurity, justice and public trust, increasing the urgent need for rumor detection. Existing detection methods typically treat all rumors as a single homogeneous category, neglecting the diverse semantic hierarchies within rumors. Rumors pervade various domains, each with its distinct characteristics. These methods tend to lag in expressiveness when confronted with real-world scenarios involving multiple semantic levels. Furthermore, the diversity of rumors also complicates the collection of datasets, and inevitably introduces noisy data, which hinders the correctness of the learned representations. To address these challenges, we propose a rumor detection framework with Hierarchical Prototype Contrastive Learning (HPCL). In this framework, we construct a set of dynamically updated hierarchical prototypes through contrastive learning to encourage capturing the hierarchical semantic structure within rumors. Additionally, we design a difficulty metric function based on the distance between instances and prototypes, and introduce curriculum learning to mitigate the adverse effects of noisy data. Experiments on four public datasets demonstrate that our approach achieves state-of-the-art performance. Our code is publicly released at https://github.com/Coder-HenryZa/HPCL",Low
"STRIDE (Spoofing, Tampering, Repudiation, Information Disclosure, Escalation of privilege) in advance metering infrastructure (AMI) and cloud computing have been confronted with numerous botnet attacks within the IoT systems. Botmasters aid botnets in engineering their operations by changing the codes and updating the bots to continue to thwart the recent detection schemes. The study proposed AdKNN, a composition of k's nearest neighbour, an enhanced Adam's optimiser in this paper. The KNN in AdKNN is for the early detection of the botnet within the AMI networks, while the new Adam-based optimiser is responsible for optimising AdKNN. An extensive experiment has demonstrated that our model AdKNN is efficient and effective for botnet classification in AMI networks on the publicly available dataset for IoT-botnet detection evaluated on AMI network dataset compared to existing state-of-the-art (SOTA) models. Performance-wise, AdKNN achieves the highest overall accuracy of 99% with 99% precision and low false positives in an appreciable minimal training time.
Elsevier",Low
"With the number of Internet of Things (IoT) devices proliferating, the traffic volume of IoT-based attacks has shown a gradually increasing trend. The IoT botnet attack, which aims to commit real, efficient, and profitable cybercrimes, has become one of the most severe IoT threats. Applying traditional techniques to IoT is difficult due to its particular characteristics, such as resource-constrained devices, massive volumes of data, and real-time requirements. In this paper, we explore an adaptive online learning strategy for real-time IoT botnet attack detection. Furthermore, we operate the proposed adaptive strategy in conjunction with online ensemble learning. To evaluate the proposed strategy, we use real IoT traffic data, including benign traffic data and botnet traffic data infected by Mirai. In real-time IoT botnet attack detection, our experimental results demonstrate that the proposed adaptive online learning strategy achieves remarkable performance.
Elsevier",High
"Vehicular Ad Hoc Network (VANET) facilitates the exchange of vehicular information through Vehicle-to-Vehicle (V2V) communication, contributing to Cooperative Intelligent Transportation Systems (C-ITS). The transmitted messages among vehicles are vulnerable to various security threats executed by malicious insider nodes. The dynamic VANET necessitates context-aware solutions for detecting various security attacks. Existing learning and deterministic mechanisms showed high detection accuracy for attacks on which they were trained explicitly for large datasets. Therefore, we propose an intelligent framework utilizing Deep Reinforcement Learning (DRL) for attack detection in evolving scenarios and mitigate the need for extensive training datasets. Our approach employs a Deep Q Network (DQN) trained on a compact dataset encompassing multiple attacks. The trained model is then applied to an unknown and extensive dataset, detecting various attacks with high accuracy. Notably, the model autonomously updates itself upon observing changes in the network context. This framework represents a promising security solution that is effective and adaptable for V2V communication in VANET.",Medium
"The difficulties about user security and privacy have appeared as significant concerns in recent years. The number of cyber-attacks grows at a concerning velocity, hence rendering internet users susceptible to malicious activities perpetrated by hackers. Data mining approaches are employed to extract accurate results from massive and complex databases. Furthermore, the utilization of Blockchain (BC) approaches is increasingly popular in current Internet of Things (IoT) applications as an opportunity to address issues related to privacy and security. Lots of studies have been performed on algorithms for data mining and techniques concerning blockchain. Time series data is a commonly used form of data. Time Series Classification (TSC) refers to the creation of predictive models that generate a target variable or label based on linear or sequential data inputs across a considerable duration. The possible results may be presented in either ordinal or numerical form. Even so, previous studies have shown major limitations when it comes to handling privacy and security issues that canâ€™t be applicable in dynamic instances, as well as the substantial computational cost necessary. Moreover, correctly determining the amount of sensitive parameters required to complete the classification process remains a challenge. We have put forth a comprehensive survey on the classification of blockchain data. In the first phase of our study, we conducted an analysis and categorization of both conventional data classification approaches and contemporary time series data classification techniques. We further discussed limitations and strengths of existing techniques. Finally, we highlight future research problems and directions.
Elsevier",Low
"Advanced Persistent Threats (APTs) are well-planned, persistent, and highly stealthy cyberattacks designed to steal confidential information or disrupt specific target systems. Recent studies have used system audit logs to construct provenance graphs that describe system interactions to detect potentially malicious activities. Although they are effective, they still suffer from problems such as the need for a priori knowledge, lack of attack data, and high computational overhead that limit their application. In this paper, we propose a self-supervised learning-based APT detection model, APT-MGL, which learns the embedded representations of nodes through a graph mask self-encoder and transforms the detection problem into an outlier detection problem for malicious nodes. APT-MGL characterizes the behavior of nodes based on node type, action, and interaction frequency, and fuses the features through a multi-head self-attention mechanism. Then the node embedding is obtained by combining graph features and structural information using masked graph representation learning. Finally, the unsupervised outlier detection method is used to analyze the computed embeddings and obtain the final detection results. The experimental results show that APT-MGL outperforms existing monitoring models and achieves a small overhead.
Elsevier",Low
"Federated learning-based network intrusion detection system (FL-based NIDS) has demonstrated tremendous potential in protecting the security of IoT network. It enables learning an effective intrusion detection model from massive traffic data collaboratively without data privacy leakage. However, FL-based NIDS has exhibited inherent vulnerabilities on the poisoning attacks launched by malicious clients. The poisoning attacks aim to corrupt the intrusion detection model and impair its protection capability, by injecting the poisoned traffic data into the local training dataset. We build a secure FL-based NIDS that is robust for the poisoning attacks, namely SecFedNIDS. Firstly, we propose the model-level defensive mechanism based on poisoned model detection. Specifically, we propose the gradient-based important model parameter selection method to provide the effective low-dimensional representations of the uploaded local model parameters, and then we propose the online unsupervised poisoned model detection method to identify the poisoned models and reject them to join in the global intrusion detection model. Subsequently, we design the data-level defensive mechanism based on poisoned data detection. Notably, we propose a novel poisoned data detection method based on class path similarity, to filter out the poisoned traffic data and avoid them participating in subsequent local training. We adopt layer-wise relevance propagation to extract the class path of clean traffic data, and transmit the class paths to the poisoned clients to help distinguish the poisoned traffic data. Results show that SecFedNIDS with the proposed model-level defense boosts the accuracy by up to 48% under the poisoning attacks on UNSW-NB15 dataset and 36% on CICIDS2018 dataset, and the proposed data-level defense further improves its accuracy by up to 13% on CICIDS2018 dataset.
Elsevier",Medium
"This paper presents a new framework for intrusion detection in the next-generation Internet of Things. MinMax normalization strategy is used to collect and preprocess data. The Marine Predator algorithm is then used to select relevant features to be used in the learning process. The selected features are then trained with an advanced and state-of-the-art recurrent neural network that includes an attention mechanism. Finally, Shapely values are calculated to determine how much each feature contributes to the final output. The dataset NSL-KDD was used for intensive simulations. The results show the advantages of the proposed system as well as its superiority over state-of-the-art methods. In fact, the proposed solution achieved a rate of more than 94% for both true negative and true position, while the rates of the existing solutions are below 90% for the challenging NSL-KDD datasets.
Elsevier",Low
"The impetuous expansion of the Internet of Things (IoT) network has resulted in a noticeable increase in the production of sensitive user data. With this, to meet the demand for real-time response, a processing layer is introduced near the user end which is known as the fog computing layer. The fog layer lies in the userâ€™s vicinity and thus highly attracts malicious and/or curious intruders. As a result, the trust of the network gets negatively impacted. Motivated by the aforementioned issue, the authors consider Reputation-based trust and propose a RepuTE Framework in the Fog-IoT domain. The given framework consists of a soft voting ensemble learning model that classifies and predicts two popular reputation-based attacks namely, DoS/ DDoS and Sybil attacks. Furthermore, a novel feature selection technique is also presented that selects the most relevant features well in advance. The performance evaluation is done on NSL-KDD, CICDDOS2019, IoTID20, NBaIoT2018, TON_IoT, and UNSW_NB15 benchmarked IoT and network traffic datasets. The comprehensive performance analysis depicts that the proposed model attains 99.99% accuracy and outperforms other recent state-of-the-art works. This indicates the potential of the proposed approach for reputation-based attack filtration in the IoT domain.
Elsevier",Low
"Malicious traffic detection is one of the most important parts of cyber security. The approaches of using the flow as the detection object are recognized as effective. Benefiting from the development of deep learning techniques, raw traffic can be directly used as a feature to detect malicious traffic. Most existing work usually converts raw traffic into images or long sequences to express a flow and then uses deep learning technology to extract features and classify them, but the generated features contain much redundant or even useless information, especially for encrypted traffic. The packet header field contains most of the packet characteristics except the payload content, and it is also an important element of the flow. In this paper, we only use the fields of the packet header in the raw traffic to construct the characteristic representation of the traffic and propose a novel flow-vector generation approach for malicious traffic detection. The preprocessed header fields are embedded as field vectors, and then a two-layer attention network is used to progressively generate the packet vectors and the flow vector containing context information. The flow vector is regarded as the abstraction of the raw traffic and is used to classify. The experiment results illustrate that the accuracy rate can reach up to 99.48% in the binary classification task and the average of AUC-ROC can reach 0.9988 in the multi-classification task.",Low
"Widespread communication by mobile devices today has promoted the use of huge amounts of information on them. Malware applications are undergoing exponential growth, directly attacking these smartphones to steal information. For this reason, we have created a methodology to analyze the network packets sent by any type of mobile applications in order to validate their behavior. In order to solve this problem, we have used supervised learning systems in an attempt to modelize the traffic communication with a set of previously labeled data. This will help to actively detect if applications installed in mobile devices could be tagged as malicious.
Elsevier",Low
"As the number of Internet-connected systems rises, cyber analysts find it increasingly difficult to effectively monitor the produced volume of data, its velocity and diversity. Signature-based cybersecurity strategies are unlikely to achieve the required performance for detecting new attack vectors. Moreover, technological advances enable attackers to develop sophisticated attack strategies that can avoid detection by current security systems. As the cyber-threat landscape worsens, we need advanced tools and technologies to detect, investigate, and make quick decisions regarding emerging attacks and threats. Applications of artificial intelligence (AI) have the potential to analyze and automatically classify vast amounts of Internet traffic. AI-based solutions that automate the detection of attacks and tackle complex cybersecurity problems are gaining increasing attention. This paper comprehensively presents the promising applications of deep learning, a subfield of AI based on multiple layers of artificial neural networks, in a wide variety of security tasks. Before critically and comparatively surveying state-of-the-art solutions from the literature, we discuss the key characteristics of representative deep learning architectures employed in cybersecurity applications, we introduce the emerging trends in deep learning, and we provide an overview of necessary resources like a generic framework and suitable datasets. We identify the limitations of the reviewed works, and we bring forth a vision of the current challenges of the area, providing valuable insights and good practices for researchers and developers working on related problems. Finally, we uncover current pain points and outline directions for future research to address them.
Elsevier",Low
"Over the past five years, interest in the literature regarding the security of the Internet of Medical Things (IoMT) has increased. Due to the enhanced interconnectedness of IoMT devices, their susceptibility to cyber-attacks has proportionally escalated. Motivated by the promising potential of AI-related technologies to improve certain cybersecurity measures, we present a comprehensive review of this emerging field. In this review, we attempt to bridge the corresponding literature gap regarding modern cybersecurity technologies that deploy AI techniques to improve their performance and compensate for security and privacy vulnerabilities. In this direction, we have systematically gathered and classified the extensive research on this topic. Our findings highlight the fact that the integration of machine learning (ML) and deep learning (DL) techniques improves both the performance of cybersecurity measures and their speed, reliability, and effectiveness. This may be proven to be useful for improving the security and privacy of IoMT devices. Furthermore, by considering the numerous advantages of AI technologies as opposed to their core cybersecurity counterparts, including blockchain, anomaly detection, homomorphic encryption, differential privacy, federated learning, and so on, we provide a structured overview of the current scientific trends. We conclude with considerations for future research, emphasizing the promising potential of AI-driven cybersecurity in the IoMT landscape, especially in patient data protection and in data-driven healthcare.
Elsevier",Low
"Anomaly detection in information (IP) networks, detection of deviations from what is considered normal, is an important complement to misuse detection based on known attack descriptions. Performing anomaly detection in real-time places hard requirements on the algorithms used. First, to deal with the massive data volumes one needs to have efficient data structures and indexing mechanisms. Secondly, the dynamic nature of today's information networks makes the characterisation of normal requests and services difficult. What is considered as normal during some time interval may be classified as abnormal in a new context, and vice versa. These factors make many proposed data mining techniques less suitable for real-time intrusion detection. In this paper we present ADWICE, Anomaly Detection With fast Incremental Clustering, and propose a new grid index that is shown to improve detection performance while preserving efficiency in search. Moreover, we propose two mechanisms for adaptive evolution of the normality model: incremental extension with new elements of normal behaviour, and a new feature that enables forgetting of outdated elements of normal behaviour. These address the needs of a dynamic network environment such as a telecom management network. We evaluate the technique for network-based intrusion detection, using the KDD data set as well as on data from a telecom IP test network. The experiments show good detection quality and act as proof of concept for adaptation of normality.",Medium
"Machine learning is significantly used for malware and adversary detection in the industrial internet of things networks. However, majority of these methods require a significant prior knowledge of malware properties to identify optimal features for malware detection. This is a more significant challenge in IoT environment due to limited availability of malware samples. Some researchers utilized data deformation techniques such as converting malware to images or music to generate features that can be used for malware detection. However, these processes can be time-consuming and require a significant amount of data. This paper proposes MalGan, a framework for detecting and generating new malware samples based on the raw byte code at the edge layer of the Internet of Things (IoT) networks. Convolutional Neural Network (CNN) was utilized to extract high-level features, and boundary-seeking Generative Adversarial Network technique was used to generate new malware samples. Thus, even with a few malware samples, a significant number of previously unseen malware samples are detectable with high accuracy. To capture the short-term and long-term dependency of features, we employed an attention-based model, a combination of CNN and Long Short Term Memory. The attention mechanism improves the modelâ€™s performance by increasing or decreasing attention to certain parts of the features. The proposed method is examined extensively using standard Windows and IoT malware datasets. The experimental results indicate that our proposed MalGan is the method of choice, as it offers a higher detection rate compared to the previous malware detection algorithms.
Elsevier",Low
"Over 5.44 billion people now use the Internet, making it a vital part of daily life, enabling communication, e-commerce, education, and more. However, this huge Internet connectivity also raises concerns about online privacy and security, particularly with the rise of malicious Uniform Resource Locators (URLs). Recently, conventional ensemble models have attracted attention due to their notable benefits of reducing the variance in models, enhancing predictive performance, improving prediction accuracy, and demonstrating high generalization potential. But, its application in addressing the challenge of malicious URLs is still an open problem. These URLs often hide behind static links in emails or web pages, posing a threat to individuals and organizations. Despite blacklisting services, many harmful sites evade detection due to inadequate scrutiny or recent creation. Hence, to improve URL detection, a Diverse and Efficient Ensemble (DaE2) machine learning algorithm was developed using four ensemble models, that is, AdaBoost, Bagging, Stacking, and Voting to classify URLs. After preprocessing, the experimental result shown that all models achieved over 80 % accuracy, with AdaBoost reaching 98.5 % and Stacking offering the fastest runtime. AdaBoost and Bagging also delivered strong performance, with F1 scores of 0.980 and 0.976, respectively.
Elsevier",Low
"Network intrusion detection system is an essential part of network security research. It detects intrusion behaviors through active defense technology and takes emergency measures such as alerting and terminating intrusions. With the rapid development of machine learning technology, more and more researchers apply machine learning algorithms to network intrusion detection to improve detection efficiency and accuracy. Due to the different principles of various algorithms, they also have their advantages and disadvantages. To construct the dominant algorithm model in the field of network intrusion detection and provide the accuracy value, this paper systematically combs the application literature of machine learning algorithms in intrusion detection in the past ten years. A review is made from three categories: traditional machine learning, ensemble learning, and deep learning. Then, this paper selects the KDD CUP99 and NSL-KDD datasets to conduct comparative experiments on decision trees, Naive Bayes, support vector machines, random forests, XGBoost, convolutional neural networks, and recurrent neural networks. The detection accuracy, F1, AUC, and other indicators of these algorithms on different data sets are compared. The experimental results show that the effect of the ensemble learning algorithm is generally better. The Naive Bayes algorithm has low accuracy in recognizing the learned data, but it has obvious advantages when facing new types of attacks, and the training speed is faster. The deep learning algorithm is not particularly prominent in this experiment, but its optimal results are affected by the structure, hyperparameters, and the number of training iterations, which need further in-depth study. Finally, the main challenges facing the current network intrusion detection field are summarized, and the future research directions have been prospected.
Elsevier",Low
"In the rapidly evolving domain of machine learning, the ability to adapt to unforeseen circumstances and novel data types is of paramount importance. The deployment of Artificial Intelligence is progressively aimed at more realistic and open scenarios where data, tasks, and conditions are variable and not fully predetermined, and therefore where a closed set assumption cannot be hold. In such evolving environments, machine learning is asked to be autonomous, continuous, and adaptive, requiring effective management of uncertainty and the unknown to fulfill expectations. In response, there is a vigorous effort to develop a new generation of models, which are characterized by enhanced autonomy and a broad capacity to generalize, enabling them to perform effectively across a wide range of tasks. The field of machine learning in open set environments poses many challenges and also brings together different paradigms, some traditional but others emerging, where the overlapping and confusion between them makes it difficult to distinguish them or give them the necessary relevance. This work delves into the frontiers of methodologies that thrive in these open set environments, by identifying common practices, limitations, and connections between the paradigms Open-Ended Learning, Open-World Learning, Open Set Recognition, and other related areas such as Continual Learning, Out-of-Distribution detection, Novelty Detection, and Active Learning. We seek to easy the understanding of these fields and their common roots, uncover open problems and suggest several research directions that may motivate and articulate future efforts towards more robust and autonomous systems.
Elsevier",Medium
"Malware increasingly threatens users around the world on a variety of cybernetic platforms, resulting in damages of billions of dollars each year. In recent years, in order to improve the detection capabilities of widely used antivirus (AV) tools, machine learning (ML) algorithms and dynamic malware analysis have been leveraged for the extraction and learning of rich multivariate time-series data (MTSD) associated with behavioral information. Such MTSD can be exploited using a time-interval temporal pattern (TP) mining approach, however this approach has not been widely explored for the task of malware detection. The use of TPs enables the discovery of complex temporal relations between different variables, improves the ability to cope with missing values and noisy data, and provides explainability. In light of the continuous creation of new unknown malware on a daily basis, detection mechanisms require frequent updating to keep pace with the changing reality. Active learning (AL) can address the updatability gap by efficiently selecting and acquiring a small yet informative set of new samples while reducing the labeling efforts of experts; AL also provides maximal improvement of machine-learning-based detection models, which can further contribute to the updatability of antimalware tools. However, the use of AL methods for the acquisition of time-interval TP-based samples has yet to be explored. In this paper, we present novel AL methods and a detection framework for improved malware detection based on dynamic analysis, time-interval TPs, and ML algorithms. The proposed framework is capable of both prioritizing the acquisition of malicious samples and improving the malware detection capabilities of ML classifiers and antimalware tools. Our proposed framework was evaluated in an extensive set of experiments on a comprehensive data collection of 9,328 portable executables (5,000 benign and 4,328 malicious) that were executed in the Windows 10 environment. The results demonstrated our AL methodsâ€™ ability to prioritize the acquisition of malware and managed to acquire up to 93.5% of the malicious files each day, allowing frequent updating of antimalware tools. In addition, our framework was shown to be effective in improving the detection capabilities of several ML classifiers over time, with the best results (AUC of 95.15%) achieved by the SVM classifier. Our framework also showed that TPs can be used to identify emerging trends in malicious behavior.
Elsevier",Medium
"Most conventional cyber defenses strive to reject detected attacks as quickly and decisively as possible; however, this instinctive approach has the disadvantage of depriving intrusion detection systems (IDSes) of learning experiences and threat data that might otherwise be gleaned from deeper interactions with adversaries. For IDS technology to improve, a next-generation cyber defense is proposed in which cyber attacks are unconventionally reimagined as free sources of live IDS training data. Rather than aborting attacks against legitimate services, adversarial interactions are selectively prolonged to maximize the defenderâ€™s harvest of useful threat intelligence. Enhancing web services with deceptive attack-responses in this way is shown to be a powerful and practical strategy for improved detection, addressing several perennial challenges for machine learning-based IDS in the literature, including scarcity of training data, the high labeling burden for (semi-)supervised learning, encryption opacity, and concept differences between honeypot attacks and those against genuine services. By reconceptualizing software security patches as feature extraction engines, the approach conscripts attackers as free penetration testers, and coordinates multiple levels of the software stack to achieve fast, automatic, and accurate labeling of live web streams.
Prototype implementations are showcased for two feature set models to extract security-relevant network- and system-level features from cloud services hosting enterprise-grade web applications. The evaluation demonstrates that the extracted data can be fed back into a network-level IDS for exceptionally accurate, yet lightweight attack detection.
Elsevier",Medium
"Intrusion Detection Systems (IDS) help protect computer networks by identifying and detecting attempts to obtain unauthorized access to data via computer networks by inspecting packets separately or in the context of flows. Intrusion detection is a classification task performed on continuously generated packets with a non-stationary distribution (data stream). As such, the decision models used for Intrusion Detection must be constantly updated to account for changes in what constitutes normal traffic of a network and correctly identify attacks. This work evaluates two approaches concerning label availability to update the intrusion detection classifier. First, we analyze the impact of delayed labeling of samples on the classifiers' performance, and second, we evaluate the impact of using active learning strategies on the classifiers. Our experimental evaluation uses two datasets (CIC-IDS2017 and CSE-CIC-IDS2018) to compare different data stream classification algorithms under different evaluation measures. Based on comparison results, we studied different active learning techniques to estimate the impact of delayed labeling on packet-based IDS. We found that the performance of the classifiers is inversely proportional to the label delivery rate. Besides, the active learning strategies helped keep the performance compatible with the baselines, even with a small set of labeled instances.
Elsevier",Medium
"With the development of information technology and economic growth, the Internet of Things (IoT) industry has also entered the fast lane of development. The IoT industry system has also gradually improved, forming a complete industrial foundation, including chips, electronic components, equipment, software, integrated systems, IoT services, and telecom operators. In the event of selective forwarding attacks, virus damage, malicious virus intrusion, etc., the losses caused by such security problems are more serious than those of traditional networks, which are not only network information materials, but also physical objects. The limitations of sensor node resources in the Internet of Things, the complexity of networking, and the open wireless broadcast communication characteristics make it vulnerable to attacks. Intrusion Detection System (IDS) helps identify anomalies in the network and takes the necessary countermeasures to ensure the safe and reliable operation of IoT applications. This paper proposes an IoT feature extraction and intrusion detection algorithm for intelligent city based on deep migration learning model, which combines deep learning model with intrusion detection technology. According to the existing literature and algorithms, this paper introduces the modeling scheme of migration learning model and data feature extraction. In the experimental part, KDD CUP 99 was selected as the experimental data set, and 10% of the data was used as training data. At the same time, the proposed algorithm is compared with the existing algorithms. The experimental results show that the proposed algorithm has shorter detection time and higher detection efficiency.
Elsevier",Low
"As the digital landscape expands rapidly due to technological advancements, cybersecurity concerns have become more prevalent. Intrusion Detection Systems (IDSs), which are crucial for identifying unusual network traffic indicative of malicious activity, have become a necessity. These systems can be either hardware or software-based. However, traditional IDS models often fail to adequately protect data privacy and detect complex, unique breaches, particularly within Wireless Sensor Networks (WSNs). To address these limitations, this paper proposes a novel Stacked Convolutional Neural Network and Bidirectional Long Short Term Memory (SCNN-Bi-LSTM) model for intrusion detection in WSNs. This model leverages Federated Learning (FL) to enhance intrusion detection performance and safeguard privacy. The FL-based SCNN-Bi-LSTM model is unique in its approach, allowing multiple sensor nodes to collaboratively train a central global model without revealing private data, thereby alleviating privacy concerns. The deep learning methodology of the SCNN-Bi-LSTM model effectively identifies sophisticated and previously unknown cyber threats by meticulously examining both local and temporal linkages in network patterns. The model has been specifically designed to detect and categorize different types of Denial of Service (DoS) attacks using specialized WSN-DS and CIC-IDS-2017 datasets. Compared to traditional Artificial Deep Neural Network (ADNN) models, our proposed FL-SCNN-Bi-LSTM model demonstrated superior detection rates for complex and unknown attacks, significantly improving IDS performance. The model achieved a notable classification accuracy of approximately 99.9% precision and recall on both datasets, substantially reducing false positives and negatives. Our research underscores the potential of federated learning and deep learning in enhancing the security and privacy of WSNs. The proposed FL-SCNN-Bi-LSTM architecture not only facilitates the identification of complex cyber threats but also exemplifies how deep learning techniques can be employed to bolster intrusion detection systems while preserving user data privacy.
Elsevier",Medium
"IPv6 over Low-powered Wireless Personal Area Networks (6LoWPAN) has grown in importance in recent years, with the Routing Protocol for Low Power and Lossy Networks (RPL) emerging as a major enabler. However, RPL can be subject to attack, with severe consequences. Most proposed IDSs have been limited to specific RPL attacks and typically assume a stationary environment. In this article, we propose the first adaptive hybrid IDS to efficiently detect and identify a wide range of RPL attacks (including DIO Suppression, Increase Rank, and Worst Parent attacks, which have been overlooked in the literature) in evolving data environments. We apply our framework to networks under various levels of node mobility and maliciousness. We experiment with several incremental machine learning (ML) approaches and various â€˜concept-drift detectionâ€™ mechanisms (e.g. ADWIN, DDM, and EDDM) to determine the best underlying settings for the proposed scheme.
Elsevier",Medium
"Todayâ€™s high-bandwidth networks require adaptive analyzing approaches to recognize the network variable behaviors. The analyzing approaches should be robust against the lack of prior knowledge and provide data to impose more complex policies. In this paper, ACoPE is proposed as an adaptive semi-supervised learning approach for complex-policy enforcement in high-bandwidth networks. ACoPE detects and maintains inter-flows relationships to impose complex-policies. It employs a statistical process control technique to monitor accuracy. Whenever the accuracy decreased, ACoPE considers it as a changed behavior and uses data from a deep packet inspection module to adapt itself with the change.
The performance of ACoPE in analyzing network traffic is evaluated through UNB ISCX VPN-nonVPN and UNB ISCX Tor-nonTor datasets. The performance is compared with 10 different stream and traditional classification algorithms. ACoPE outperforms the stream classifiers, with 95.92% accuracy, 86.21% precision, and 73.29% recall in VPN dataset, and with 81.12% accuracy, 73.59% precision, and 61.08% recall in Tor dataset. The effectiveness of ACoPE to address the main constraints in analyzing of high-bandwidth networks to enforce security policies, namely comprehensive processing and adaptive learning, are confirmed through three different scenarios. Efficiency and accuracy of ACoPE in real high-bandwidth networks are evaluated by a pilot study, which indicates its efficiency and accuracy in analyzing high-bandwidth networks.
Elsevier",Medium
"With the development of new technologies such as big data, cloud computing, and the Internet of Things, network attack technology is constantly evolving and upgrading, and network attack detection technology is forced to undergo corresponding iterative evolution. Three main problems are associated with these technologies: the automatic representation of heterogeneous and complex network traffic data, the uneven network attack samples, and the contradiction between the accuracy of the anomaly detection model and the continuous evolution of attacks. Researchers have proposed several network attack detection techniques based on deep learning to address these problems. This study reviews and analyzes the studies aimed at dealing with such problems, considering multiple factors, such as models, traffic representation and feature extraction, threat detection model training, and model robustness improvement. Finally, the existing problems and challenges associated with the current research are analyzed with respect to data category imbalance, high-dimensional massive data processing, concept distribution drift, real-time interpretability of the detection model, and the security of the model.
Elsevier",Medium
"Deep neural networks (DNNs) are currently being deployed as machine learning technology in a wide range of important real-world applications. DNNs consist of a huge number of parameters that require millions of floating-point operations (FLOPs) to be executed both in learning and prediction modes. A more effective method is to implement DNNs in a cloud computing system equipped with centralized servers and data storage sub-systems with high-speed and high-performance computing capabilities. This paper presents an up-to-date survey on current state-of-the-art deployed DNNs for cloud computing. Various DNN complexities associated with different architectures are presented and discussed alongside the necessities of using cloud computing. We also present an extensive overview of different cloud computing platforms for the deployment of DNNs and discuss them in detail. Moreover, DNN applications already deployed in cloud computing systems are reviewed to demonstrate the advantages of using cloud computing for DNNs. The paper emphasizes the challenges of deploying DNNs in cloud computing systems and provides guidance on enhancing current and new deployments.
Elsevier",Low
"Traffic classification is essential for network management and optimization, enhancing user experience, network performance, and security. However, evolving technologies and complex network environments pose challenges. Recently, researchers have turned to machine learning for traffic classification due to its ability to automatically extract and distinguish traffic features, outperforming traditional methods in handling complex patterns and environmental changes while maintaining high accuracy. Federated learning, a distributed learning approach, enables model training without revealing original data, making it appealing for traffic classification to safeguard user privacy and data security. However, applying it to this task poses two challenges. Firstly, common client devices like routers and switches have limited computing resources, which can hinder efficient training and increase time costs. Secondly, real-world applications often demand personalized models and tasks for clients, posing further complexities. To address these issues, we propose Split Federated Mutual Learning (SFML), an innovative federated learning architecture designed for traffic classification that combines split learning and mutual learning. In SFML, each client maintains two models: a privacy model for the local task and a public model for the global task. These two models learn from each other through knowledge distillation. Furthermore, by leveraging split learning, we offload most of the computational tasks to the server, significantly reducing the computational burden on the client. Experimental results demonstrate that SFML outperforms typical training architectures in terms of convergence speed, model performance, and privacy protection. Not only does SFML improve training efficiency, but it also satisfies the personalized needs of clients and reduces their computational workload and communication overhead, providing users with a superior network experience.
Elsevier",Medium
"Unmanned Aerial Vehicles (UAVs), also known as drones, have recently been proposed as flying base stations for providing reliable service to IoT devices. However, due to the lack of effective authentication schemes, UAVs are often hijacked by adversaries, which raises a high potential for sensitive information leakage. Therefore, designing a real-time authentication scheme is essential to enhance UAV safety. Up to the present, several works exist about pilot authentication by classifying radio-control signals. As propagating through the open environment, radio-control signals can be sniffed, analyzed, and simulated, posing significant threats to UAV security. For this reason, we propose a novel deep learning-based drone pilot authentication scheme, DP-Authentication, to protect UAVs from malicious radio-manipulated attacks. Specifically, we collect UAV flight data from the onboard PX4 flight stack and feed them into the authentication scheme to validate pilot legal status dynamically. As verified by comprehensive experiments, the proposed authentication scheme can authenticate pilots with an accuracy of 95.24% and detect malicious hijacking with an accuracy of 96.82%. Thanks to the low system overhead, it holds great promise for deployment on the UAV side to monitor pilot legal status in real-time.
Elsevier",Low
"The Zero-touch network and Service Management (ZSM) framework represents an emerging paradigm in the management of the fifth-generation (5G) and Beyond (5G+) networks, offering automated self-management and self-healing capabilities to address the escalating complexity and the growing data volume of modern networks. ZSM frameworks leverage advanced technologies such as Machine Learning (ML) to enable intelligent decision-making and reduce human intervention. This paper presents a comprehensive survey of the applications of the ZSM framework, covering network optimization, traffic monitoring, energy efficiency, and security aspects of next-generation networks. The paper explores the challenges associated with ZSM, particularly those related to ML, which necessitate the need to explore diverse network automation solutions. In this context, the study investigates the application of Automated ML (AutoML) within a ZSM framework, to reduce network management costs and enhance performance. AutoML automates the selection and tuning process of a ML model for a given task. Specifically, the focus is on AutoMLâ€™s ability to predict application throughput and autonomously adapt to data drift. Experimental results demonstrate the superiority of the proposed AutoML pipeline over traditional ML in terms of prediction accuracy. Integrating AutoML and ZSM concepts significantly reduces network configuration and management efforts, allowing operators to allocate more time and resources to other important tasks. The paper also provides a high-level 5G system architecture incorporating AutoML and ZSM concepts. This research highlights the potential of ZSM and AutoML to revolutionize the management of 5G+ networks, enabling automated decision-making and empowering network operators to achieve higher efficiency, improved performance, and enhanced user experience.
Elsevier",Medium
"Deep neural networks have advanced significantly in the last several years and are now widely employed in numerous significant real-world applications. However, recent research has shown that deep neural networks are vulnerable to backdoor attacks. Under such attacks, attackers release backdoor models that achieve satisfactory performance on benign samples while behaving abnormally on inputs with predefined triggers. Successful backdoor attacks can have serious consequences, such as attackers using backdoor generation methods to bypass critical face recognition authentication systems. In this paper, we propose PBADT, a precise backdoor attack with dynamic trigger. Unlike existing work that uses static or random trigger masks, we design an interpretable trigger mask generation framework that places triggers at positions that have the most significant impact on the prediction results. Meanwhile, backdoor attacks are made more efficient by using forgettable events to improve the efficiency of backdoor attacks. The proposed backdoor method is extensively evaluated on three face recognition datasets, LFW, CelebA, and VGGFace, while further evaluated on two general image datasets, CIFAR-10 and GTSRB. Our approach achieves almost perfect attack performance on backdoor data.
Elsevier",Low
"In recent years, as the boundaries of computing have expanded with the emergence of the Internet of Things (IoT) and its increasing number of devices continuously producing flows of data, it has become paramount to boost speed and to reduce latency. Recent approaches to this growing complexity and data deluge aim to integrate seamlessly and securely diverse computing tiers and data environments, spanning from core cloud to edge - the Computing Continuum (or Edge-to-Cloud Continuum). Typically, the cloud is used for resource-intensive computations while the edge for low-latency tasks.
This provides an opportunity to run complex AI-enabled applications across multiple tiers specifically facilitating the training of Machine Learning (ML) models at the â€œedgeâ€ of the Internet (i.e., beyond centralized computing facilities such as cloud datacenters). Federated Learning (FL) represents a novel ML paradigm for collaborative training, capitalizing on processing capabilities at the edge for training purposes while addressing privacy concerns. A set of clients (i.e., edge devices) collaboratively train a shared model under the supervision of a centralized server without exchanging personal data. However, several challenges arise from the decentralized nature of FL in the Computing Continuum context: statistical heterogeneity (data drift between parties), system heterogeneity (due to the nature of the environment), volatility (e.g., client dropouts), security threats and, persistently, privacy (although no personal data is transmitted, the shared model updates include information about data used for training).
As opposed to previous studies dedicated to federated learning (typically on homogeneous, edge-based infrastructures), this survey aims to present a systematic overview of the existing literature addressing how state-of-the-art Federated Learning (FL) systems contend with the challenges previously outlined within the edge-to-cloud Computing Continuum, in particular heterogeneity, volatility and large-scale distribution. We analyze representative tools for implementing, monitoring, configuring and deploying such systems. We highlight significant efforts made to overcome statistical heterogeneity and security problems in FL. We specifically analyze the quality of the experimental evaluation done for existing systems and the relevant benchmarks. Finally, we discuss some open issues and future directions (e.g., lack of experiments in realistic environments) to support the broader adoption of FL across the continuum and to eventually fulfill the vision of the AI and edge computing convergence - the edge intelligence.
Elsevier",Medium
"The Internet of Things (IoT) refers to a network of Internet-enabled devices that can make different operations, like sensing, communicating, and reacting to changes arising in the surrounding environment. Nowadays, the number of IoT devices is already higher than the world population. These devices operate by exchanging data between them, sometimes through an intermediate cloud infrastructure, and may be used to enable a wide variety of novel services that can potentially improve the quality of life of billions of people. Nonetheless, all that glitters is not gold: the increasing adoption of IoT comes with several privacy concerns due to the lack or loss of control over the sensitive data exchanged by these devices. This represents a key challenge for software engineering researchers attempting to address those privacy concerns by proposing (semi-)automated solutions to identify sources of privacy leaks. In this respect, a notable trend is represented by the adoption of smart solutions, that is, the definition of techniques based on artificial intelligence (AI) algorithms. This paper proposes a systematic literature review of the research in smart detection of privacy concerns in IoT devices. Following well-established guidelines, we identify 152 primary studies that we analyze under three main perspectives: (1) What are the privacy concerns addressed with AI-enabled techniques; (2) What are the algorithms employed and how they have been configured/validated; and (3) Which are the domains targeted by these techniques. The key results of the study identified six main tasks targeted through the use of artificial intelligence, like Malware Detection or Network Analysis. Support Vector Machine is the technique most frequently used in literature, however in many cases researchers do not explicitly indicate the domain where to use artificial intelligence algorithms. We conclude the paper by distilling several lessons learned and implications for software engineering researchers.
Elsevier",Low
"Ensemble learningâ€“the application of multiple learning models on the same taskâ€“is a common technique in multiple domains. While employing multiple models enables reaching higher classification accuracy, this process can be time consuming, costly, and make scaling more difficult. Given that each model may have different capabilities and costs, assigning the most cost-effective set of learners for each sample is challenging. We propose SPIREL, a novel method for cost-effective classification. Our method enables users to directly associate costs to correct/incorrect label assignment, computing resources and run-time, and then dynamically establishes a classification policy. For each analyzed sample, SPIREL dynamically assigns a different set of learning models, as well as its own classification threshold. Extensive evaluation on two large malware datasetsâ€“a domain in which the application of multiple analysis tools is commonâ€“demonstrates that SPIREL is highly cost-effective, enabling us to reduce running time byâˆ¼ 80% while decreasing the accuracy and F1-score by only 0.5%. We also show that our approach is both highly transferable across different datasets and adaptable to changes in individual learning model performance.
Elsevier",Medium
"Cybersecurity incident response is a very crucial part of the cybersecurity management system. Adversaries emerge and evolve with new cybersecurity tactics, techniques, and procedures (TTPs). It is essential to detect the TTPs in a timely manner to respond effectively and mitigate the vulnerabilities to secure business operations. This research focuses on TTP identification and detection based on a machine learning approach. Early identification and detection are paramount in protecting, responding to, and recovering from such adversarial attacks. Analyzing use cases is a critical tool to ensure proper and in-depth evaluation of sector-specific cybersecurity challenges. In this regard, this study investigates existing known methodologies for cyber-attacks such as Mitre attacks, and developed a method for identifying threat cases. In addition, Windows-based threat cases are implemented, comprehensive datasets are generated, and supervised machine learning models are applied to detect threats effectively and efficiently. Random forest outperforms other models with the highest accuracy of 99%. Future work can be done for generating threat cases based on multiple log sources, including network security and endpoint protection device, and achieve high accuracy by removing false positives using machine learning. Similarly, real-time threat detection is also envisioned for future work.
Elsevier",Low
"Recently, there has been a substantial interest in on-device Machine Learning (ML) models to provide intelligence for the Internet of Things (IoT) applications such as image classification, human activity recognition, and anomaly detection. Traditionally, ML models are deployed in the cloud or centralized servers to take advantage of their abundant computational resources. However, sharing data with the cloud and third parties degrades privacy and may cause propagation delay in the network due to a large amount of transmitted data impacting the performance of real-time applications. To this end, deploying ML models on-device (i.e., on IoT devices), in which data does not need to be transmitted, becomes imperative. However, deploying and running ML models on already resource-constrained IoT devices is challenging and requires intense energy consumption. Numerous works have been proposed in the literature to address this issue. Although there are considerable works that discuss energy-aware ML approaches for on-device implementation, there remains a gap in the literature on a comprehensive review of this subject. In this paper, we provide a review of existing studies focusing on-device ML models for IoT applications in terms of energy consumption. One of the key contributions of this study is to introduce a taxonomy to define approaches for employing energy-aware on-device ML models on IoT devices in the literature. Based on our review in this paper, our key findings are provided and the open issues that can be investigated further by other researchers are discussed. We believe that this study will be a reference for practitioners and researchers who want to employ energy-aware on-device ML models for IoT applications.",Low
"Mobile Edge Computing (MEC) is a key technology that emerged to address the increasing computational demands and communication requirements of vehicular networks. It is a form of edge computing that brings cloud computing capabilities closer to end-users, specifically within the context of vehicular networks, which are part of the broader Internet of Vehicles (IoV) ecosystem. However, the dynamic nature of traffic flows in MEC in 5G/6G vehicular networks poses challenges for accurate prediction and resource allocation when aiming to provide edge service for mobile vehicles. In this paper, we present a novel approach to predict the traffic flow of MEC in 5G/6G vehicular networks using graph-based learning. In our framework, MEC servers in vehicular networks are construed as nodes to construct a dynamic similarity graph and a dynamic transition graph over a duration of multiple days. We utilize Graph Attention Networks (GAT) to learn and fuse the node embeddings of these dynamic graphs. A transformer model is subsequently employed to predict the vehicle frequency accessing the edge computing services for the next day. Our experimental results have shown that the model achieves high accuracy in predicting edge service access volumes with low error metrics.
Elsevier",Low
"Due to data collection, there is a potential risk concerning security and privacy, so IoT reliability and survivability are of utmost concern. In this paper, we address the concern using two methods. The first method is device identification, which uses an extensive set of machine learning algorithms for identifying IoT devices. The algorithms include Logistic Regression, K- Nearest Neighbour, Support Vector Classifier, Random Forest, Gradient Boosting, AdaBoost, Light Gradient Boosting Machine, Extreme Gradient Boosting Convolution Neural Networks, and Long Short Term Memory are used for device identification. The performance of these models has been evaluated using multiple statistical measures, and weobserve that LSTM outperforms all other baseline models. The second method proposed for ensuring the survivability of theIoT environment is a blockchain-based architecture for smart homes to ensuretransparency and data protection.
Elsevier",Low
"This paper delves into the transformative role of machine learning (ML) techniques in revolutionizing the security of electric and flying vehicles (EnFVs). By exploring key domains such as predictive maintenance, cyberattack detection, and intelligent decision-making, the study uncovers pivotal insights that will shape the future of this technology.
From a theoretical perspective, ML emerges as a cornerstone for fortifying EnFV safety, offering real-time threat detection, predictive maintenance capabilities, and enhanced anomaly detection. In practical terms, ML-based solutions are envisioned as instrumental in preventing cyberattacks, reducing downtime, and improving overall safety.
The research contributions of this study encompass a comprehensive overview of ML applications in EnFV security, identification of challenges, and paving the way for future research directions. While acknowledging research limitations, particularly the need for real-world implementation, the study emphasizes the crucial yet underexplored ethical considerations in ML for EnFV security. Future research suggestions focus on Explainable AI techniques, real-time ML algorithms for resource-constrained environments, and privacy-preserving ML techniques, aiming for a transparent, efficient, and privacy-aware integration of ML in EnFV security. By addressing key security challenges, ML can potentially revolutionize the EnFV domain, paving the way for a future of efficient, sustainable, and connected transportation systems.
Elsevier",Low
"Software-Defined Network (SDN) is a perception of networking that separates the network into different layers responsible for applications, control, and data, thus providing a flexible and programmable architecture and making the network more reliable. However, from this centralization of network management, one of the main threats to SDNs remains Denial of Service and Distributed Denial of Service (DoS/DDoS) attacks, which are ever more developed and sophisticated nowadays.
Despite the several works found in this field, notably reviews and surveys, and given the significant recourse for using SDNs, at the same time, the expansion of DoS/DDoS attacks, which are increasingly imposing and destructive. We believe that a good understanding of DoS/DDoS attacks will help us confront these cyber threats to better counter them according to the SDN context.
This paper aims to analyze the DoS/DDoS attacks targeting the different layers and bounds of the SDN; it also attempts to provide an overview and summary of the current state-of-the-art in detecting and mitigating DoS/DDoS attacks over SDN environments and propose a classification of these solutions. Additionally, this work discusses the challenges and possible future research directions of DoS/DDoS attacks in SDN infrastructures and shares our vision issues related to this area.
Elsevier",Low
"Exploitability prediction has become increasingly important in cybersecurity, as the number of disclosed software vulnerabilities and exploits are soaring. Recently, machine learning and deep learning algorithms, including Support Vector Machine (SVM), Decision Tree, deep Neural Networks and their ensemble models, have achieved great success in vulnerability evaluation and exploitability prediction. However, they make a strong assumption that the data distribution is static over time and therefore fail to consider the concept drift problems due to the evolving system behaviours. In this work, we propose a novel consecutive batch learning algorithm, called Real-time Dynamic Concept Adaptive Learning (RDCAL), to deal with the concept drift and dynamic class imbalance problems existing in exploitability prediction. Specifically, we develop a Class Rectification Strategy (CRS) to handle the â€˜actual driftâ€™ in sample labels and a Balanced Window Strategy (BWS) to boost the minority class during real-time learning. Experimental results conducted on the real-world vulnerabilities collected between 1988 to 2020 show that the overall performance of classifiers, including Neural Networks, SVM, HoeffdingTree and Logistic Regression (LR), improves over 3% by adopting our proposed RDCAL algorithm. Furthermore, RDCAL achieves state-of-the-art performance on exploitability prediction compared with other concept drift algorithms.
Elsevier",Medium
"Because of the increasing application of reinforcement learning (RL), particularly deep Q-learning algorithm, research organizations utilize it with increasing frequency. The prediction of cyber vulnerability and development of efficient real-time online network intrusion detection (NID) systems are progressions toward becoming RL-powered. An open issues in NID is the model design and prediction of real-time online data composed of a series of time-related feature patterns. There have been concerns regarding the operation of the developed systems because cyber-attack scenarios vary continuously to circumvent NID. These issues have been related to the human interaction significance and the decrease in accuracy verification. Therefore, we employ an RL that permits a deep auto-encoder in the Q-network (DAEQ-N). The proposed DAEQ-N attempts to achieve the maximum prediction accuracy in online learning systems into which continuous behavior patterns are fed and which are trained with more significant weights by classifying it as either â€œnormalâ€ or â€œanomalous.â€
Elsevier",Medium
"This paper presents an adaptive network intrusion detection (ANID) method based on the selective ensemble of kernel extreme learning machines (KELMs) with random features (termed ANID-SEoKELM), aiming at identifying various unauthorized uses, misuses and abuses of computer systems in real time. To generate a lightweight intrusion detector, multiple KELMs are learned independently based on the Bagging strategy with sparse random feature representation (SRFR), to reduce noise and redundant or irrelevant information in network connection instances and ensure the diversity of base learners for the effective ensemble of base learners. A marginal distance minimization (MDM)-based selective ensemble (MDMbSE) method is introduced to generate the ultimate intrusion detector. To ensure the adaptability of the intrusion detector, an incremental learning-based detection-model updating procedure is also derived. Extensive validation and comparative experiments on the benchmark KDD99 dataset and a hybrid heterogeneous network simulation platform mixed with wireless networks and Ethernet networks demonstrate that the ANID-SEoKELM is able to adapt to the dynamically changing network environments hence it can achieve higher detection accuracies stably and efficiently than classic single learner-based intrusion detection methods and representative ensemble-based intrusion detection methods.
Elsevier",Medium
"Intrusion Detection Systems (IDS) play a crucial role in securing computer networks against malicious activities. However, their efficacy is consistently hindered by the persistent challenge of class imbalance in real-world datasets. While various methods, such as resampling techniques, ensemble methods, cost-sensitive learning, data augmentation, and so on, have individually addressed imbalance classification issues, there exists a notable gap in the literature for effective hybrid methodologies aimed at enhancing IDS performance. To bridge this gap, our research introduces an innovative methodology that integrates hybrid undersampling and oversampling strategies within an ensemble classification framework. This novel approach is designed to harmonize dataset distributions and optimize IDS performance, particularly in intricate multi-class scenarios. In-depth evaluations were conducted using well-established intrusion detection datasets, including the Car Hacking: Attack and Defense Challenge 2020 (CHADC2020) and IoTID20. Our results showcase the remarkable efficacy of the proposed methodology, revealing significant improvements in precision, recall, and F1-score metrics. Notably, the hybrid-ensemble method demonstrated an exemplary average F1 score exceeding 98% for both datasets, underscoring its exceptional capability to substantially enhance intrusion detection accuracy. In summary, this research represents a significant contribution to the field of IDS, providing a robust solution to the pervasive challenge of class imbalance. The hybrid framework not only strengthens IDS efficacy but also illuminates the seamless integration of undersampling and oversampling within ensemble classifiers, paving the way for fortified network defenses.
Elsevier",Low
"The Internet of Medical Things (IoMT) has revolutionized the healthcare industry by enabling physiological data collection using sensors, which are transmitted to remote servers for continuous analysis by physicians and healthcare professionals. This technology offers numerous benefits, including early disease detection and automatic medication for patients with chronic illnesses. However, IoMT technology also presents significant security risks, such as violating patient privacy or exposing sensitive data to interception attacks due to wireless communication, which could be fatal for the patient. Additionally, traditional security measures, such as cryptography, are challenging to implement in medical equipment due to the heterogeneous communication and their limited computation, storage, and energy capacity. These protection methods are also ineffective against new and zero-day attacks. It is essential to adopt robust security measures to ensure data integrity, confidentiality, and availability during data collection, transmission, storage, and processing. In this context, using Intrusion Detection Systems (IDS) based on Machine Learning (ML) can bring a complementary security solution adapted to the unique characteristics of IoMT systems. Therefore, this paper investigates how IDS based on ML can address security and privacy issues in IoMT systems. First, the generic three-layer architecture of IoMT is provided, and the security requirements of IoMT systems are outlined. Then, the various threats that can affect IoMT security are identified, and the advantages, disadvantages, methods, and datasets used in each solution based on ML at the three layers that make up IoMT are presented. Finally, the paper discusses the challenges and limitations of applying IDS based on ML at each layer of IoMT, which can serve as a future research direction.
Elsevier",Low
"With the wide spread of sensors and smart devices in recent years, the data generation speed of the Internet of Things (IoT) systems has increased dramatically. In IoT systems, massive volumes of data must be processed, transformed, and analyzed on a frequent basis to enable various IoT services and functionalities. Machine Learning (ML) approaches have shown their capacity for IoT data analytics. However, applying ML models to IoT data analytics tasks still faces many difficulties and challenges, specifically, effective model selection, design/tuning, and updating, which have brought massive demand for experienced data scientists. Additionally, the dynamic nature of IoT data may introduce concept drift issues, causing model performance degradation. To reduce human efforts, Automated Machine Learning (AutoML) has become a popular field that aims to automatically select, construct, tune, and update machine learning models to achieve the best performance on specified tasks. In this paper, we conduct a review of existing methods in the model selection, tuning, and updating procedures in the area of AutoML in order to identify and summarize the optimal solutions for every step of applying ML algorithms to IoT data analytics. To justify our findings and help industrial users and researchers better implement AutoML approaches, a case study of applying AutoML to IoT anomaly detection problems is conducted in this work. Lastly, we discuss and classify the challenges and research directions for this domain.",Medium
"Modern networks generate a massive amount of traffic data streams. Analyzing this data is essential for various purposes, such as network resources management and cyber-security analysis. There is an urgent need for data analytic methods that can perform network data processing in an online manner based on the arrival of new data. Online machine learning (OL) techniques promise to support such type of data analytics. In this paper, we investigate and compare the OL techniques that facilitate data stream analytics in the networking domain. We also investigate the importance of traffic data analytics and highlight the advantages of online learning in this regard, as well as the challenges associated with OL-based network traffic stream analysis, e.g.,Â concept drift and the imbalanced classes. We review the data stream processing tools and frameworks that can be used to process such data online or on-the-fly along with their pros and cons, and their integrability with de facto data processing frameworks. To explore the performance of OL techniques, we conduct an empirical evaluation on the performance of different ensemble- and tree-based algorithms for network traffic classification. Finally, the open issues and the future directions in analyzing traffic data streams are presented. This technical study presents valuable insights and outlook for the network research community when dealing with the requirements and purposes of online data streams analytics and learning in the networking domain.
Elsevier",Medium
"Digital Twins (DTs), being the virtual replicas of their physical counterparts, share valuable knowledge of the underlying physical processes and act as data acquisition and dissemination sources to Cyber–Physical System (CPS). Moreover, without obstructing the ongoing operations, DTs also provide an assessment platform for evaluating the operational behavior and security of the CPS. Therefore, they become a potential source of data breaches and a broad attack surface for attackers to launch covert attacks. To detect and mitigate security loopholes in DTs, one of the potential solutions is to leverage a gamification approach that can assess the security level of DTs while providing security analysts with a controlled and supportive virtual training environment. Artificial Intelligence/Machine Learning (AI/ML)-based approaches can complement the idea of security orchestration and automation in the gamification approach. However, AI/ML-based DTs security solutions are generally constrained by the lack of transparency of AI operations, which results in less confidence in the decisions made by the AI models. To address the explainable security challenges of DTs, this article proposes a gamification approach called sEcuriNg dIgital twins through GaMification Approach (ENIGMA). While leveraging DTs as an offensive security platform, ENIGMA provides gaming scenarios to assess DTs’ security and train security analysts. The game players within ENIGMA are humans (the attacker team) and AI agents (the defender team). Furthermore, ENIGMA is supported by an eXplainable AI (XAI)-based DT security assessment model that explains the decisions made based on the SHAP values by the AI model on attack vectors for the defender team, i.e., the AI agent. The SHAP values illustrate the contribution of different features towards predicting the outcome of attack vectors. This explanation can help security analysts to take security measures based on reasoned and trustworthy decisions. Finally, experimental validation has been carried out to demonstrate the viability of ENIGMA.",Low
"Open Shortest Path First (OSPF) is one of the most widely used intra-domain routing protocols. Unfortunately, it has many serious security issues. Falsification over OSPF is one of the most critical vulnerabilities that can cause routing loops and a black hole. In this paper, we introduce a novel approach by using a technique from non-linear statistical analysis to identify OSPF attacks. Firstly, we evaluate the capability of the non-linear technique to identify OSPF attacks using a controlled testbed where we introduce different types of LSA falsifications. Secondly, we evaluate our approach to detect different types of OSPF attacks using OSPF traffic associated with a single OSPF router and OSPF traffic associated with a set of OSPF routers. In both cases, our approach can detect anomalous behaviour quickly. Finally, we use various successful machine learning classifiers to analyze the outputs obtained from the non-linear analysis and calibrate their suitability in discovering such anomalies.
Elsevier",Low
"Broad Learning System (BLS) is a new deep learning model proposed recently, which shows its effectiveness in many fields, such as image recognition and fault detection. In this paper, we propose a secure, efficient, and verifiable outsourcing algorithm for BLS. This algorithm enables resource constrained devices to outsource BLS algorithm to untrusted cloud server to complete model training, which is of great significance for the promotion and application of BLS algorithm. Compared with the original BLS algorithm, this algorithm not only improves the efficiency of the algorithm on the client, but also ensures that the sensitive information of the client will not be leaked to the cloud server. In addition, in our algorithm, the client can verify the correctness of returned results with a probability of almost 1. Finally, we analyze the security and efficiency of our algorithm in theory and prove our algorithms feasibility through experiments.
Elsevier",Low
"Network embedding, which aims to generate dense, low-dimensional and representative embedding representations for all nodes in the network, is a crucial step for various AI-based tasks related to network analytics, such as node classification, community detection, and link prediction. In addition to network topology, node attributes are also easily accessible and ubiquitous information that serve as an important role in network embedding. How to jointly incorporate these two kinds of information into unified embedding representations is becoming one of the research focuses on network embedding. However, most existing methods are based on matrix factorization, which is not suitable for relatively large-scale datasets due to their quadratic time and space complexity. Moreover, most real networks evolve over time with varying topological structure or node attributes. This aspect requires the network embedding method to support efficient updating of embedding representations with respect to the evolution of the network. To address these two challenges, we propose a scalable attribute-aware network embedding (SANE) method to efficiently learn the joint embedding representations from topology and attributes. By enforcing the alignment of a locally linear relationship between each node and its K-nearest neighbors in topology and attribute space, the joint embedding representations provide more informative than a single representation extracted from topology or attributes alone. In addition, we devise incremental SANE to support updates of embedding representations in a dynamic environment. Several experiments are conducted on various datasets, demonstrating the effectiveness and scalability of the proposed method.
Elsevier",Medium
"Security is becoming a major concern for many mission-critical applications wireless sensor networks (WSNs) are envisaged to support. The inherently vulnerable characteristics of WSNs appoint them susceptible to various types of attacks. This work restrains its focus on how to defend against a particularly harmful form of attack, the Sybil attack. Sybil attacks can severely deteriorate the network performance and compromise the security by disrupting many networking protocols. This paper presents a rule-based anomaly detection system, called RADS, which monitors and timely detects Sybil attacks in large-scale WSNs. At its core, the proposed expert system relies on an ultra-wideband (UWB) ranging-based detection algorithm that operates in a distributed manner requiring no cooperation or information sharing between the sensor nodes in order to perform the anomaly detection tasks. The feasibility of the proposed approach is proven analytically, while the performance of RADS in exposing Sybil attacks is extensively assessed both mathematically and numerically. The obtained results demonstrate that RADS achieves high detection accuracy and low false alarm rate appointing it a promising ADS candidate for this class of wireless networks.
Elsevier",Low
"The Internet of Things (IoT) consists of heterogeneous devices such as wearables and monitoring devices that collect data to provide autonomous decision-making and smart applications. IoT technologies, such as the Internet of Medical Things (IoMT), have become gradually popular for medical purposes, combining IoT and medical devices to achieve good health and well-being. However, IoMT devices are often tight and have resource constraints, which leads to limited local data processing in the device. Edge computing provides access to additional computation and storage resources for IoMT devices, bringing intelligent processing closer to the data sources. This technology opens up great possibilities for IoMT applications, especially when combined with Artificial Intelligence (AI). Edge AI runs AI computations close to the IoT devices and users instead of centralized services such as cloud servers. This paper investigates the potential of Edge AI and IoMT. In this sense, this survey is the first work to further detail Edge AI and Machine Learning Operations in IoMT domains and wearable technology, thus contributing to the literature by comprehensively exploring the potential of ML strategies and operations at the networkâ€™s edge and intelligence distribution. This study also presents a case study on heart anomaly detection.
Elsevier",Low
"A Cyber-Physical System (CPS) integrates physical devices (i.e., sensors) with cyber (i.e., informational) components to form a context sensitive system that responds intelligently to dynamic changes in real-world situations. A core element of CPS is the collection and assessment of information from noisy, dynamic, and uncertain physical environments that must be transformed into usable knowledge in real-time. Machine learning algorithms such as cluster analysis can be used to extract useful information and patterns from data generated from physical devices based on which novel applications of CPS can make informed decisions. In this paper we propose to use a density-based data stream clustering algorithm, built on the Multiple Species Flocking model, for the monitoring of big data, generated from numerous applications such as machine monitoring, health monitoring, sensor networks. In the proposed approach, approximate results are available on demand at anytime, so it is particularly apt for real life monitoring applications.
Elsevier",Medium
"Term representation methods as computable and semantic tools have been widely applied in social network analysis. This paper provides a new perspective that can incrementally factorize co-occurrence matrix to query latest semantic vectors. We divide the streaming social network data into old and updated training tasks respectively, and factorize the training objective function based on stochastic gradient methods to update vectors. We prove that the incremental objective function is convergent. Experimental results demonstrate that our incremental factorizing can save a substantial amount of time by speeding up training convergence. The smaller the updated data is, the faster the update factorizing process can be, even 30 times faster than existing methods in certain cases. To evaluate the correctness of incremental representation, social text similarity/relatedness, linguistic tasks, network event detection, social user multi-label classification and user clustering for social network analysis are employed as benchmarks in this paper.
Elsevier",Medium
"In the context of rapidly urbanizing smart cities reliant on IoT networks, efficient load management is critical for sustainable energy use. This paper proposes an AI-enhanced Multi-Stage Learning-to-Learning (MSLL) approach tailored for secure load management in IoT networks. The proposed approach leverages MMStransformer, a transformer-based model designed to handle multivariate, correlated data, and to capture long-range dependencies inherent in load forecasting. MMStransformer employs a multi-mask learning-to-learning strategy, optimizing computational efficiency without compromising prediction accuracy. The study addresses the dynamic and complex nature of smart city data by integrating diverse environmental and operational variables. Security and privacy concerns inherent in IoT networks are also addressed, ensuring secure data handling and communication. Experimental results demonstrate the efficacy of the proposed approach, achieving competitive performance compared to traditional methods and baseline models. The findings highlight the potential of AI-driven solutions in enhancing load forecasting accuracy while ensuring robust security measures in smart city infrastructures. This research contributes to advancing the state-of-the-art in AI applications for sustainable urban development and energy management.
Elsevier",Medium
"Artificial intelligence (AI) is a powerful technology that helps cybersecurity teams automate repetitive tasks, accelerate threat detection and response, and improve the accuracy of their actions to strengthen the security posture against various security issues and cyberattacks. This article presents a systematic literature review and a detailed analysis of AI use cases for cybersecurity provisioning. The review resulted in 2395 studies, of which 236 were identified as primary. This article classifies the identified AI use cases based on a NIST cybersecurity framework using a thematic analysis approach. This classification framework will provide readers with a comprehensive overview of the potential of AI to improve cybersecurity in different contexts. The review also identifies future research opportunities in emerging cybersecurity application areas, advanced AI methods, data representation, and the development of new infrastructures for the successful adoption of AI-based cybersecurity in today's era of digital transformation and polycrisis.
Elsevier",Low
"5G brought an evolution on the network architecture employing the service-based paradigm, enabling flexibility in realizing customized services across different technology domains. Such paradigm gives rise to the adoption of analytics and Artificial Intelligence/Machine Learning (AI/ML) in mobile communications with the ease of collecting various measurements related to end-users and the network, which can be exposed towards consumers, including 3rd party applications. AI/ML may influence network planning and optimization considering the service life-cycle and introduce new operations provision, paving the way towards 6G. This article provides a survey on AI/ML considering the business, the fundamentals and algorithms across the radio, control, and management planes. It sheds light on the key technologies that assist the adoption of AI/ML in 3rd Generation Partnership Project (3GPP) networks considering service request, reporting, data collection and distribution and it overviews the main AI/ML algorithms characterizing them into user-centric and network-centric. Finally, it explores the main standardization and open source activities on AI/ML, highlighting the lessons learned and the further challenges that still need to be addressed to reap the benefits of AI/ML in automation for beyond 5G/6G mobile systems.
Elsevier",Low
"Smart cities result from integrating advanced technologies and intelligent sensors into modern urban infrastructure. The Internet of Things (IoT) and data integration are pivotal in creating interconnected and intelligent urban spaces. In this literature review, we explore the different methods of information fusion used in smart cities, along with their advantages and challenges. However, there are notable challenges in managing diverse data sources, handling large data volumes, and meeting the near-real-time demands of various smart city applications.
The review aims to examine smart city applications in detail, incorporating quality evaluation and information fusion techniques and identifying critical issues while outlining promising research directions. In order to accomplish our goal, we conducted a comprehensive search of literature and applied selective criteria. We identified 59 recent studies addressing machine learning (ML) and deep learning (DL) techniques in smart city applications. These studies were obtained from various databases such as ScienceDirect (SD), Scopus, Web of Science (WoS), and IEEE Xplore. The main objective of this study is to provide more detailed insights into smart cities by supplementing existing research. The word cloud visualisation of machine learning/deep learning and information fusion in smart cities papers shows a diverse landscape, covering both technical aspects of artificial intelligence and practical applications in urban settings. Apart from technical exploration, the study also delves into the ethical and privacy implications arising in smart cities. Moreover, it thoroughly examines the challenges that must be addressed to realise this urban revolution's potential fully.
Elsevier",Low
"In this paper, a visual object tracking method is proposed based on sparse 2-dimensional discrete cosine transform (2D DCT) coefficients as discriminative features. To select the discriminative DCT coefficients, we give two propositions. The propositions select the features based on estimated mean of feature distributions in each frame. Some intermediate tracking instances are obtained by (a) computing feature similarity using kernel, (b) finding the maximum classifier score computed using ratio classifier, and (c) combinations of both. Another intermediate tracking instance is obtained using incremental subspace learning method. The final tracked instance amongst the intermediate instances are selected by using a discriminative linear classifier learned in each frame. The linear classifier is updated in each frame using some of the intermediate tracked instances. The proposed method has a better tracking performance as compared to state-of-the-art video trackers in a dataset of 50 challenging video sequences.",Low
"The building sector accounts for 36 % of the total global energy usage and 40% of associated Carbon Dioxide emissions. Therefore, the forecasting of building energy consumption plays a key role for different building energy management applications (e.g., demand-side management and promoting energy efficiency measures), and implementing intelligent control strategies. Thanks to the advancement of Internet of Things in the last few years, this has led to an increase in the amount of buildings energy related-data. The accessibility of this data has inspired the interest of researchers to utilize different data-driven approaches to forecast building energy consumption. In this study, we first present state of-the-art Machine Learning, Deep Learning and Statistical Analysis models that have been used in the area of forecasting building energy consumption. In addition, we also introduce a comprehensive review of the existing research publications that have been published since 2015. The reviewed literature has been categorized according to the following scopes: (I) building type and location; (II) data components; (III) temporal granularity; (IV) data pre-processing methods; (V) features selection and extraction techniques; (VI) type of approaches; (VII) models used; and (VIII) key performance indicators. Finally, gaps and current challenges with respect to data-driven building energy consumption forecasting have been highlighted, and promising future research directions are also recommended.
Elsevier",Low
"Context
Knowledge graphs describe knowledge resources and their carriers through visualization. Moreover, they mine, analyze, construct, draw, and display knowledge and their interrelationships to reveal the dynamic development law of the knowledge field. Furthermore, knowledge graphs provide practical and valuable references for subject research. With the development of software engineering, powerful semantic processing and organizational interconnection capabilities of knowledge graphs are gradually required. Current research suggests using knowledge graphs for code or API recommendation, vulnerability mining, and positioning to improve the efficiency and accuracy of development and design. However, software engineering lacks a systematic analysis of the knowledge graphs application.
Objective
This paper explores the construction techniques and application status of knowledge graphs in the field of software engineering, broadens the application prospects of knowledge graphs in this field, and facilitates the subsequent research of researchers.
Methods
We collected over 100 documents from 2017 to date and selected 55 directly related documents for systematic analysis. Then, we analyzed the organized knowledge mainly stored in software engineering knowledge graphs, including software architecture, code details, and security reports.
Results
We studied the emerging research methods in ontology modeling, named entity recognition, and knowledge fusion in graph construction and found that current knowledge graphs are mainly used in intelligent software development, software vulnerability mining, security testing, and API recommendation.
Conclusion
Our research on the innovation of knowledge graph in software engineering and the future construction of integrating open-source community software and developer recommendations with knowledge-driven microservice O&M aspects can inspire more scholars and knowledge workers to use knowledge graph technology, which is important to solve software engineering problems and promote the development of both fields.
Elsevier",Low
"Data stream mining presents notable challenges in the form of concept drift and evolution. Existing learning algorithms, typically designed within a supervised learning framework, require class labels for all data points. However, this is an impractical requirement given the rapid pace of data streams, which often results in label scarcity. Recognizing the realistic necessity of learning from data streams with limited labels, we propose an adaptive, data-driven, prototype-based semi-supervised learning framework specifically tailored to handle evolving data streams. Our method employs a prototype-based data representation, summarizing the continuous flow of streaming data using dynamic prototypes at varying levels of granularity. This technique enables improved data abstraction, capturing the underlying local data distributions more accurately. The model also incorporates reliability modeling and efficient emerging class discovery, dynamically updating the significance of prototypes over time and swiftly adapting to local concept drift. We further leverage these adaptive prototypes to intuitively detect concept evolution, i.e., identifying novel classes from a local density perspective. To minimize the need for manual labeling while optimizing performance, we incorporate active learning into our method. This method employs a dual-criteria approach for data point selection, considering both uncertainty and local density. These manually labeled data points, together with unlabeled data, serve to update the model efficiently and robustly. Empirical validation using several benchmark datasets demonstrates promising performance in comparison to existing state-of-the-art techniques.",Medium
"Heterogeneous Networks (HetNets) play an imperative role in enhancing the quality-of-service (QoS) for end-users by increasing the spectral efficiency (SE) of the network and reducing the power consumption of user equipment (UE). However, the dynamic and distributed nature of HetNets makes them susceptible to several types of attacks (e.g., jamming, eavesdropping). Also, new technologies of 5G like massive MIMO, mmWave, and NOMA bring unique security concerns to 5G HetNets, which were not present in pre-5G HetNets. Implementing traditional security techniques such as access control, encryption, and network security seems to be insufficient for 5G HetNets and their inherent vulnerabilities. Physical layer security (PLS) has evolved as a novel approach and robust substitute to cryptography methods for ensuring secure transmission of confidential data. Also, AI methods are critical in transforming the HetNetsâ€™ security from just allowing safe communications between UEs to security-enabled intelligence systems. Motivated by the benefits of AI and PLS in improving wireless security, this paper seeks to provide a comprehensive survey of artificial intelligence (AI) enabled PLS techniques for secure data transmission in 5G HetNets. We have introduced physical layer threats and significant security issues in 5G HetNets. Then, we provide an outline of the conventional PLS techniques and challenges associated with them in the design of secure transmission techniques for 5G HetNets, such as security-oriented beamforming, cooperative jamming, etc. Then we provide an in-depth analysis of the role of AI in optimizing and designing the intelligent PLS techniques, which can be used to enhance the security of data transmission in 5G HetNets. We have also performed a strengths, weaknesses, opportunities, and threats (SWOT) analysis of the existing PLS techniques to further assist the readers in designing secure transmission techniques. Finally, we discuss various issues and future research directions associated with designing AI-enabled secure data transmission techniques.
Elsevier",Low
"Federated Learning (FL) enables collaborative model training without sharing data, but traditional static averaging of local updates leads to poor performance on heterogeneous data. The following remedies, either by scheduling data distribution or mitigating local discrepancies, predominately fail to handle fine-grained heterogeneity (e.g., local imbalanced labels). To commence, we reveal that static averaging leads to the global model suffering from the mean fallacy. That is, the averaging process favors the local model with large parameters numerically rather than knowledge. To tackle this, we introduce FedVSA, a simple-yet-effective model aggregation framework sensitive to heterogeneous local data merits. Specifically, we invent a new global loss function for FL by prioritizing the valuable local updates, facilitating efficient convergence. We deduce a softmax-based aggregation rule and prove its convergence property via rigorous theoretical analysis. Additionally, we expose poisoning threats of model replacement that utilize the mean fallacy for attacks. To mitigate this threat, we propose a two-step mechanism involving auditing historic local training statistics and analyzing the Shapley Value. Through extensive experiments, we show that FedVSA achieves faster convergence (~1.52×) and higher accuracy (~1.6%) compared to the baselines. It also effectively mitigates poisoning attacks by agilely recovering and returning to normal aggregation.",Medium
"Nowadays, network traffic analysis is quite pervasive in human practice. Website fingerprinting attack, which is a new variant of traffic analysis attacks, identifies the websites visited by clients in encrypted and anonymized Tor connections by observing patterns in packet flows. Previous website fingerprinting attacks focus on static models in which the classifier is trained within a time period and then it is utilized to identify targeted websites. Static attacks cannot handle the time effect on the accuracy since their classifiers are not trained on the newest versions of the websites. Consequently, their accuracy drops drastically when tested on captured traffic traces of websites, days after training. This time effect is known as concept drift.
In order to maintain the performance of the classifier, the classifier must be updated over time. In static attacks, updating the classifier includes updating the whole dataset again along with retraining the classifier. Recollecting, maintaining, and updating datasets as well as updating and retraining the classifier are very time and memory consuming. To deal with the emerging issues arising from concept drift and expensive retraining phases, this paper proposes AdaWFPA; an adaptive online website fingerprinting attack which is based on adaptive stream mining algorithms. AdaWFPA avoids concept drift by updating its model over time. Our empirical analyses and experiments over real world datasets indicate the superiority of our approach; as it offers better accuracy, precision, and recall in comparison with other state-of-the-art methods in the literature.
Elsevier",Medium
"Smart grid uses the power of information technology to intelligently deliver energy by using a two-way communication and wisely meet the environmental requirements by facilitating the integration of green technologies. The inherent weakness of communication technology has exposed the system to numerous security threats. Several survey papers have discussed these problems and their countermeasures. However, most of these papers classified attacks based on confidentiality, integrity, and availability, but they excluded the accountability. In addition, the existing countermeasures focus on countering some specific attacks or protecting some specific components, but there is no global approach to secure the entire system. In this paper, we review the security requirements, provide descriptions of several severe cyber-attacks, and propose a cyber-security strategy to detect and counter these attacks. Lastly, we provide some future research directions.
Elsevier",Low
"Precision health leverages information from various sources, including omics, lifestyle, environment, social media, medical records, and medical insurance claims to enable personalized care, prevent and predict illness, and precise treatments. It extensively uses sensing technologies (e.g., electronic health monitoring devices), computations (e.g., machine learning), and communication (e.g., interaction between the health data centers). As health data contain sensitive private information, including the identity of patient and carer and medical conditions of the patient, proper care is required at all times. Leakage of these private information affects the personal life, including bullying, high insurance premium, and loss of job due to the medical history. Thus, the security, privacy of and trust on the information are of utmost importance. Moreover, government legislation and ethics committees demand the security and privacy of healthcare data. Besides, the public, who is the data source, always expects the security, privacy, and trust of their data. Otherwise, they can avoid contributing their data to the precision health system. Consequently, as the public is the targeted beneficiary of the system, the effectiveness of precision health diminishes. Herein, in the light of precision health data security, privacy, ethical and regulatory requirements, finding the best methods and techniques for the utilization of the health data, and thus precision health is essential. In this regard, firstly, this paper explores the regulations, ethical guidelines around the world, and domain-specific needs. Then it presents the requirements and investigates the associated challenges. Secondly, this paper investigates secure and privacy-preserving machine learning methods suitable for the computation of precision health data along with their usage in relevant health projects. Finally, it illustrates the best available techniques for precision health data security and privacy with a conceptual system model that enables compliance, ethics clearance, consent management, medical innovations, and developments in the health domain.
Elsevier",Low
"A key purpose of a data management system is to monitor and control the large volume of data and other relevant information. Focusing on the data management issues and developing a detection system is necessary for resolving attacks or intrusion in a network. This network instruction detection system helps to identify unseen and unpredictable attacks in management station via loophole to break network security. Conventional instruction detection system has complexities in exploiting, enhancing the security features and this research work focuses on solving above issue. The proposed research work is designed for efficient and flexible network intrusion detection system using NaÃ¯ve Bayes classifier and deep neural networks. The experimental results show that proposed Deep Neural Network-based Intrusion Detection System is suitable for classification with high accuracy and precision in both binary and multiclass, recall and f- measure values. Compared with other state-of-the-art approaches, the analytic accuracy has been improved.
Elsevier",Low
"There is a paradigm shift from traditional power distribution systems to smart grids (SGs) due to advances in information and communication technology. An advanced metering infrastructure (AMI) is one of the main components in an SG. Its relevance comes from its ability to collect, process, and transfer data through the internet. Although the advances in AMI and SG techniques have brought new operational benefits, they introduce new security and privacy challenges. Security has emerged as an imperative requirement to protect an AMI from attack. Currently, ensuring security is a major challenge in the design and deployment of an AMI. This study provides a systematic survey of the security of AMI systems from diverse perspectives. It focuses on attacks, mitigation approaches, and future visions. The contributions of this article are fourfold: First, the vulnerabilities that may exist in all components of an AMI are described and analyzed. Second, it considers attacks that exploit these vulnerabilities and the impact they can have on the performance of individual components and the overall AMI system. Third, it discusses various countermeasures that can protect an AMI system. Fourth, it presents the open challenges relating to AMI security as well as future research directions. The uniqueness of this review is its comprehensive coverage of AMI components with respect to their security vulnerabilities, attacks, and countermeasures. The future vision is described at the end.
Elsevier",Low
"Caused by the rising of new network types, e.g., Internet of Things (IoT), within the last decade and related challenges like Big Data and data processing delay, new paradigms such as Edge and Fog computing emerged. Although these paradigms can partially address those challenges, their performance can still be affected by various issues, such as faults or network inefficiencies. To establish efficient network infrastructures for these paradigms, Network Management and Orchestration (NMO) techniques are introduced to improve various aspects of networking e.g., Quality of Service (QoS) provisioning, resource management, task allocation, and many others. Therefore, NMO primarily uses various methods like statistical models, heuristic techniques or Artificial Intelligence (AI) to automate networking decision-making. In this study, we investigate NMO issues, related orchestration challenges and the usage of Machine Learning (ML) techniques as a sub-field of AI for NMO purposes. The focus rests on new Edge-based networking and computing paradigms that employ resource-constraint devices to perform different tasks in environments like Extreme Edge, Cloud-of-Things (CoT) or Mist. We provide a comprehensive survey including a state-of-the-art review, research challenges and future directions. The study shows the challenges of NMO in such paradigms and provides information on how ML-based techniques can improve the performance of Edge-based networking paradigms.
Elsevier",Low
"The construction of Smart Cities is inseparable from the healthy operation of markets. Reasonable data analysis can provide a crucial foundation for the development of market behavior by considering the enormous amount of data generatedÂ by a market economy. To this end, we propose enhanced cluster generative adversarial networks (eClusterGAN) to achieve latent space clustering. However, data storage security is crucial. Moreover, we suggest a GAN-based network intrusion detection system (GANsingle bondNIDS) that uses adversarial learning to assist the generator in learning the spatial distribution of normal network flows. The simulation results showed that the proposed eClusterGAN and GANsingle bondNIDS outperformed the benchmarks in terms of clustering accuracy, running time, precision, recall, and F1, which can support researchers in studying economic data trends. The construction of Smart Cities can effectively ensure healthy market development by discovering and disseminating the potential value of market economic data.
Elsevier",Low
"In the field of network security, the process of labeling a network traffic dataset is specially expensive since expert knowledge is required to perform the annotations. With the aid of visual analytic applications such as RiskID, the effort of labeling network traffic is considerable reduced. However, since the label assignment still requires an expert pondering several factors, the annotation process remains a difficult task. The present article introduces a novel active learning strategy for building a random forest model based on user previously-labeled connections. The resulting model provides to the user an estimation of the probability of the remaining unlabeled connections helping him in the traffic annotation task. The article describes the active learning strategy, the interfaces with the RiskID system, the algorithms used to predict botnet behavior, and a proposed evaluation framework. The evaluation framework includes studies to assess not only the prediction performance of the active learning strategy but also the learning rate and resilience against noise as well as the improvements on other well known labeling strategies. The framework represents a complete methodology for evaluating the performance of any active learning solution. The evaluation results showed proposed approach is a significant improvement over previous labeling strategies.
Elsevier",Medium
"Learning-based sketch improves accuracy and reduces the space overhead of traditional sketches by using machine learning to recognize input data patterns. However, most learning-based sketches adopts offline learning to train a learning model, which shows poor accuracy in measuring dynamic network traffic. To accommodate unpredictable changes in network traffic, it is urgent to study a learning-based sketch that not only supports timely updating recognition rules but also keeps accurate and efficient network traffic measurement. We propose a novel adaptive learning-based sketch (ALSketch) to accurately and efficiently measure dynamic network traffic with online machine learning. ALSketch incrementally constructs a learner to identify hot and cold flows and store them separately. Such a strategy reduces hash conflicts and improves estimated accuracy under limited memory usage. We perform extensive comparative experiments to verify the performance of ALSketch. Experimental results reveal that the average relative error of ALSketch is approximately 1. 86âˆ¼ 5. 25 times lower than that of traditional sketches under the same memory conditions.
Elsevier",Medium
"Artificial intelligence employs Machine Learning (ML) and Deep Learning (DL) to analyze data. In both, the data is stored centrally. The data involved may be sensitive and leakage may incur consequences. Applications dealing with intimate data, with critical results, cannot afford this risk and are termed Data-Sensitive Applications (DSA). Some examples are healthcare, finance, etc. The data required for DSA cannot be stored centrally due to large amounts, or isolated data islands. The ML and DL techniques following a data-centralized approach have difficulties in handling the scattered data frequently associated with DSA. Federated Learning (FL) acknowledges the scattered data and provides a more secure and efficient way to analyze such data. This motivates previously reluctant entities like banks to collaborate for variety and quantity of data. Most DSA transitioned to FL, but the migration is not without concerns. These include communication costs, heterogeneity, and malicious attacks. In this paper, we deeply analyze the role of FL in DSA and provide a taxonomy for the studies and implementations of FL. Then we provide an insight into DSA covering works in healthcare and finance. A glance is provided at attempts in non-DSA with possible DSA applications. Finally, we discuss FL's open issues and challenges with their possible solutions.
Elsevier",Low
"Data stream clustering plays an important role in data stream mining for knowledge extraction. Numerous researchers have recently studied density-based clustering algorithms due to their capability to generate arbitrarily shaped clusters. However, most of the algorithms are either fully offline, hybrid online/offline, or cannot handle the property of evolving data stream. Recently, a fully online clustering algorithm for evolving data stream called CEDAS was proposed. However, similar to other density-based clustering algorithms, CEDAS requires predefining the global optimal radius of micro-clusters, which is a difficult task; in addition, an erroneous choice deteriorates cluster performance. Moreover, the algorithm ignores the presence of temporarily irrelevant micro-clusters, which may be relevant in the future. In this study, we present a fully online density-based clustering algorithm called buffer-based online clustering for evolving data stream (BOCEDS). This algorithm recursively updates the micro-cluster radius to its local optimal. It also introduces a buffer for storing irrelevant micro-clusters and a fully online pruning method for extracting the temporarily irrelevant micro-cluster from the buffer. In addition, BOCEDS proposes an online micro-cluster energy-updating function based on the spatial information of the data stream. Experimental results are compared with those of CEDAS and other alternative hybrid online/offline density-based clustering algorithms, and BOCEDS proves its superiority over the other clustering algorithms. The sensitivity of clustering parameters is also measured. The proposed algorithm is then applied to real-world weather data streams to demonstrate its capability to detect changes in data stream and discover arbitrarily shaped clusters. The proposed BOCEDS can be available in https://sites.google.com/view/md-manjur-ahmed and https://sites.google.com/view/kamrul-just.
Elsevier",Medium
"Web applicationsâ€™ popularity has raised attention in various service domains, which increased the concern about cyber-attacks. One of these most serious and frequent web application attacks is a Cross-site scripting attack (XSS). It causes grievous harm to victims. Existing security methods against XSS fail due to the evolving nature of XSS attacks. One evolving aspect of XSS attacks is feature drift which changes the feature relevancy and causes degradation in the performance. Unfortunately, dynamic awareness of drift occurrence is missing. Thus, this study attempts to fill the gap by proposing a feature drift-aware algorithm for detecting the evolved XSS attacks. The proposed approach is a dynamic feature selection based on a deep Q-network multi-agent feature selection (DQN-MAFS) framework. Each agent is associated with one feature and is responsible for selecting or deselecting its feature. DQN-MAFS provides a sub-model for reward distribution over agents, which is named as fair agent reward distribution based dynamic feature selection FARD-DFS. This framework is capable of supporting real-time, dynamic updates and adjustment of embedded knowledge as long as new labelled data arrives. DQN-MAFS has been evaluated using four real XSS attack datasets with various feature length sizes. The evaluation process was conducted and compared with state-of-the-art works. The obtained results show the superiority of our FARD-DFS over the benchmarks in terms of the majority of metrics. The improvement percentages of the mean accuracy and F1-measure ranged from 1.01% to 12.1% and from 0.55% to 6.88%, respectively, in comparison with the benchmarks. This approach can be deployed as an autonomous detection system without the need for any offline retraining process of the model to detect the evolved XSS attack.
Elsevier",High
"Artificial Intelligence (AI) and Machine Learning (ML) are being used more and more to handle complex tasks in many different areas. As a result, interconnected information systems are growing, which means that autonomous systems are needed to help them adapt, find complex patterns, and make better decisions in areas like cybersecurity, finance, healthcare, authentication, marketing, and supply chain optimization. Even though there have been improvements in self-learning methods for complex pattern recognition in linked information systems, these studies still do not have a complete taxonomy that sorts these methods by how they can be used in different areas. It is hard to fully understand important factors and do the comparisons that are needed to drive the growth and use of autonomous learning in linked systems because of this gap. Because these methods are becoming more important, new study is looking into how they can be used in different areas. Still, recent study shows that we do not fully understand the environment of other uses for independent learning methods, which encourages us to keep looking into it. We come up with a new classification system that puts applications into six groups: finding cybersecurity threats, finding fraud in finance, diagnosing and monitoring healthcare, biometric authentication, personalized marketing, and optimizing the supply chain in systems that are all connected. The latest developments in this area can be seen by carefully looking at basic factors like pros and cons, modeling setting, and datasets. In particular, the data show that Elsevier and Springer both put out a lot of important papers (26.5 % and 11.8 %, respectively). With rates of 12.9 %, 11 %, and 8 %, respectively, the study shows that accuracy, mobility, and privacy are the most important factors. Tools like Python and MATLAB are now the most popular ways to test possible answers in this growing field.",Low
"Heterogeneous networks (HetIoT) of high-capacity and resource-constrained IoT devices and their edge associations for on-device distributed critical workloadsâ€”called the edge-of-things (EoT)â€”attract short-burst, botnet-based zero-day attacks that exploit latent vulnerabilities due to heterogeneous device properties, dynamic operational contexts, and insufficient security scrutiny of the constituent proprietary devices. Such a scenario necessitates a device-specific network intrusion detection (NID) technique for localizing the threat space and updated rule learning through online (real-time) model retraining. Furthermore, scarce labeled knowledge base and high levels of class imbalance of NID datasets complicate the ID system design process for EoT environments, as online detection cannot afford computationally expensive data balancing techniques; this necessitates a class imbalance invariant traffic inference technique for data preprocessing. Therefore, we propound the ONIDS online NID technique, which consists of a two-fold solution for the above problems. First, we propose a Beta distribution-based inference technique for efficient traffic behavior approximationâ€”invariant of class imbalance and capable of non-cumulative traffic processing of smaller sample sizes. Then, we put forth an online ID technique called ELMO for class imbalance invariant time-bound training of smaller sample sizes on resource-constrained device-specific network traffic. Together, they are invariant of traffic class imbalances and adaptable to resultant concept drift categories exhibited by HetIoT attack behaviors. ONIDS has low memory and compute footprints and can efficiently process large and small amounts of traffic, making it suitable for online and offline NID. It also exhibits qualitative and quantitative superiorityâ€”particularly on smaller data samples.
Elsevier",High
"Granular traffic classification categorizes traffic into detailed classes like application names and services. Application names represent parent applications, such as Facebook, while application services are the individual actions within the parent application, such as Facebook-comment. These granular classes are still insufficient to keep pace with modern applications that offer various services. Accordingly, this paper further divides the application service class into inter-application and intra-application services to provide more insights. Inter-application service refers to a similar service between different parent applications, such as Facebook-comment and Youtube-comment, whereas intra-application service differentiates services within the same parent application, such as Facebook-comment and Facebook-post. Most studies focus on classification at the application name and inter-application service levels. In contrast, classification at the intra-application service level receives far less attention due to its complexity despite providing the highest flexibility. Therefore, this paper presents GRAIN, a granular multi-label approach to classify encrypted traffic at all three levels of granular classification: application name, inter-application and intra-application service levels using a classifier chain. GRAIN chains two random forest classifiers to produce a multi-label classification using seven novel statistical features based on packet payload length. The utilized features are independent of the packet payload content, thus unaffected by packet encryption and preserving user privacy. Our performance evaluation showed that GRAIN achieved an average F-measure of 99% at the application name level, 93% at the inter-application service level and 88% at the intra-application service level. To test for robustness, we compared GRAIN against four baseline classifiers and the ISCX VPN-nonVPN public dataset in which GRAIN maintained its comparable performance across all tests.
Elsevier",Low
"As the adversarial robustness research of deep neural networks has struggled in attack and defense games with static defense methodology, scholars have introduced the dynamic idea of the systems control to changeover the passive defense position though adapting decision-making. According to the different levels at which dynamism acts on neural networks, dynamic defense methods can be mainly divided into two categories: dynamic feedback control based on input level and uncertainty estimation detection based on decision level. Although both methods aim to hinder the success of the attacker, they cannot achieve the perfect conditions for constructing black box attacks because they ignore the positive role of dynamics in defense at the model level. Inspired by conventional ensemble selection technology in machine learning that treats different models as mutable objects for improving accuracy in uncertain data, this work investigates the robustness issue from a new dynamic aspect: model-level dynamic defense, whether the dynamic attributes depend on input or decision. Specifically, the Dirichlet prior combined with diversity constraint is imposed on the ensemble parameter in training phase to construct select criterion and candidate sub-models. Therefore, the final prediction of ensemble can be strategically selected though the rank of different sub-modelsâ€™ uncertainty value for robust decision-making in the test phase. The experimental results indicate the comprehensive promotion of robustness (at least 4.17% in black-box attack conditions and at least 1.78% in the case of high-disturbance white-box attack budge) of the proposed method compared with common dynamic and static defense methods.
Elsevier",Medium
"Deep learning models have shown to achieve high performance in encrypted traffic classification. However, when it comes to production use, multiple factors challenge the performance of these models. The emergence of new protocols, especially at the application layer, as well as updates to previous protocols affect the patterns in input data, making the modelâ€™s previously learned patterns obsolete. Furthermore, proposed model architectures for encrypted traffic classification are usually tested on datasets collected in controlled settings, which makes the reported performances unreliable for production use. In this paper, we study how the performances of two high-performing state-of-the-art encrypted traffic classifiers change on multiple real-world datasets collected over the course of two years from a major ISPâ€™s network. We investigate the changes in traffic data patterns highlighting the extent to which these changes, also known as data drift, impact the performance of the two models in service-level as well as application-level classification. We propose best practices for architecture adaptations to improve the accuracy of the model in the face of data drift. We show that our best practices are generalizable to other encryption protocols and different levels of labeling granularity.
Elsevier",Medium
"Multiple nations are now in the process of launching 5G mobile networks. All of the telecoms industry has been revolutionized by the concept of 5G networks. Since late 2018, researchers and telecom players have been actively exploring the potential of 6G as a successor to 5G. It is anticipated that the 6G generation would have even more severe performance criteria than the 5G generation had. New sophisticated technologies and paradigms must be included into 6G network designs and procedures to satisfy the requirements and standards of the sixth generation. As a result, several publications and studies covering the relevant technologies, techniques, algorithms, and architectures have appeared in the recent few years. The current paper fills a gap in the literature by offering informative recommendations for further study of 6G networks. Connectivity targets within 6G are outlined, and the study explains the wireless progression toward 6G networks. The technologies are outlined, as are the obstacles that must be surmounted. This chapter also provides a thorough and up-to-date overview of the state of the art. Additionally, multiple classifications of 6G capabilities and techniques are provided, with an emphasis on the advantages and disadvantages of each process. In order to accelerate the advancement of 6G technologies and fulfill their needs, we also identify open problems and potential future possibilities.",Low
"The increasingly frequent network intrusions have brought serious impacts to the production and life, thus malicious network traffic detection has received more and more attention in recent years. However, the traditional rule matching-based and machine learning-based malicious network traffic detection methods have the problems of relying on human experience as well as low detection efficiency. The continuous development of deep learning technology provides new ideas to solve malicious network traffic detection, and the deep learning models are also widely used in the field of malicious network traffic detection. Compared with other deep learning models, bidirectional temporal convolutional network (BiTCN) has achieved better detection results due to its ability to obtain bidirectional semantic features of network traffic, but it does not consider the different meanings as well as different importance of different subsequence segments in network traffic sequences; In addition, the loss function used in BiTCN is the negative log likelihood function, which may lead to overfitting problems when facing multi-classification problems and data imbalance problems. To solve these problems, this paper proposes a malicious network traffic detection model based on BiTCN and multi-head self-attention (MHSA) mechanism, namely BiTCN_MHSA, it innovatively uses the MHSA mechanism to assign different weights to different subsequences of network traffic, thus making the model more focused on the characteristics of malicious network traffic as well as improving the efficiency of processing global network traffic; Moreover, it also changes its loss function to a cross-entropy loss function to penalize misclassification more severely, thereby speeding up the convergence. Finally, extensive experiments are conduced to evaluate the efficiency of proposed BiTCN_MHSA model on two public network traffic, the experimental results verify that the proposed BiTCN_MHSA model outperforms six state-of-the-arts in precision, recall, F1-measure and accuracy.
Elsevier",Low
"With the development of times and new technologies, lifelong learning has become the norm. Online education breaks the limits of traditional classroom, based on Internet technology, learners can learn more freely. However, due to the lack of high-quality network platforms in the market, some excellent teachersâ€™ recorded courses cannot be well shared. The development of the Web system is different from the offline teaching mode. The precise user course recommendation system and the high stability of the system are suitable for learners of different ages and different fields. Web online education platform based on Django framework, MTV model designed on the basis of MVC, using the glue language Python development, using mysql database to store data to the hard disk, using Navicat as the graphical interface of mysql management end. Users can watch online courses and videos recorded by teachers from major teaching institutions or universities. In addition, users can download courseware and other materials related to the course, comment on the course, and accurately recommend the course. The online deployment of the system has been completed through Nginx+ UWSGI. The purpose of the system is to integrate the existing teaching resources, so that the network teaching resources can meet the needs of different users, and meet the needs of learners of different ages and different fields through accurate recommendation of learning courses for different users.
ACM Digital Library",Low
"In this context, based on the development of distributed computing, especially grid technology, a new service computing model, cloud computing, has emerged. Therefore, data mining technology with automatic incremental learning ability has become a hot research technology in the industry. It makes the management process of its virtualized resources more complicated, and it is necessary to manage resource entities from multiple levels and realize finer-grained management operations. Therefore, in the cloud environment, how to find an efficient and reasonable computing resource scheduling model is very important, which determines the performance of the cloud computing system and becomes an important research direction at present. These bionic intelligent algorithms have the characteristics of strong search ability and parallelism, which can quickly find the optimal scheduling scheme of cloud computing resources and effectively improve the utilization efficiency of cloud computing resources. In addition, mobile access network security and mobile terminal security issues are becoming increasingly prominent. Support vector machine technology has shown excellent performance in obtaining global optimal solution and high-dimensional sample space, which has aroused the research enthusiasm of many scholars at home and abroad. In this paper, based on the in-depth study of the existing cloud computing, secondly, in-depth study of cloud computing scheduling model based on the support vector machine algorithm.
ACM Digital Library",Medium
"A theory of three-way decision is about thinking, problem-solving, and computing in threes or through triads. In this paper, we review fifteen years of research on three-way decision by using the philosophy-theory-application triad and the who-what-when triad. First, we discuss the philosophy, theory, and application of three-way decision. At the philosophy level, we delve into the philosophical roots and fundamental nature of three-way decision to reveal the underlying philosophical thinking. At the theory level, we provide an insightful analysis of the theory and methodology of three-way decision. At the application level, we examine the integration of three-way decision with other theories and their applications and effectiveness in real-world scenarios. Second, we focus on bibliometrics analytics by using the who-what-when triad, which attempts to answer a fundamental question of â€œwho did what whenâ€. We propose a 3Ã— 3 model by applying the 3Ã— 3 method of three-way decision. The first 3 is the author-topic-time triad. The second 3 represents a three-level analysis for each of the first three:(1) categorizing authors into the three levels of prolific authors, frequent authors, and occasional authors,(2) classifying topics into the three levels of the core topics, emerging topics, and to-be-explored topics, and (3) dividing articles into the three levels of initial investigations, further developments, and most recent studies. Finally, we perform a bibliometrics analysis of three-way decision articles by using the 3Ã— 3 model of three-way decision. The results not only reveal the current status and trend of three-way decision research but also provide a road map for future research.
Elsevier",Low
"Wireless communication and computation technologies are becoming increasingly complex and dynamic due to the sophisticated and ubiquitous Internet of things (IoT) applications. Therefore, future wireless networks and computation solutions must be able to handle these challenges and dynamic user requirements for the success of IoT systems. Recently, learning strategies (particularly deep learning and reinforcement learning) are explored immensely to deal with the complexity and dynamic nature of communication and computation technologies for IoT systems, mainly because of their power to predict and efficient data analysis. Learning strategies can significantly enhance the performance of IoT systems at different stages, including at IoT node level, local communication, long-range communication, edge gateway, cloud platform, and corporate data centers. This paper presents a comprehensive overview of learning strategies for IoT systems. We categorize learning paradigms for communication and computing technologies in IoT systems into reinforcement learning, Bayesian algorithms, stochastic learning, and miscellaneous. We then present research in IoT with the integration of learning strategies from the optimization perspective where the optimization objectives are categorized into maximization and minimization along with corresponding applications. Learning strategies are discussed to illustrate how these strategies can enhance the performance of IoT applications. We also identify the key performance indicators (KPIs) used to evaluate the performance of IoT systems and discuss learning algorithms for these KPIs. Lastly, we provide future research directions to further enhance IoT systems using learning strategies
Elsevier",Medium
"Faced with the rapid increase in smart Internet-of-Things (IoT) devices and the high demand for new business-oriented services in the fifth-generation (5G) and beyond network, the management of mobile networks is getting complex. Thus, traditional Network Management and Orchestration (MANO) approaches cannot keep up with rapidly evolving application requirements. This challenge has motivated the adoption of the Zero-touch network and Service Management (ZSM) concept to adapt the automation into network services management. By automating network and service management, ZSM offers efficiency to control network resources and enhance network performance visibility. The ultimate target of the ZSM concept is to enable an autonomous network system capable of self-configuration, self-monitoring, self-healing, and self-optimization based on service-level policies and rules without human intervention. Thus, the paper focuses on conducting a comprehensive survey of E2E ZSM architecture and solutions for 5G and beyond networks. The article begins by presenting the fundamental ZSM architecture and its essential components and interfaces. Then, a comprehensive review of the state-of-the-art for key technical areas, i.e., ZSM automation, cross-domain E2E service lifecycle management, and security aspects, are presented. Furthermore, the paper contains a summary of recent standardization efforts and research projects towards the ZSM realization in 5G and beyond networks. Finally, several lessons learned from the literature and open research problems related to ZSM realization are also discussed in this paper.
Elsevier",Low
"At the confluence of two great paradigms such as Edge Computing and Artificial Intelligence, Edge Intelligence arises. This new concept is about the smart exploitation of Edge Computing by bringing together reasoning and learning by Artificial Intelligence algorithms and the sensors/actuators computing capabilities. Security is the third paradigm that must join the team in order to have resilient and reliable systems to be used in real-world applications and use cases. Hence, smartness is, in this context, a puzzle of several independent pieces which, once fitted, can derived unprecedented benefits: a) security, b) low communication latency and network load, c) cost and energy saving and d) scalability by means of resource virtualization close to the IoT data generators (IoT devices). In fact, by paying exclusive attention to some of those main pillars and, therefore, disregarding others, edge computation once in operation often suffers from bad performance, unforeseen events or does not exploit the enormous potential that should be unlocked if a proper and complete specification had been laid down. With all this in mind, this work provides a technical review of the available and up-to-date frameworks to implement secure Edge Intelligence, pinpoints the most relevant unfilled gaps (strengths and weaknesses) and, last but nos least, includes challenges and future research lines as a result of our exploration.
Elsevier",Low
"In this paper, an adaptive network intrusion detection method using fuzzy rough set-based feature selection and GA-GOGMM-based pattern learning is presented. Based on the fuzzy rough set theory, the optimal attribute subset of network connection records is achieved by the information gain ratio criterion in advance. A greedy algorithm-based global optimal Gaussian mixture model (GMM) clustering method, termed GA-GOGMM, is introduced, to extract the intrinsic structure of network instances to achieve highly-discernable and stable normal and intrusion pattern libraries for the subsequent network intrusion detection (NID). GA-GOGMM-based pattern learning can achieve the optimal GMM of network traffic instances for the pattern clustering while avoiding the negative effect of the empirical initialization of clustering numbers and random initialization of clustering centers with a low computational complexity. An adaptive model updating mechanism is further introduced for the online updating of normal and intrusion pattern libraries to ensure the adaptability of the NID model. Extensive validation and comparative experiments, conducted on a benchmark dataset NSL-KDD and a self-built Nidsbench-based network simulation platform, show that the proposed ANID approach leads to a significant improvement in detection accuracies with low false alarms and missing reports on both known and unknown attacks. It can effectively adapt to the dynamic changing network environments with high detection accuracy and low false alarm rate as well as low missing reporting rate, which has significant application prospects.
Elsevier",Medium
"This paper presents a portable toolkit, SwitchNet, for extracting relations from textual input. We summarize four data protocols for relation extraction tasks, including relation classification, relation extraction, triple extraction, and distant supervision relation extraction. This neural architecture is modular, so it can take as input data at different stages of the information extraction process (simple text, text and entities or entity pairs as relation candidates) and compute the rest of the process (named entity recognition and relation classification). We systematically design four information flows to integrate the above protocols by sharing network building blocks and switching different information flows. This framework can extract multiple triples (subject, predicate, object) in one pass. This framework enhances the use of relation classification models in end-to-end triple extraction by inferring pairs of entities of interest and using the shared representation mechanism.
Elsevier",Low
"In this paper, we propose a General Non-negative Matrix Factorization based on the left Semi-Tensor Product (lGNMF) and the General Non-negative Matrix Factorization based on the right Semi-Tensor Product (rGNMF), which factorize an input non-negative matrix into two non-negative matrices of lower ranks based on gradient method. In particular, the proposed models are able to remove the dimension matching constraints required by conventional NMF models. Both theoretical derivation and experimental results show that the conventional NMF is a special case of the proposed lGNMF and rGNMF. We find the method for the best efficacy of the image restoration in lGNMF and rGNMF by experiments on baboon and lenna images. Moreover, inspired by the Incremental Non-negative Matrix Factorization (INMF), we propose the Incremental lGNMF (IlGNMF) and Incremental rGNMF (IrGNMF), We also conduct the experiments on JAFFE database and ORL database, and find that IlGNMF and IrGNMF realize saving storage space and reducing computation time in incremental facial training.
Elsevier",Medium
"As the thematic literature of machine learning suggests, reinforcement learning falls between the two methods of supervised learning and unsupervised learning. In this method, the learning agent receives a reward or punishment from the environment according to its action. Therefore, the learning agent interacts with the environment through trial and error and learns to choose the optimal action to achieve its goal. In the meantime, the eligibility traces are considered as one of the main mechanisms of reinforcement learning in receiving delayed rewards. In the use of conventional reinforcement learning methods, when the learning agent achieves a goal, only the value function of the last state-action pair changes, but all the trace states are affected based on the eligibility traces. In other words, the delayed rewards are distributed throughout the trace. Like the ant pheromone effect, this method can increase learning speed empirically to some extent. A soccer robot on the field encounters moving obstacles such as balls, rival robots, and home robots, and fixed obstacles such as gates and flags, so its environment is in a very dynamic state. Therefore, due to the dynamic environment, it is an important issue for autonomous soccer robots to avoid obstacles and should be considered in real play. The main idea of this study is to determine the appropriate traces for the robot to move towards the ball and ultimately to score with the approach of avoiding obstacles in simulating a real soccer match. The desired results obtained from a game played indicate a high level of online experience and decision-making power in the face of new situations.
Elsevier",Low
"In this work, we present a new neural network named WAvelet-Based Broad LEarning System (WABBLES). WABBLES is based on the flat structure of the broad learning system. Such structure offers an alternative to deep learning models, such as convolutional neural networks. The WABBLES network uses multiresolution analysis to look for subtle, yet important features from the input data for a better classification performance. WABBLES uses wavelets to map the input signal, to obtain more relevant features from it. This is achieved by autonomously learning and adjusting the dilation and translation parameters of a wavelet, which control its shape. In this way, the resulting mapping nodes have a better representation of the most important features for the classification problem. The construction of the model is described here, along with special considerations and algorithms involved. Finally, the proposed model is tested using a database of synthetic astronomical data and a benchmark dataset called the Breast Cancer Wisconsin Dataset (Original). The conducted experiments provide a comparison between the proposed model and several machine learning algorithms with different performance metrics applied to the context of exoplanet identification and breast cancer detection. Our results confirm that the WABBLES model obtains superior accuracy and F-score percentages than the other models.
Elsevier",Low
"Time series data mining becomes an active research area due to the rapid proliferation of temporal-dependent applications. Dimensionality reduction and uncertainty handling play a pivotal role in extracting the time series pattern. Most of the dimensionality reduction schemes are designed based on the assumption that every class of samples follows the Gaussian distribution. Lack of this property in real time data distribution does not allow dimensionality reduction techniques to characterize the different classes well and measure the data uncertainty accurately. In addition to, applying an uncertainty measurement evenly on inconsistent time series data samples may underestimate the source of uncertainty among various sub-samples. This paper presents the Handling UNcertainty and missing value prediction in Time series (HUNT). The proposed approach employs Adaptive Reservoir Filling for sampling the time series and Discrepant Sample dependent Chebyshev inequality for handling the uncertainty. The HUNT implements the adaptive reservoir filling using discrepancy estimation over a statistical population and decides the reservoir size according to the variations in the data stream. The state of the statistical population ensures the uncertainty handling over discrepant samples. The proposed approach precisely replaces the missing values with the support of the Mean-Mode imputation method. To effectively select the key features, it applies both the indirect and direct performance measures on the statistical samples. Finally, the proposed model generates the fine-tuned statistical samples through segmentation to facilitate the time series pattern matching. The experimental results demonstrate that the HUNT approach significantly outperforms the existing time series pattern matching approaches such as KSample approach by 18% higher recall and UG-Miner approach by 20% minimum Mean Absolute Error (MAE) while testing on the Weather forecasting dataset.",Low
"Traffic management in software-defined networks (SDNs) is critical for efficient bandwidth utilization and resource provisioning. Recent works on SDN load balancing (LB) have focused on identifying and rerouting elephant flows (EFs) for effective bandwidth usage. These techniques have some limitations, such as using source-to-destination hop count as the primary rerouting metric and not differentiating the types of flow that result in frequent resource conflicts when handling EF with long-lived bandwidth. Besides, current EF detection techniques use predefined bandwidth-use thresholds that cannot adapt to the ever-changing traffic condition. Also, detecting EF on switches results in high controller-switch bandwidth and high EF detection time. This study presents an ant colony optimization-based technique for rerouting EFs while considering load-balancing in the SDN links. This technique, called DPLBAnt, is formulated as a shortest-path problem in SDN that can alleviate the high controller-switch load. The proposed technique first detects EF by using a pair of classifiers on both SDN controller and switches. Most EF candidates are sifted on the switches, resulting in accurate and efficient detection of EF. Then, DPLBAnt obtains the global state of the SDN from which the most optimal paths for congested links are retrieved, and EF are redirected accordingly. The performance of the proposed DPLBAnt has been extensively simulated. Results indicate its superior performance over Equal-Cost Multi-Path (ECMP) and FlowSeer techniques in terms of average end-to-end delay (54% and 7.9% better), average network throughput (3. 5Ã— and 1. 5Ã— better), and average packet loss (18% and 10% better) respectively. The overall performance indicates that the proposed LB technique based on detection and rerouting of EFs can improve SDNâ€™s overall performance.
Elsevier",Low
"Due to their characteristics of mobility, flexibility and adaptive altitude, unmanned aerial vehicles (UAVs), also known as drones, have immense potential applications in wireless systems. In particular, UAVs can operate as flying mobile terminals within a cellular network, and such cellular-connected UAVs have enabled several applications ranging from real-time video streaming to item delivery. Furthermore, the deployment of UAVs in wireless networks as flying base stations (BSs), servers or relays is expected to enhance network performance in terms of coverage, capacity, reliability and energy-efficiency. In this paper, a comprehensive tutorial on the potential applications and benefits of UAVs is presented. Specifically, UAVs as aerial BSs collecting data from ground sensors, transmitting power to energy-constrained devices and broadcasting information to ground users are explored along with representative design objectives. Then, UAVs as relay nodes for coverage extension, pathloss compensation and interference mitigation are surveyed comprehensively. Besides, UAVs as mobile servers performing edge computing for ground users are introduced, and various aerial offloading modes are reviewed thoroughly. Furthermore, radio and computation resource optimization, as well as mathematical tools and developed algorithms, are described in UAV-assisted communicating, relaying, and computing systems. Finally, open problems and potential research directions pertaining UAV-assisted communications are presented. In a nutshell, this tutorial provides key guidelines on how to design, analyze and optimize UAV-assisted wireless communication systems.
Elsevier",Low
"Network and information security are regarded as some of the most pressing problems of contemporary economy, affecting both individual citizens and entire societies, making them a highlight for homeland security. Innovative approaches to handle this challenge are undertaken by the scientific community, proposing the utilization of the emerging, advanced machine learning methods. This very paper puts forward a novel approach to the detection of cyberattacks taking inventory of the practical application of information granules. The feasibility of utilizing Granular Computing (GC) as a solution to the most current challenges in cybersecurity is researched. To the best of our knowledge, granular computing has not yet been widely examined or used for cybersecurity application purposes. The major contribution of this work is a method for constructing information granules from network data. We then report promising results on a benchmark dataset.",Low
"Network security is becoming increasingly important in our daily livesnot only for organizations but also for individuals. Intrusion detection systems have been widely used to prevent information from being compromised, and various machine-learning techniques have been proposed to enhance the performance of intrusion detection systems. However, higher-quality training data is an essential determinant that could improve detection performance. It is well known that the marginal density ratio is the most powerful univariate classifier. In this paper, we propose an effective intrusion detection framework based on a support vector machine (SVM) with augmented features. More specifically, we implement the logarithm marginal density ratios transformation to form the original features with the goal of obtaining new and better-quality transformed features that can greatly improve the detection capability of an SVM-based detection model. The NSL-KDD dataset is used to evaluate the proposed method, and the empirical results show that it achieves a better and more robust performance than existing methods in terms of accuracy, detection rate, false alarm rate and training speed.",Low
"Network security plays an essential role in secure communication and avoids financial loss and crippled services due to network intrusions. Intruders generally exploit the flaws of popular software to mount a variety of attacks against network computer systems. The damage caused in the network attacks may vary from a little disruption in service to on developing financial loss. Recently, intrusion detection systems (IDSs) comprising machine learning techniques have emerged for handling unauthorized usage and access to network resources.
 With the passage of time, a wide variety of machine learning techniques have been designed and integrated with IDSs. Still, most of the IDSs reported poor intrusion detection results using false positive rate and detection rate. For solving these issues, researchers focused on the development of ensemble classifiers involving the integration of predictions by multiple individual classifiers. The ensemble classifiers enable to compensate for the weakness of individual classifiers and use their combined knowledge to enhance its performance. This study presents motivation and comprehensive review of intrusion detection systems based on ensembles in machine learning as an extension of our previous work in the field. Particularly, different ensemble methods in the field are analysed, taking into consideration different types of ensembles, and various approaches for integrating the predictions of individual classifiers for an ensemble classifier. The representative studies are compared in chronological order for systematic and critical analysis, understanding the current challenges and status of research in the field. Finally, the study presents essential future research directions for the development of effective IDSs.",Low
"New classifier-independent, dynamic, unsupervised approach for detecting concept drift.Reduced number of false alarms and increased relevance of drift detection.Results comparable to supervised approaches, which require fully labeled streams.Our approach generalizes the notion of margin density, as a signal to detect drifts.Experiments on cybersecurity datasets, show efficacy for detecting adversarial drifts. Classifiers deployed in the real world operate in a dynamic environment, where the data distribution can change over time. These changes, referred to as concept drift, can cause the predictive performance of the classifier to drop over time, thereby making it obsolete. To be of any real use, these classifiers need to detect drifts and be able to adapt to them, over time. Detecting drifts has traditionally been approached as a supervised task, with labeled data constantly being used for validating the learned model. Although effective in detecting drifts, these techniques are impractical, as labeling is a difficult, costly and time consuming activity. On the other hand, unsupervised change detection techniques are unreliable, as they produce a large number of false alarms. The inefficacy of the unsupervised techniques stems from the exclusion of the characteristics of the learned classifier, from the detection process. In this paper, we propose the Margin Density Drift Detection (MD3) algorithm, which tracks the number of samples in the uncertainty region of a classifier, as a metric to detect drift. The MD3 algorithm is a distribution independent, application independent, model independent, unsupervised and incremental algorithm for reliably detecting drifts from data streams. Experimental evaluation on 6 drift induced datasets and 4 additional datasets from the cybersecurity domain demonstrates that the MD3 approach can reliably detect drifts, with significantly fewer false alarms compared to unsupervised feature based drift detectors. At the same time, it produces performance comparable to that of a fully labeled drift detector. The reduced false alarms enables the signaling of drifts only when they are most likely to affect classification performance. As such, the MD3 approach leads to a detection scheme which is credible, label efficient and general in its applicability.",Medium
"New data driven framework for simulating exploratory attacks on black box classifiers.Algorithms for simple evasion attacks, to more sophisticated reverse engineering attacks.Formal adversarial model and metrics for baseline evaluation of secure learning strategies.Experimental evaluation on 10 datasets, with linear and non-linear defender models.Experimental evaluation on the black box Google Cloud Platform classifier system. While modern day web applications aim to create impact at the civilization level, they have become vulnerable to adversarial activity, where the next cyber-attack can take any shape and can originate from anywhere. The increasing scale and sophistication of attacks, has prompted the need for a data driven solution, with machine learning forming the core of many cybersecurity systems. Machine learning was not designed with security in mind and the essential assumption of stationarity, requiring that the training and testing data follow similar distributions, is violated in an adversarial domain. In this paper, an adversarys view point of a classification based system, is presented. Based on a formal adversarial model, the Seed-Explore-Exploit framework is presented, for simulating the generation of data driven and reverse engineering attacks on classifiers. Experimental evaluation, on 10 real world datasets and using the Google Cloud Prediction Platform, demonstrates the innate vulnerability of classifiers and the ease with which evasion can be carried out, without any explicit information about the classifier type, the training data or the application domain. The proposed framework, algorithms and empirical evaluation, serve as a white hat analysis of the vulnerabilities, and aim to foster the development of secure machine learning frameworks.",Low
"New malware emerges at a rapid pace and often incorporates Domain Generation Algorithms (DGAs) to avoid blocking the malwareâ€™s connection to the command and control (C2) server. Current state-of-the-art classifiers are able to separate benign from malicious domains (binary classification) and attribute them with high probability to the DGAs that generated them (multiclass classification). While binary classifiers can label domains of yet unknown DGAs as malicious, multiclass classifiers can only assign domains to DGAs that are known at the time of training, limiting the ability to uncover new malware families. In this work, we perform a comprehensive study on the detection of new DGAs, which includes an evaluation of 59,690 classifiers. We examine four different approaches in 15 different configurations and propose a simple yet effective approach based on the combination of a softmax classifier and regular expressions (regexes) to detect multiple unknown DGAs with high probability. At the same time, our approach retains state-of-the-art classification performance for known DGAs. Our evaluation is based on a leave-one-group-out cross-validation with a total of 94 DGA families. By using the maximum number of known DGAs, our evaluation scenario is particularly difficult and close to the real world. All of the approaches examined are privacy-preserving, since they operate without context and exclusively on a single domain to be classified. We round up our study with a thorough discussion of class-incremental learning strategies that can adapt an existing classifier to newly discovered classes.",Medium
"New network attack platforms such as personal to personal botnets pose a great threat to cyberspace, but there is no corresponding detection method to detect them. In order to improve the security of topological networks, this research designs a mathematical modeling analysis method for potential attack detection based on convolutional neural networks. This method determines the potential attack risk assessment function through the feature extraction of vulnerable areas in network topology and the probability model of potential attacks, and then detects potential attacks by means of convolutional neural network data modeling. The experimental results show that the false detection rate and missed detection rate of the three methods for potential attacks are lower than 9% and 8% respectively, but the false detection rate and missed detection rate of the method given in the study are the lowest, and can always be kept below 5%. At the same time, the detection time of potential attacks of this method is shorter than that of the other two detection methods. The detection of potential attacks provides a technical guarantee for the safe operation of the network.",Low
"NOTICE OF RETRACTION: While investigating potential publication-related misconduct in connection with the ICIMTech 2021 Conference Proceedings, serious concerns were raised that cast doubt on the integrity of the peer-review process and all papers published in the Proceedings of this Conference. The integrity of the entire Conference has been called into question. As a result, of its investigation, ACM has decided to retract the Entire Conference Proceedings and all related papers from the ACM Digital Library.None of the papers from this Proceeding should be cited in the literature because of the questionable integrity of the peer review process for this Conference.",Low
"Nowadays the existing legacy management-based healthcare system maintains and processes a large amount of health-related data. The widespread adoption of the Internet of Things (IoT) and its progressive development have promised the way for the development of IoT-enabled healthcare with impressive data processing and big data storage capabilities. Intelligent medical healthcare intends to offer a framework to remotely monitor usersâ€™ health-related data as the Industrial Internet of Things (IIoT) develops. Because they are stored on a cloud server, the data are still susceptible to manipulation and privacy breaches. The Keras Xception Deep Learning System (KX-DLS) with Dynamic Searchable Symmetric Encryption (DSSE) scheme is a revolutionary IoT-based deep learning intelligent privacy-preserving system that is advantageous for digital healthcare and its functionalities to handle security-related challenges. The dataset is being used to pre-train the system, and usersâ€™ personal information is kept separate in a secure location. Without knowing any personal information about the users, we analyse health-related data stored in the cloud and build a sophisticated security framework based on a deep learning model. With the most extensive collection of security features, our framework for learning intelligent privacy preservation optimizes the system to guarantee high data integrity and few privacy breaches. As a result, it may be useful in situations where users employ mobile devices with limited resources to engage a healthcare cloud system for extensive virtual health services, and the results of this research show that it has been a better-secured model in comparison with state-of-the-art previous techniques.",Low
"Nowadays, the intrusion detection system (IDS) plays a crucial role in the Internet of Things (IoT) networks, which could effectively protect sensitive data from various attacks. However, the existing works have not considered multiview features fusion and failed to capture the semantic relationships among the anomalous requests. They are not robust and cannot detect the attack types in real-time. This paper proposes a lightweight intrusion detection system based on deep learning and knowledge graph. First, our system extracts semantic relationships and key features by knowledge graph and statistical analysis. Then, IoT network requests are converted into word vectors through multiview feature fusion and feature alignment. Finally, an attention-based CNN-BiLSTM model is designed to identify malicious request attacks, which can capture long-distance dependence and contextual semantic information. Experiment results show that the proposed model significantly outperforms the existing solution in the robustness of the model. Moreover, it can select more critical features for IDS to achieve better accuracy and lower the false alarm rate. Compared with the state-of-the-art systems, the proposed IDS achieves a higher detection accuracy of 90.01%. In addition, our system can detect various stealthy attack types (including DoS, Probe, R2L, and U2L) and extract semantic relationships among features.",Low
"Nowadays, wireless body area networks WBANs is emerging as a promising technology with a considerable potential in improving patients healthcare services. The integration of WBAN and cloud computing technology provides a platform to create a new digital paradigm with leading features called cloud-assisted WBAN. The foremost concern of cloud-assisted WBAN is the security and privacy of data either collected and stored by WBAN sensors or transmitted to cloud over an insecure network. Among these, data availability is the most nagging security issue. The major threat to data availability is distributed denial of service attack DDoS normally launched from various distributed locations. In order to assure the all time availability of patients data, we propose a distributed victim based DDoS attack detection mechanism based on very fast decision tree VFDT learning model in cloud-assisted WBAN. The evaluation and performance analysis shows that the proposed mechanism could detect DDoS attack with high accuracy, and reduced false positive and false negative ratio.",Low
"Online services risk distributed denial of service attacks due to their availability. These attacks overload system resources and make them unusable by legitimate users. This study aims to analyse publicly available datasets spanning three years. This analysis uses machine learning classifiers to detect and classify the attacks. The experimental results of this approach demonstrate precise attack detection and classification with minimal false-positive rates. This study utilised publicly available datasets and employed machine learning classifiers. Decision tree and random forest classifiers achieved the highest accuracy rates, and the K-nearest neighbours and support vector machine classifiers took longer to execute. Correlation coefficient and recursive feature elimination approaches gave more insights into the features of the utilised datasets. Machine learning models were used to analyse attacks and determine the best accuracies for detection. Machine learning provided favourable detection rates for DDoS attacks, underscoring the importance of algorithm selection.",Low
"Optimizing the detection of intrusions is becoming more crucial due to the continuously rising rates and ferocity of cyber threats and attacks. One of the popular methods to optimize the accuracy of intrusion detection systems (IDSs) is by employing machine learning (ML) techniques. However, there are many factors that affect the accuracy of the ML-based IDSs. One of these factors is noise, which can be in the form of mislabelled instances, outliers, or extreme values. Determining the extent effect of noise helps to design and build more robust ML-based IDSs. This paper empirically examines the extent effect of noise on the accuracy of the ML-based IDSs by conducting a wide set of different experiments. The used ML algorithms are decision tree (DT), random forest (RF), support vector machine (SVM), artificial neural networks (ANNs), and Na\""{\i}ve Bayes (NB). In addition, the experiments are conducted on two widely used intrusion datasets, which are NSL-KDD and UNSW-NB15. Moreover, the paper also investigates the use of these ML algorithms as base classifiers with two ensembles of classifiers learning methods, which are bagging and boosting. The detailed results and findings are illustrated and discussed in this paper.",Low
"Optimum-path forest (OPF) is a graph-based machine learning method that can overcome some limitations of the traditional machine learning algorithms that have been used in intrusion detection systems. This paper presents a novel approach for intrusion detection using a modified OPF (MOPF) algorithm for improving the performance of traditional OPF in terms of detection rate (DR), false alarm rate (FAR), and time of execution. To address the problem of scalability in large datasets and also for achieving high attack recognition rates, the proposed framework employs the k-means clustering algorithm, as a partitioning module, for generating different homogeneous training subsets from original heterogeneous training samples. In the proposed MOPF algorithm, the distance between unlabeled samples and the root (prototype) of every sample in OPF is also considered in classifying unlabeled samples with the aim of improving the accuracy rate of traditional OPF algorithm. Moreover, the centrality and the prestige concepts in the social network analysis are employed in a pruning module for determining the most informative samples in training subsets to speed up the traditional OPF algorithm. The experimental results on NSL-KDD dataset show that the proposed method performs better than traditional OPF in terms of accuracy rate, DR, FAR, and cost per example (CPE) evaluation metrics. We proposed a modified version of optimum-path forest (MOPF) for intrusion detection.Social network analysis is used for pruning the training set to speed up the OPF.A partitioning module is used to improve the detection rate of low-frequent attacks.The classification phase of traditional OPF is modified for improving the accuracy.Our method improved detection/false alarm rate and execution time of traditional OPF.",Low
"Over the past decades, researchers have been proposing different Intrusion Detection approaches to deal with the increasing number and complexity of threats for computer systems. In this context, Random Forest models have been providing a notable performance on their applications in the realm of the behaviour-based Intrusion Detection Systems. Specificities of the Random Forest model are used to provide classification, feature selection, and proximity metrics. This work provides a comprehensive review of the general basic concepts related to Intrusion Detection Systems, including taxonomies, attacks, data collection, modelling, evaluation metrics, and commonly used methods. It also provides a survey of Random Forest based methods applied in this context, considering the particularities involved in these models. Finally, some open questions and challenges are posed combined with possible directions to deal with them, which may guide future works on the area.",Low
"Over the past few years, wireless sensor networks have gained significant attention. They have been distributed in the real world in order to collect valuable raw sensed data. Due to high density, WSNs (Wireless Sensors Networks) are exposed to faults and nasty attacks. Likewise, the sensors readings are inaccurate and unreliable, which make Wireless Sensor Networks vulnerable to outliers. Abnormal data, outliers or anomalies are usually considered to be those sensor readings that have deviated significantly from normal behavior. However, the challenge is to ensure data quality, secure monitoring and reliable detection of interesting and critical events. In this survey, we describe a comprehensive overview of existing outlier detection techniques specifically used for the wireless sensor networks. Moreover, we present a comparative table used as a guideline to select which technique is adequate for the application in terms of characteristics such as detection mode, architectural structure and correlation extraction.",Low
"Over the recent years, deep learning has been considered as one of the primary choices for handling huge amounts of data. Having deeper hidden layers, it surpasses classical methods for detection of outliers in wireless sensor networks. The convolutional neural network (CNN) is a biologically-inspired computational model which is one of the most popular deep learning approaches. It comprises neurons that self-optimize through learning. EEG generally known as electroencephalography is a tool used for investigation of brain function, and EEG signal gives time-series data as output. In this paper, the authors propose a state-of-the-art technique designed by processing the time-series data generated by the sensor nodes stored in a large dataset into discrete one-second frames, and these frames are projected onto 2D map images. A convolutional neural network (CNN) is then trained to classify these frames. The result improves detection accuracy.",Low
"Phishing attacks are the biggest cybersecurity threats in the digital world. Attackers exploit users by impersonating real, authentic websites to obtain sensitive information such as passwords and bank statements. One common technique in these attacks is using malicious URLs. These malicious URLs mimic legitimate URLs, misleading users into interacting with malicious websites. This practice, URL phishing, presents a big threat to internet security, emphasizing the need for advanced detection methods. So we aim to enhance phishing URL detection by using machine learning and deep learning models, leveraging a set of low-level URL features derived from n-gram analysis. In this paper, we present a method for detecting malicious URLs using statistical features extracted from n-grams. These n-grams are extracted from the hexadecimal representation of URLs. We employed 4 experiments in our paper. The first 3 experiments used machine learning with the statistical features extracted from these n-grams, and the fourth experiment used these grams directly with deep learning models to evaluate their effectiveness. Also, we used Explainable AI (XAI) to explore the extracted features and evaluate their importance and role in phishing detection. A key advantage of our method is its ability to reduce the number of features required and reduce the training time by using fewer features after applying XAI techniques. This stands in contrast to the previous study, which relies on high-level URL features and needs pre-processing and a high number of features (87 high-level URL-based features). So our technique only uses statistical features extracted from n-grams and the n-gram itself, without the need for any high-level features. Our method is evaluated across different n-gram lengths (2, 4, 6, and 8), aiming to optimize detection accuracy. We conducted four experiments in our study. In the first experiment, we focused on extracting and using 12 common statistical features like mean, median, etc. In the first experiment, the XGBoost model achieved the highest accuracy using 8-gram features with 82.41%. In the second experiment, we expanded the feature set and extracted an additional 13 features, so our feature count became 25. XGBoost in the second experiment achieved the highest accuracy with 86.40%. Accuracy improvement continued in the third experiment, we extracted an additional 16 features (character count features), and these features increased XGBoost accuracy to 88.15% in the third experiment. In the fourth experiment, we directly fed n-gram representations into deep learning models. The Convolutional Neural Network (CNN) model achieved the highest accuracy of 94.09% in experiment four. Also, we applied XAI techniques, SHapley Additive exPlanations (SHAP), and Local Interpretable Model-agnostic Explanations (LIME). Through the explanation provided by XAI methods, we were able to determine the most important features in our feature set, enabling a reduction in feature count. Using fewer features (4, 7, 10, 13, 15), we got good accuracy compared to the 41 features used in experiment three and reduced the modelsâ€™ training times and complexity. This research aimed to enhance phishing URL detection by using machine learning and deep learning models, leveraging a set of low-level URL features derived from n-gram analysis. Our findings show the importance of using minimal statistical features to identify malicious URLs. Notably, the use of CNN had a great advancement, achieving an accuracy rate of 94.09% with using n-grams of URLs, surpassing traditional machine learning models. This achievement not only validates the efficacy of deep learning models in complex pattern recognition tasks but also highlights the efficiency of our feature selection approach, which relies on a lower number of features and is less complex compared to existing high-level feature-based studies. The research outcomes demonstrate a promising pathway toward developing more robust, efficient, and scalable phishing detection systems.",Low
"Phishing detection in Semantic Web systems is crucial to safeguarding users from malicious attacks. In this context, this work presents a deep learning-based phishing attack detection model using MobileBERT for feature extraction and hyperparameter optimization using covariance matrix adaptation evolution strategy (CMA-ES). The model obtained a 95% classification accuracy. Important benchmarks like accuracy, recall, and F1-score show good ability to discriminate between phishing and legitimate emails. Applying CMA-ES, which improved detection accuracy, helps to verify the model even more. MobileBERT and CMA-ES together offer Semantic Web systems a fresh, efficient method of phishing detection.",Low
"Protecting our servers and machines is vital since they store our data and resources. Attackers use the latest tools and technologies to launch the attack. Among, several cyberattacks, DDoS attacks are the worst. DDoS attackers employ a variety of methods to exploit machines and consume all server resources to block authorised users. Current DDoS detection methods depend on network topology, cannot detect all types of attacks, use outdated or invalid datasets, and require powerful and expensive infrastructure hardware. In our research, we filter non-legitimate traffic and use machine learning classifiers to predict attack types from attack traffic. These two processes together reduce attack volume and identify attack type to provide a comprehensive DDoS protection and enable targeted reaction and mitigation. We used CIC DDoS 2019 data for the experiment. It records MSSQL, PortMap, LDAP, NetBIOS, Syn, UDP, UDPLag, and benign traffic attacks. Experiments yield promising and satisfying results.",Low
"Quantitative characterizations and estimations of uncertainty are of fundamental importance for machine learning classification, particularly in safety-critical settings where continuous real-time monitoring requires explainable and reliable scoring. Reliance on the maximum a posteriori principle to determine label classification can obscure the certainty of a label assignment. We develop a theoretical framework for quantitative scores of certainty and competence based on predicted probability estimates, formally prove their properties, and empirically confirm the inferential power of these properties across different data modalities, tasks and model architectures. Our theoretical results establish that competent models have distinct distributions of certainty for true and false positives conditioned on inputs similar to training and testing data, and prove that this framework provides a reliable means to infer the quality of model predictions and detect false positives. Our empirical results bear out that there are distinct distributions of certainty scores on training and holdout data, as well as data that is a priori out-of-distribution. For expert models, at least 62.1% of false positives could be identified when using a cut-off at at the bottom 5% TP threshold. Further, we found a strong negative correlation between empirical competence and the FPR95TPR rate for EnergyBased out-of-distribution (OOD) detectors. Finally, we developed two forms of an OOD detector that were able to reliably distinguish in-distribution data from OOD data for both frequentist and Bayesian models, performing better on average than previous state-of-the-art EnergyBased OOD detection methods, and improving upon the baseline Monte Carlo Dropout AUPR-OUT performance on average by 14.4% and 16.5%, and reducing the FPR95TPR by 54.2% and 37.6%.",Low
"Real-time lightweight time series anomaly detection has become increasingly crucial in cybersecurity and many other domains. Its ability to adapt to unforeseen pattern changes and swiftly identify anomalies enables prompt responses and critical decision-making. While several such anomaly detection approaches have been introduced in recent years, they primarily utilize a single type of recurrent neural networks (RNNs) and have been implemented in only one deep learning framework. It is unclear how the use of different types of RNNs available in various deep learning frameworks affects the performance of these anomaly detection approaches due to the absence of comprehensive evaluations. Arbitrarily choosing a RNN variant and a deep learning framework to implement an anomaly detection approach may not reflect its true performance and could potentially mislead users into favoring one approach over another. In this paper, we aim to study the influence of various types of RNNs available in popular deep learning frameworks on real-time lightweight time series anomaly detection. We reviewed several state-of-the-art approaches and implemented a representative anomaly detection approach using well-known RNN variants supported by three widely recognized deep learning frameworks. A comprehensive evaluation is then conducted to analyze the performance of each implementation across real-world, open-source time series datasets. The evaluation results provide valuable guidance for selecting the appropriate RNN variant and deep learning framework for real-time, lightweight time series anomaly detection.",Low
"Real-time network- and host-based Anomaly Detection Systems (ADSs) transform a continuous stream of input data into meaningful and quantifiable anomaly scores. These scores are subsequently compared to a fixed detection threshold and classified as either benign or malicious. We argue that a real-time ADSâ€™ input changes considerably over time and a fixed threshold value cannot guarantee good anomaly detection accuracy for such a time-varying input. In this article, we propose a simple and generic technique to adaptively tune the detection threshold of any ADS that works on threshold method. To this end, we first perform statistical and information-theoretic analysis of network- and host-based ADSsâ€™ anomaly scores to reveal a consistent time correlation structure during benign activity periods. We model the observed correlation structure using Markov chains, which are in turn used in a stochastic target tracking framework to adapt an ADSâ€™ detection threshold in accordance with real-time measurements. We also use statistical techniques to make the proposed algorithm resilient to sporadic changes and evasion attacks. In order to evaluate the proposed approach, we incorporate the proposed adaptive thresholding module into multiple ADSs and evaluate those ADSs over comprehensive and independently collected network and host attack datasets. We show that, while reducing the need of human threshold configuration, the proposed technique provides considerable and consistent accuracy improvements for all evaluated ADSs.",Medium
"Real-time outlier detection from a data stream is an increasingly important problem, especially as sensor-generated data streams abound in many applications owing to the prevalence of IoT and emergence of digital twins. Several density-based approaches have been proposed to address this problem, but arguably none of them is fast enough to meet the performance demand of real applications. This paper is founded upon a novel observation that, in many regions of the data space, data distributions hardly change across window slides. We propose a new algorithm, abbr. STARE, which identifies local regions in which data distributions hardly change and then skips updating the densities in those regions-a notion called stationary region skipping. Two techniques, data distribution approximation and cumulative net-change-based skip, are employed to efficiently and effectively implement the notion. Extensive experiments using synthetic and real data streams as well as a case study show that STARE is several orders of magnitude faster than the existing algorithms while achieving comparable or higher accuracy.",Medium
"Real-world graphs are dynamic, constantly evolving with new interactions, such as financial transactions in financial networks. Temporal Graph Neural Networks (TGNNs) have been developed to effectively capture the evolving patterns in dynamic graphs. While these models have demonstrated their superiority, being widely adopted in various important fields, their vulnerabilities against adversarial attacks remain largely unexplored. In this paper, we propose T-SPEAR, a simple and effective adversarial attack method for link prediction on continuous-time dynamic graphs, focusing on investigating the vulnerabilities of TGNNs. Specifically, before the training procedure of a victim model, which is a TGNN for link prediction, we inject edge perturbations to the data that are unnoticeable in terms of the four constraints we propose, and yet effective enough to cause malfunction of the victim model. Moreover, we propose a robust training approach T-SHIELD to mitigate the impact of adversarial attacks. By using edge filtering and enforcing temporal smoothness to node embeddings, we enhance the robustness of the victim model. Our experimental study shows that T-SPEAR significantly degrades the victim model's performance on link prediction tasks, and even more, our attacks are transferable to other TGNNs, which differ from the victim model assumed by the attacker. Moreover, we demonstrate that T-SHIELD effectively filters out adversarial edges and exhibits robustness against adversarial attacks, surpassing the link prediction performance of the na\""{\i}ve TGNN by up to 11.2% under T-SPEAR. The code and datasets are available at https://github.com/wooner49/T-spear-shield.",Low
"Recent botnet activities targeting IoT infrastructure and turning computing devices into cryptocurrency miners indicate an increase in the botnet attack surface and capabilities. These facts emphasize the importance of investigating alternative methods for detecting botnets. One of them is using stream mining algorithms to classify malicious network traffic. Although some initiatives seek to adopt stream mining strategies to detect botnets, several research topics still need to be discussed. Our goal is to compare the use of single and ensemble-based stream mining algorithms to identify botnet network flows. Since obtaining examples of malicious network flows could be a hassle to security managers, we also investigate whether the use of ensembles could reduce the number of labeled instances required to update the classification model. Our results indicate that the ensemble-based Ozaboost algorithm with the prequential evaluation strategy outperforms the other selected algorithms. We also found that ensemble-based algorithms and some botnet characteristics (C&amp;C communication protocol) requires less labeled instances while maintains high performance.",Medium
"Recent years have witnessed the proliferation of learning-based Android malware detectors. These detectors can be categorized into three types, String-based, Image-based and Graph-based. Most of them have achieved good detection performance under the ideal setting. In reality, however, detectors often face out-of-distribution samples due to the factors such as code obfuscation, concept drift (e.g., software development technique evolution and new malware category emergence), and adversarial examples (AEs). This problem has attracted increasing attention, but there is a lack of comparative studies that evaluate the existing various types of detectors under these challenging environments. In order to fill this gap, we select 12 representative detectors from three types of detectors, and evaluate them in the challenging scenarios involving code obfuscation, concept drift and AEs, respectively. Experimental results reveal that none of the evaluated detectors can maintain their ideal-setting detection performance, and the performance of different types of detectors varies significantly under various challenging environments. We identify several factors contributing to the performance deterioration of detectors, including the limitations of feature extraction methods and learning models. We also analyze the reasons why the detectors of different types show significant performance differences when facing code obfuscation, concept drift and AEs. Finally, we provide practical suggestions from the perspectives of users and researchers, respectively. We hope our work can help understand the detectors of different types, and provide guidance for enhancing their performance and robustness.",Medium
"Recently, general understanding of the importance of system security has increased and has been reflected in undergraduate and graduate curricula that offer security courses. These security education components are certainly a step in the right direction and should contribute to increased attention to security during the design and development phases of software and system engineering. However, the importance of system security is not always reflected in the quantity of required security courses or the budget allocated for such courses. One way for getting the most ""bang for the buck"" when it comes to security education is to have a small set of concentrated security courses that pair hands-on labs and lasting security principles; then, augment these security courses with additional security components and activities in other required courses from the curriculum. To provide education that is both relevant now and in the future, we use a principles-based approach together with hands-on security labs. We offer seven strategies for making this approach successful. Based on direct observation, student surveys, anecdotal evidence, and job placement, we believe that this approach has been successful in producing graduates both conscious of security issues and capable of designing and developing secure systems.",Low
"Research highlights We develop an improved incremental SVM algorithm, named RS-ISVM, to deal with network intrusion detection. To reduce the noise generated by feature differences, we propose a modified kernel function U-RBF, with the mean and mean square difference values of feature attributes embedded in kernel function RBF. Given the oscillation problem that usually occurs in traditional incremental SVM's follow-up learning process, we present a reserved set strategy which can keep those samples that are more likely to be the support vectors in the following computation process. Moreover, in order to shorten the training time, a concentric circle method is suggested to be used in selecting samples to form the reserved set. We develop an improved incremental SVM algorithm, named RS-ISVM, to deal with network intrusion detection. To reduce the noise generated by feature differences, we propose a modified kernel function U-RBF, with the mean and mean square difference values of feature attributes embedded in kernel function RBF. Then, given the oscillation problem that usually occurs in traditional incremental SVM's follow-up learning process, we present a reserved set strategy which can keep those samples that are more likely to be the support vectors in the following computation process. Moreover, in order to shorten the training time, a concentric circle method is suggested to be used in selecting samples to form the reserved set. Academic researches and data experiments show that RS-ISVM can ease the oscillation phenomenon in the learning process and achieve pretty good performance, meanwhile, its reliability is relative high.",Medium
"Robustness to malicious attacks is of paramount importance for distributed learning. Existing works often consider the classical Byzantine attacks model, which assumes that some workers can send arbitrarily malicious messages to the server and disturb the aggregation steps of the distributed learning process. To defend against such worst-case Byzantine attacks, various robust aggregators have been proven effective and much superior to the often-used mean aggregator. In this paper, we show that robust aggregators are too conservative for a class of weak but practical malicious attacks, as known as label poisoning attacks, where the sample labels of some workers are poisoned. Surprisingly, we are able to show that the mean aggregator is more robust than the state-of-the-art robust aggregators in theory, given that the distributed data are sufficiently heterogeneous. In fact, the learning error of the mean aggregator is proven to be optimal in order. Experimental results corroborate our theoretical findings, demonstrating the superiority of the mean aggregator under label poisoning attacks.",Low
"Rule-based intrusion detection systems generally rely on hand crafted signatures developed by domain experts. This could lead to a delay in updating the signature bases and potentially compromising the security of protected systems. In this paper, we present a biologically-inspired computational approach to dynamically and adaptively learn signatures for network intrusion detection using a supervised learning classifier system. The classifier is an online and incremental parallel production rule-based system. A signature extraction system is developed that adaptively extracts signatures to the knowledge base as they are discovered by the classifier. The signature extraction algorithm is augmented by introducing new generalisation operators that minimise overlap and conflict between signatures. Mechanisms are provided to adapt main algorithm parameters to deal with online noisy and imbalanced class data. Our approach is hybrid in that signatures for both intrusive and normal behaviours are learnt. The performance of the developed systems is evaluated with a publicly available intrusion detection dataset and results are presented that show the effectiveness of the proposed system.",Medium
"Security concerns are a major deterrent in many applications wireless sensor networks are envisaged to support. To date, various security mechanisms have been proposed for these networks dealing with either Medium Access Control (MAC) layer or network layer security issues, or key management problems. Security visualization is the latest weapon that has been added in the arsenal of a security officer who is tasked with detecting network anomalies by analyzing large amounts of audit data. This paper proposes a novel security visualization system for analyzing and detecting complex patterns of sensor network attacks, called SRNET. Both selective forwarding and jamming attacks are identified through visualizing and analyzing network traffic data on multiple coordinated views, namely the multidimensional crossed view, the crossed view perspective, and the track area view. Through simulations, we demonstrate that SRNET is able to help detect and further identify the root cause of the aforementioned sensor network attacks.",Low
"Security threats for computer networks have increased dramatically over the last decade, becoming bolder and more brazen. There is a strong need for effective Intrusion Detection Systems IDS that are designed to interpret intrusion attempts in incoming network traffic intelligently. In this paper, the authors explored the capabilities of Deep Belief Networks DBN-one of the most influential deep learning approach-in performing intrusion detection after training with the NSL-KDD dataset. Additionally, they examined the impact of using Extreme Learning Machine ELM and Regularized ELM on the same dataset to evaluate the performance against DBN and Support Vector Machine SVM approaches. The trained system identifies any type of unknown attack in the dataset examined. In addition to detecting attacks, the proposed system also classifies them into five groups. The implementation with DBN and SVM give a testing accuracy of about 97.5% and 88.33% respectively with 40% of training data selected from the NSL-KDD dataset. On the other hand, the experimental results show around 98.20% and 98.26% testing accuracy respectively for ELM and RELM after reducing the data dimensions from 41 to 9 essential features with 40% training data. ELM and RELM perform better in terms of testing accuracy upon comparison with DBN and SVM.",Low
"Self-developed malware was usually used by advanced persistent threat (APT) attackers to launch APT attacks. Therefore, we can enhance the understanding and cognition of APT attacks by comprehending the behavior of APT malware. Unfortunately, the current research cannot effectively explain the relationship between the recognition, detection, and defense of APT. The model of similar studies also lacks an explanation about it. To defend against APT attacks and inquire about the similarity of different APT attacks, this study proposes an APT malware classification method based on a combination of multiple deep learning algorithms and transfer learning by collecting malware used in several famous APT groups in public. By extracting the application programming interface (API) system calls, with the vector representation of features by combining dynamic LSTM and attention algorithm, we can obtain API at different APT families classification contributions trained dynamic. Thus, we used transfer learning to perform multiple classifications of the APT family. This study aims to reduce the burden of network security staff from reviewing a large number of suspicious files when defending against APT attacks. Additionally, it can effectively intercept them in the initial invasion stage of APT to perform targeted defense against specific APT attacks by combining threat intelligence in public. The experimental result shows that the proposed method can achieve 99.2% in distinguishing common malware from APT malware and assign APT malware to different APT families with an accuracy of 95.5%.",Medium
"Since the first computer virus hit the Advanced Research Projects Agency Network (ARPANET) in the early 1970s, the security community interest revolved around ways to expose the identities of malware writers. Knowledge of the adversarial identities promised additional leverage to security experts in their ongoing battle against those perpetrators. At the dawn of computing era, when malware writers and malicious software were characterized by the lack of experience and relative simplicity, the task of uncovering the identities of virus writers was more or less straightforward. Manual analysis of source code often revealed personal, identifiable information embedded by authors themselves. But these times have long gone. Modern day's malware writers extensively use numerous malware code generators to mass produce new variants and employ advanced obfuscation techniques to hide their identities. As a result the work of security experts trying to uncover the identities of malware writers became significantly more challenging and time consuming. To gain insight into the identity of an adversary, we turn our attention to authorship attribution research, which offers a broad spectrum of techniques for identifying an author of a document, based on the analysis of an author's writing style. Equipped with these methods, we explore attribution of Android binaries and the role of features related to the development process on the determination of Android binary authorship.Within this context, we propose an incremental approach to perform authorship attribution of Android apps. First to a set of known authors and then the generation of new profiles for unknown apps. We assess the effectiveness of our approach on several sets of malicious and legitimate Android binaries produced by actual developers, as opposed to using artificially created authors' data. We achieve 97.5% accuracy on these authorsÂ» data. We further evaluate our approach on more than 131,000 apps collected from various sources including 10 different markets around the globe.",Medium
"Smart grid is an alternative solution of the conventional power grid which harnesses the power of the information technology to save the energy and meet todays' environment requirements. Due to the inherent vulnerabilities in the information technology, the smart grid is exposed to wide variety of threats that could be translated into cyber-attacks. In this paper, we develop a deep learning-based intrusion detection system to defend against cyber-attacks in the advanced metering infrastructure network. The proposed machine learning approach is trained and tested extensively on an empirical industrial dataset which is composed of several attack' categories including the scanning, buffer overflow, and denial of service attacks. Then, an experimental comparison in terms of detection accuracy is conducted to evaluate the performance of the proposed approach with Na\""{\i}ve Bayes, Support Vector Machine, and Random Forest. The obtained results suggest that the proposed approaches produce optimal results comparing to the other algorithms. Finally, we propose a network architecture to deploy the proposed anomaly-based intrusion detection system across the Advanced metering infrastructure network. In addition, we propose a network security architecture composed of two types of Intrusion detection system types, Host and Network based, deployed across the Advanced Metering Infrastructure network to inspect the traffic and detect the malicious one at all the levels.",Low
"Social media have been growing rapidly and become essential elements of many peopleâ€™s lives. Meanwhile, social media have also come to be a popular source for identity deception. Many social media identity deception cases have arisen over the past few years. Recent studies have been conducted to prevent and detect identity deception. This survey analyzes various identity deception attacks, which can be categorized into fake profile, identity theft, and identity cloning. This survey provides a detailed review of social media identity deception detection techniques. It also identifies primary research challenges and issues in the existing detection techniques. This article is expected to benefit both researchers and social media providers.",Low
"SQL injection attacks potentially affect all applications, especially web applications, that utilize a database backend. While these attacks are generally against the applications and not the database directly, there are some techniques that can be deployed to mitigate the risk at the database server. Database intrusion detection systems are often based on signatures of known exploits and honey tokens, traps set in the database. This paper examines the threat from SQL injection attacks, the reasons traditional database access control is not sufficient to stop them, and some of the techniques used to detect them. Moreover, it proposes a model for an anomalous SQL detector which observes the database traffic from the perspective of the database server itself. The proposed anomaly model can be used in conjunction with the existing methods to give the database server a way to mitigate the SQL injection risk that is a major application security problem.",Low
"Stock data are characterized by high dimensionality and sparsity, making stock trend prediction highly challenging. Although the Light Gradient Boosting Machine (LightGBM), based on web semantics, excels at capturing global features and efficiently performs in stock trend prediction, it does not consider the issue of declining prediction performance caused by the changing distribution of stock data over time (concept drift phenomenon). Accordingly, this work introduces the Convolutional Neural Network (CNN) into the prediction model to leverage its ability to effectively capture local features. Additionally, local features are combined with global features to obtain a comprehensive set of feature information. Lastly, the model processes new data in real-time, continuously learns new knowledge, updates model parameters, and effectively addresses the decline in model performance caused by concept drift. Experimental results demonstrate that the proposed model outperforms other models indicating its ability to efficiently perform well in stock trend prediction.",Medium
"Stream computing paradigm, with the characteristics of real-time arrival and departure, has been admitted as a major computing paradigm in big data. Relevant theories are flourishing recently with the surge development of stream computing platforms such as Storm, Kafka and Spark. Rough set theory is an effective tool to extract knowledge with imperfect information, however, related discussions on synchronous immigration and emigration of objects have not been investigated. In this paper, stream computing learning method is proposed on the basis of existing incremental learning studies. This method aims at solving challenges resulted from simultaneous addition and deletion of objects. Based on novel learning method, a stream computing algorithm called single-object stream-computing-based three-way decisions (SS3WD) is developed. In this algorithm, the probabilistic rough set model is applied to approximate the dynamic variation of concepts. Three-way regions can be determined without multiple scans of existing information granular. Extensive experiments not only demonstrate better efficiency and robustness of SS3WD in the presence of objects streaming variation, but also illustrate that stream computing learning method is an effective computing strategy for big data. Present stream computing learning method to solve the stream mining tasks.Formalize the problem knowledge updating of stream computing in the semantic of three-way decisions.Present a new method SS3WD for knowledge updating of stream computing.Correctness and Completeness of SS3WD is demonstrated theoretically and practically.Extend the application domain of three-way decisions to stream computing.",Medium
"Stream Machine Learning is rapidly gaining popularity within the network monitoring community as the big data produced by network devices and end-user terminals goes beyond the memory constraints of standard monitoring equipment. We consider a stream-based machine learning approach to network security, conceiving adaptive machine learning algorithms for the analysis of continuously evolving network data streams. Using a sliding-window adaptive-size approach, we show that adaptive random forests models are able to keep up with important concept drifts in the underlying network data streams, by keeping high accuracy with continuous re-training at concept drift detection times.",Medium
"Streaming media server is the core system of audio and video application in the Internet; it has a wide range of applications in music recommendation. As song libraries and users of music websites and APPs continue to increase, user interaction data are generated at an increasingly fast rate, making the shortcomings of the original offline recommendation system and the advantages of the real-time streaming recommendation system more and more obvious. This paper describes in detail the working methods and contents of each stage of the real-time streaming music recommendation system, including requirement analysis, overall design, implementation of each module of the system, and system testing and analysis, from a practical scenario. Moreover, this paper analyzes the current research status and deficiencies in the field of music recommendation by analyzing the user interaction data of real music websites. From the actual requirements of the system, the functional and performance goals of the system are proposed to address these deficiencies, and then the functional structure, general architecture, and database model of the system are designed, and how to interact with the server side and the client side is investigated. For the implementation of data collection and statistics module, this paper adopts Flume and Kafka to collect user behavior data and uses Spark Streaming and Redis to count music popularity trends and support efficient query. The recommendation engine module in this paper is designed and optimized using Spark to implement incremental matrix decomposition on data streams, online collaborative topic model, and improved item-based collaborative filtering algorithm. In the system testing section, the functionality and performance of the system are tested, and the recommendation engine is tested with real datasets to show the discovered music themes and analyze the test results in detail.",Low
"Supervised learning, while deployed in real-life scenarios, often encounters instances of unknown classes. Conventional algorithms for training a supervised learning model do not provide an option to detect such instances, so they miss-classify such instances with 100% probability. Open Set Recognition (OSR) and Non-Exhaustive Learning (NEL) are potential solutions to overcome this problem. Most existing methods of OSR first classify members of existing classes and then identify instances of new classes. However, many of the existing methods of OSR only makes a binary decision, i.e., they only identify the existence of the unknown class. Hence, such methods cannot distinguish test instances belonging to incremental unseen classes. On the other hand, the majority of NEL methods often make a parametric assumption over the data distribution, which either fail to return good results, due to the reason that real-life complex datasets may not follow a well-known data distribution. In this paper, we propose a new online non-exhaustive learning model, namely, Non-Exhaustive Gaussian Mixture Generative Adversarial Networks (NE-GM-GAN) to address these issues. Our proposed model synthesizes Gaussian mixture based latent representation over a deep generative model, such as GAN, for incremental detection of instances of emerging classes in the test data. Extensive experimental results on several benchmark datasets show that NE-GM-GAN significantly outperforms the state-of-the-art methods in detecting instances of novel classes in streaming data.",Medium
"Tensors are higher order generalizations of matrices to model multi-aspect data, e.g., a set of purchase records with the schema (user_id, product_id, timestamp, feedback). Tensor factorization is a powerful technique for generating a model from a tensor, just like matrix factorization generates a model from a matrix, but with higher accuracy and richer information as more attributes are available in a higher- order tensor than a matrix. The data model obtained by tensor factorization can be used for classification, recommendation, anomaly detection, and so on. Though having a broad range of applications, tensor factorization has not been popularly applied compared with matrix factorization that has been widely used in recommender systems, mainly due to the high computational cost and poor scalability of existing tensor factorization methods. Efficient and scalable tensor factorization is particularly challenging because real world tensor data are mostly sparse and massive. In this paper, we propose a novel distributed algorithm, called Lock-Free Tensor Factorization (LFTF), which significantly improves the efficiency and scalability of distributed tensor factorization by exploiting asynchronous execution in a re-formulated problem. Our experiments show that LFTF achieves much higher CPU and network throughput than existing methods, converges at least 17 times faster and scales to much larger datasets.",Low
The aim is to improve cities' management of natural and municipal resources and in turn the quality of life of their citizens.,Low
"The application of the Internet of Things (IoT) is highly expected to have comprehensive economic, business, and societal implications for our smart lives; indeed, IoT technologies play an essential role in creating a variety of smart applications that improve the nature and well-being of life in the real world. Consequently, the interconnected nature of IoT systems and the variety of components of their implementation have given rise to new security concerns. Cyber-attacks and threats in the IoT ecosystem significantly impact the development of new intelligent applications. Moreover, the IoT ecosystem suffers from inheriting vulnerabilities that make its devices inoperable to benefit from instigating security techniques such as authentication, access control, encryption, and network security. Recently, great advances have been achieved in the field of Machine Intelligence (MI), Deep Learning (DL), and Machine Learning (ML), which have been applied to many important applications. ML and DL are regarded as efficient data exploration techniques for discovering â€œnormalâ€ and â€œabnormalâ€ IoT component and device behavior inside the IoT ecosystem. Therefore, ML/DL approaches are required to convert the security of IoT systems from providing safe Device-to-Device (D2D) communication to providing security-based intelligence systems. The proposed work examines ML/DL technologies that may be utilized to provide superior security solutions for IoT devices. The potential security risks associated with the IoT are discussed, including pre-existing and newly emerging threats. Furthermore, the benefits and challenges of DL and ML techniques are examined to enhance IoT security.",Low
"The approximate membership query (AMQ) data structure is a kind of space-efficient probabilistic data structure. It can approximately indicate whether an element exists in a set. The AMQ data structure has been widely used in network measurements, network security, network caching, etc. Resizing is an extensively utilized operation of the AMQ data structure, but it can lead to system performance degradation. We summarize two main problems that lead to such degradation. Specifically, one of them is that the resizing operation can block other operations, while the other one is that the throughput of AMQ structures will deteriorate after multiple resizing operations due to more computation cost. However, existing related work cannot alleviate both of them. Therefore, we propose a novel AMQ data structure called bamboo filters, which can alleviate the two problems simultaneously. Bamboo filters can insert, look up, and delete an element in constant time. They can also dynamically resize in a fine-grained way. Furthermore, we propose space utilization adaptive bamboo filters that adaptively trigger resizing operations according to the space utilization, thereby achieving lower average memory consumption. Experimental results show that our scheme significantly outperforms state-of-the-art work. Especially, bamboo filters achieve &lt;inline-formula&gt; &lt;tex-math notation=""LaTeX""&gt;$2.12times $ &lt;/tex-math&gt;&lt;/inline-formula&gt; lookup throughput of the logarithmic dynamic cuckoo filter.",Low
"The automotive industry is undoubtedly taking giant strides toward a paradigm shift.&nbsp;In essence, wireless network communication and artificial intelligence technologies are stimulating the gradual evolution of the autonomy of intelligent vehicles. This shift causes a divergence in vehicle architecture to become assembled with&nbsp;software-driven rather than mechanical-driven components, producing an integrated connected central unit that perceives and processes the surrounding environment, makes autonomous decisions, and controls the entire vehicle. The emerging vehicular network technologies, including vehicle-to-everything and in-vehicle channels, facilitate wired and wireless bidirectional communication within the vehicle and to other vehicles, infrastructure actors, and the Cloud to integrate it with its surrounding environment in real-time. Despite the promised potential benefits of intelligent vehicles, including improved mobility, driving safety, and economic and environmental gains, such increased network connectivity and complexity expose them to a vast attack surface. Researchers have identified a wide range of internal and external security threats due to connectivity and automation vulnerabilities within the vehicle-to-everything wireless network channels and the lack of core security measures, such as authentication, authorization, and encryption within the in-vehicle network. The dynamic nature of these vehicular networks and their ever-changing threat landscape originate new pressing security challenges that can cause severe safety destruction. In this paper, we propose BoostSec, a novel online security analytics solution that leverages advanced incremental ensemble learning to provide robust, rapid, and adaptive protection of vehicular networks against known and unknown attacks. We further augment the proposed solution with an agnostic interpretability analysis of the results. We conducted extensive experiments on three publicly available benchmark datasets representing vehicular environments in various contexts. The experimental evaluation proves that the proposed framework outpaces current baseline approaches and meets the challenges with remarkable performance, demonstrated by its (1) generalization covering a wide range of attacks across various vehicular network contexts; (2) real-time analysis reflected in efficient computation footprint; (3) adaptability against unseen attacks; (4) robustness against adversarial attacks; and (5) augmented interpretability analysis.",High
"The bumps and dips in data streams are valuable patterns for data mining and networking scenarios such as online advertising and botnet detection. In this paper, we define the wave, a data stream pattern with a serious deviation from the stable arrival rate for a period of time. We then propose Pontus, an efficient framework for wave detection and estimation. In Pontus, a lightweight data structure is utilized for the preliminary processing of incoming packets in the data plane to take advantage of its high processing speed; then, the powerful control plane carries out computationally intensive wave detection and estimation. In particular, we propose the Multi-Stage Progressive Tracking strategy which detects waves in stages and removes any disqualified items promptly to save memory. Hash collisions are addressed by a Stage Variance Maximization technique to reduce estimation error. Moreover, we prove the theoretical error bound and establish upper bounds of false positive and false negative. Experiment results show that the software version of Pontus can achieve around 97% F1-Score even under scarce memory when baselines fail. Furthermore, the implemented prototype of Pontus based on P4 achieves 842x higher throughput than the baseline strawman solution.",Medium
"The concept drift phenomenon describes how the statistical properties of a data distribution change over time. In cybersecurity domain, where data arrives continuously and rapidly in a sequential manner, concept drift can be a significant challenge. Identifying concept drift, it enables security analysts to detect emerging attacks, respond promptly, and make informed decisions based on the changing nature of the data being analyzed. The Adaptive Reservoir Neural Gas (AR-NG) clustering algorithm is proposed in this paper to handle concept drift in real-time data streams. It is a novel approach that combines reservoir computing power with the neural gas algorithm, allowing the algorithm to automatically update its clustering structure as new data arrives. Furthermore, in order to effectively handle evolving data streams that significantly change over time in unexpected ways, the proposed method incorporates a density-based clustering mechanism (DBCM) to concept drift detection. Experiments on real-time data streams show that the proposed algorithm is effective at mitigating the impact of concept drift, making it a useful tool for real-time data analysis and decision-making in dynamic environments.",Medium
"The Controller Area Network (CAN) is the most widely used in-vehicle communication protocol, which still lacks the implementation of suitable security mechanisms such as message authentication and encryption. This makes the CAN bus vulnerable to numerous cyber attacks. Various Intrusion Detection Systems (IDSs) have been developed to detect these attacks. However, the high generalization capabilities of Artificial Intelligence (AI) make AI-based IDS an excellent countermeasure against automotive cyber attacks. This article surveys AI-based in-vehicle IDS from 2016 to 2022 (August) with a novel taxonomy. It reviews the detection techniques, attack types, features, and benchmark datasets. Furthermore, the article discusses the security of AI models, necessary steps to develop AI-based IDSs in the CAN bus, identifies the limitations of existing proposals, and gives recommendations for future research directions.",Low
"The deployment of learning-based models to detect malicious activities in network traffic flows is significantly challenged by concept drift. With evolving attack technology and dynamic attack behaviors, the underlying data distribution of recently arrived traffic flows deviates from historical empirical distributions over time. Existing approaches depend on a significant amount of labeled drifting samples to facilitate the deep model to handle concept drift, which faces labor-intensive manual labeling and the risk of label noise. In this paper, we propose ReCDA, a Concept Drift Adaptation method with Representation enhancement, which consists of a self-supervised representation enhancement stage and a weakly-supervised classifier tuning stage. Specifically, in the initial stage, ReCDA introduces drift-aware perturbation and representation alignment to facilitate the model in acquiring robust representations from drift-aware and drift-invariant perspectives. Moreover, in the subsequent stage, a meticulously crafted instructive sampling strategy and a robust representation constraint encourage the model to learn discriminative knowledge about benign and malicious activities during fine-tuning, thereby enhancing performance further. We conduct comprehensive evaluations on several benchmark datasets under varying degrees of concept drift. The experiment results demonstrate the superior adaptability and robustness of the proposed method.",Medium
"The development of the Internet has led to the complexity of network encrypted traffic. Identifying the specific classes of network encryption traffic is an important part of maintaining information security. The traditional traffic classification based on machine learning largely requires expert experience. As an end-to-end model, deep neural networks can minimize human intervention. This paper proposes the CLD-Net model, which can effectively distinguish network encrypted traffic. By segmenting and recombining the packet payload of the raw flow, it can automatically extract the features related to the packet payload, and by changing the expression of the packet interval, it integrates the packet interval information into the model. We use the ability of Convolutional Neural Network (CNN) to distinguish image classes, learn and classify the grayscale images that the raw flow has been preprocessed into, and then use the effectiveness of Long Short-Term Memory (LSTM) network on time series data to further enhance the modelâ€™s ability to classify. Finally, through feature reduction, the high-dimensional features learned by the neural network are converted into 8 dimensions to distinguish 8 different classes of network encrypted traffic. In order to verify the effectiveness of the CLD-Net model, we use the ISCX public dataset to conduct experiments. The results show that our proposed model can distinguish whether the unknown network traffic uses Virtual Private Network (VPN) with an accuracy of 98% and can accurately identify the specific traffic (chats, audio, or file) of Facebook and Skype applications with an accuracy of 92.89%.",Low
"The envisioned sixth-generation (6G) networks anticipate robust support for diverse applications, including massive machine-type communications, ultra-reliable low-latency communications, and enhanced mobile broadband. Intelligent Reflecting surface (IRS) have emerged as a key technology capable of intelligently reconfiguring wireless propagation environments, thereby enhancing overall network performance. Traditional optimization techniques face limitations in meeting the stringent performance requirements of 6G networks due to the intricate and dynamic nature of the wireless environment. Consequently, deep learning (DL) techniques are employed within the IRS framework to optimize wireless system performance. This article provides a comprehensive survey of the latest research in DL-aided IRS models, covering optimal beamforming, resource allocation control, channel estimation and prediction, signal detection, and system deployment. The focus is on presenting promising solutions within the constraints of different hardware configurations. The survey explores challenges, opportunities, and open research issues in DL-aided IRS, considering emerging technologies such as digital twins, computer vision, blockchain, network function virtualization, integrated sensing and communication, software-defined networking, mobile edge computing, unmanned aerial vehicles, and non-orthogonal multiple access. Practical design issues associated with these enabling technologies are also discussed, providing valuable insights into the current state and future directions of this evolving field.",Low
"The existing detection methods can either only classify known types of cyberattacks or only distinguish network anomalies to identify whether unknown cyberattacks are present, they are unable to distinguish both known and unknown cyberattack types. To solve these problems, a causal transformer-based cyberattack detection method is proposed. This method aims to eliminate false associations caused by noise features through causal attention to obtain an intelligently interpretable detection method that can classify known attacks and unknown attack types. Validation is performed on two broad and representative datasets. The results show that the proposed causal transformer detection method can not only correctly classify known attacks but also achieve a 100% success rate in identifying cyberattacks on some datasets. Additionally, more than 99% of unknown attack types can be effectively identified and classified, providing timely and effective guidance for cybersecurity defense.",Low
"The existing Industrial Internet of Things (IIoT) temporal data analysis methods often suffer from issues such as information loss, difficulty balancing spatial and temporal features, and being affected by training data noise, which can lead to varying degrees of reduced model accuracy. Therefore, a new anomaly detection method was proposed, which integrated Transformer and adversarial training. Firstly, a bidirectional spatiotemporal feature extraction module was constructed by combining Graph Attention Networks (GAT) and Bidirectional Gated Recurrent Unit (BiGRU), which can simultaneously extract spatial and temporal features. Then, by combining multi-scale convolution with Long Short-Term Memory (LSTM), multi-scale contextual information was captured. Finally, an improved Transformer was used to fuse multi-dimensional features, combined with an adversarial-trained variational autoencoder to calculate the anomalies of the input data. This method outperforms other comparison models by conducting experiments on four publicly available datasets.",Low
"The field of cysec is evolving fast. Security professionals are in need of intelligence on past, current and â€”ideally â€” upcoming threats, because attacks are becoming more advanced and are increasingly targeting larger and more complex systems. Since the processing and analysis of such large amounts of information cannot be addressed manually, cysec experts rely on machine learning techniques. In the textual domain, pre-trained language models such as Bidirectional Encoder Representations from Transformers (BERT) have proven to be helpful as they provide a good baseline for further fine-tuning. However, due to the domain-knowledge and the many technical terms in cysec, general language models might miss the gist of textual information. For this reason, we create a high-quality dataset1 and present a language model2 specifically tailored to the cysec domain that can serve as a basic building block for cybersecurity systems. The model is compared on 15 tasks: Domain-dependent extrinsic tasks for measuring the performance on specific problems, intrinsic tasks for measuring the performance of the internal representations of the model, as well as general tasks from the SuperGLUE benchmark. The results of the intrinsic tasks show that our model improves the internal representation space of domain words compared with the other models. The extrinsic, domain-dependent tasks, consisting of sequence tagging and classification, show that the model performs best in cybersecurity scenarios. In addition, we pay special attention to the choice of hyperparameters against catastrophic forgetting, as pre-trained models tend to forget the original knowledge during further training.",Medium
"The focus of this research is to develop a classifier using an artificial immune system (AIS) combined with population-based incremental learning (PBIL) and collaborative filtering (CF) for network intrusion detection. AIS is a powerful tool in terms of extirpating antigens inspired by the principles and processes of the natural immune system. PBIL uses past experiences to evolve into new species through learning and adopting the idea of CF for classification. The novelty of this research is in its combining of the three above mentioned approaches to develop a new classifier which can be applied to detect network intrusion, with incremental learning capability, by adapting the weight of key features. In addition, four mechanisms: creating a new antibody using PBIL, dynamic adjustment of feature weighting using clonal expansion, antibody hierarchy adjustment using mean affinity, as well as usage rates, are proposed to intensify AIS performance. As shown by the comparison carried out with other artificial intelligence and evolutionary computation approaches in network anomaly detection problems, our PBIL-AISCF classifier can achieve high accuracy for the benchmark problem.",Medium
"The growing complexity of security threats and the pervasive prevalence of cyberattacks have become more apparent in the present era, and the advent of big data, characterized by its distinctive features, has introduced layers of complexity to security tasks. Intrusion Detection Systems (IDSs) constitute a crucial line of defense, but their adaptation to the realm of big data is imperative. While traditional Machine Learning (ML)-based IDSs have been pivotal in detecting malicious patterns, they are often incapable to keep pace with the demands of expansive big data networks. This paper proposes a novel decentralized Multi-Agent Reinforcement Learning (MARL)-based IDS designed to address the specific challenges posed by big data. Our solution employs decentralized cooperative MARL, securing communicative channels throughout the detection process and concurrent data preprocessing which significantly reduces the overall processing time. Furthermore, the integration of Cloud computing and Big Data streaming techniques further facilitates real-time intrusion detection as cloudâ€™s resources allow rapid pre-process and analyse of massive data streams using powerful clusters. Likewise, Big Data streaming techniques ensure that potential intrusions are identified and addressed as they occur. Experimental results, conducted on the widely recognized NSLKDD benchmark dataset, demonstrate the superiority of our solution over other state-of-the-art approaches for big data networks, achieving an accuracy rate of 97.44%.",Medium
"The growth in the number of IoT devices and applications, as well as their heterogeneity and hardware limitations, make it difficult to apply traditional security mechanisms. In this way, the IoT layer has become a highly vulnerable part of the network. In this context, an intrusion detection system with low computational complexity is proposed for online recognition of denial-of-service attacks. A common feature of denial-of-service attacks is the sudden increase of a particular type of packet or request. To track this sudden increase, network traffic is first filtered by protocol, and then reduced to the number of packets over time. On these data, the techniques of sliding window and the comparison of moving averages, both adjustable by variables, are applied to identify the anomalies. Tests carried out on data extracted from pcap files, containing attacks carried out on real devices, demonstrate the accuracy in recognizing attacks. Furthermore, the tools and techniques for implementing the proposed model in a realistic environment are described.",Medium
"The identification of anomalous activities is a challenging and crucially important task in sensor networks. This task is becoming increasingly complex with the increasing volume of data generated in real-world domains, and greatly benefits from the use of predictive models to identify anomalies in real time. A key use case for this task is the identification of misbehavior that may be caused by involuntary faults or deliberate actions. However, currently adopted anomaly detection methods are often affected by limitations such as the inability to analyze large-scale data, a reduced effectiveness when data presents multiple densities, a strong dependence on user-defined threshold configurations, and a lack of explainability in the extracted predictions. In this paper, we propose a distributed deep learning method that extends growing hierarchical self-organizing maps, originally designed for clustering tasks, to address anomaly detection tasks. The SOM-based modeling capabilities of the method enable the analysis of data with multiple densities, by exploiting multiple SOMs organized as a hierarchy. Our map-reduce implementation under Apache Spark allows the method to process and analyze large-scale sensor network data. An automatic threshold-tuning strategy reduces user efforts and increases the robustness of the method with respect to noisy instances. Moreover, an explainability component resorting to instance-based feature ranking emphasizes the most salient features influencing the decisions of the anomaly detection model, supporting users in their understanding of raised alerts. Experiments are conducted on five real-world sensor network datasets, including wind and photovoltaic energy production, vehicular traffic, and pedestrian flows. Our results show that the proposed method outperforms state-of-the-art anomaly detection competitors. Furthermore, a scalability analysis reveals that the method is able to scale linearly as the data volume presented increases, leveraging multiple worker nodes in a distributed computing setting. Qualitative analyses on the level of anomalous pollen in the air further emphasize the effectiveness of our proposed method, and its potential in determining the level of danger in raised alerts.",Low
"The inadvertent stealing of private/sensitive information using Knowledge Distillation (KD) has been getting significant attention recently and has guided subsequent defense efforts considering its critical nature. Recent work Nasty Teacher proposed to develop teachers which can not be distilled or imitated by models attacking it. However, the promise of confidentiality offered by a nasty teacher is not well studied, and as a further step to strengthen against such loopholes, we attempt to bypass its defense and steal (or extract) information in its presence successfully. Specifically, we analyze Nasty Teacher from two different directions and subsequently leverage them carefully to develop simple yet efficient methodologies, named as HTC and SCM, which increase the learning from Nasty Teacher by upto 68.63% on standard datasets. Additionally, we also explore an improvised defense method based on our insights of stealing. Our detailed set of experiments and ablations on diverse models/settings demonstrate the efficacy of our approach.",Medium
"The increasing bulk of data generation in industrial and scientific applications has fostered practitioners' interest in mining large amounts of unlabeled data in the form of continuous, high speed, and time-changing streams of information. An appealing field is association stream mining, which models dynamically complex domains via rules without assuming any a priori structure. Different from the related frequent pattern mining field, its goal is to extract interesting associations among the forming features of such data, adapting these to the ever-changing dynamics of the environment in a pure online fashion-without the typical offline rule generation. These rules are adequate for extracting valuable insight which helps in decision making. This paper details Fuzzy-CSar, an online genetic fuzzy system designed to extract interesting rules from streams of samples. It evolves its internal model online, being able to quickly adapt its knowledge in the presence of drifting concepts. The different complexities of association stream mining are presented in a set of novel synthetic benchmark problems. Thus, the behavior of the online learning architecture presented is carefully analyzed under these conditions. Furthermore, the analysis is extended to real-world problems with static concepts, showing its competitiveness. Experiments support the advantages of applying Fuzzy-CSar to extract knowledge from large volumes of information.",Medium
"The increasing utilization of Electronic Control Units (ECUs) and wireless connectivity in modern vehicles has favored the emergence of security issues. Recently, several attacks have been demonstrated against in-vehicle networks therefore drawing significant attention. This paper presents an Intrusion Detection System (IDS) based on a regression learning approach which estimates certain parameters by using correlated/redundant data. The estimated values are compared to observed ones to identify abnormal contexts that would indicate intrusion. Experiments performed with real-world vehicular data have shown that more than 90% of vehicle speed data can be precisely estimated within the error bound of 3 kph. The proposed IDS is capable of detecting and localizing attacks in real-time, which is fundamental to achieve automotive security.",Low
"The increment of computer technology use and the continued growth of companies have enabled most financial transactions to be performed through the electronic commerce systems, such as using the credit card system, telecommunication system, healthcare insurance system, etc. Unfortunately, these systems are used by both legitimate users and fraudsters. In addition, fraudsters utilized different approaches to breach the electronic commerce systems. Fraud prevention systems (FPSs) are insufficient to provide adequate security to the electronic commerce systems. However, the collaboration of FDSs with FPSs might be effective to secure electronic commerce systems. Nevertheless, there are issues and challenges that hinder the performance of FDSs, such as concept drift, supports real time detection, skewed distribution, large amount of data etc. This survey paper aims to provide a systematic and comprehensive overview of these issues and challenges that obstruct the performance of FDSs. We have selected five electronic commerce systems; which are credit card, telecommunication, healthcare insurance, automobile insurance and online auction. The prevalent fraud types in those E-commerce systems are introduced closely. Further, state-of-the-art FDSs approaches in selected E-commerce systems are systematically introduced. Then a brief discussion on potential research trends in the near future and conclusion are presented.",Medium
"The Internet of Things (IoT) and Cyber-Physical Systems (CPS) are the backbones of Industry 4.0, where data quality is crucial for decision support. Data quality in these systems can deteriorate due to sensor failures or uncertain operating environments. Our objective is to summarize and assess the research efforts that address data quality in data-centric CPS/IoT industrial applications. We systematically review the state-of-the-art data quality techniques for CPS and IoT in Industry 4.0 through a systematic literature review (SLR) study. We pose three research questions, define selection and exclusion criteria for primary studies, and extract and synthesize data from these studies to answer our research questions. Our most significant results are (i) the list of data quality issues, their sources, and application domains, (ii) the best practices and metrics for managing data quality, (iii) the software engineering solutions employed to manage data quality, and (iv) the state of the data quality techniques (data repair, cleaning, and monitoring) in the application domains. The results of our SLR can help researchers obtain an overview of existing data quality issues, techniques, metrics, and best practices. We suggest research directions that require attention from the research community for follow-up work.",Low
"The Internet of Things (IoT) has seen it all from being just another innovation to a leading technology; it is now a binding force that interconnects various aspects of our lives. The IoT's tremendous growth is driven by emerging applications and evolving business models, reinforced by falling cost resources and computing. However, it's heterogeneous nature and restricted existence raise various security concerns for the network operators, IoT service providers, and consumers alike. Machine learning has established itself as an inherent part of several IoT applications and is now taking over to address IoT's critical security challenges. The use of machine learning for securing IoT provides the IoT infrastructure with autonomously updated real-time security features. This paper performs a top-down survey on various active attacks faced by IoT networks and aims to showcase how machine learning has become an integral part of the IoT security model.",Low
"The Internet of Things is an emerging technology that integrates the Internet and physical smart objects. This technology currently is used in many areas of human life, including education, agriculture, medicine, military and industrial processes, and trade. Integrating real-world objects with the Internet can pose security threats to many of our day-to-day activities. Intrusion detection systems (IDS) can be used in this technology as one of the security methods. In intrusion detection systems, early and correct detection (with high accuracy) of intrusions is considered very important. In this research, game theory is used to develop the performance of intrusion detection systems. In the proposed method, the attacker infiltration mode and the behavior of the intrusion detection system as a two-player and nonparticipatory dynamic game are completely analyzed and Nash equilibrium solution is used to create specific subgames. During the simulation performed using MATLAB software, various parameters were examined using the definitions of game theory and Nash equilibrium to extract the parameters that had the most accurate detection results. The results obtained from the simulation of the proposed method showed that the use of intrusion detection systems in the Internet of Things based on cloud-fog can be very effective in identifying attacks with the least amount of errors in this network.",Low
"The key success factor of the business depends upon correct and timely information. The vital resources of the organization should be protected from inside and outside threats. Among many threats of network security, intrusion has become a crucial reason for many organizations to incur loss. Many researchers are trying their level best to handle the different types of intrusion affecting the business. To detect such a type of intrusion, our initiative is to us a very popular soft computing tool namely back propagation neural network (BPNN). We have prepared a flexible BPNN architecture to identify the intrusion with the help of anomaly detection methodology. The result we obtained is better than or at per with many best research paper in this field of study. We have used KDD dataset for our experiment.",Low
"The main objective of this paper is intrusion detection system for a cloud environment using combined PFCM-RNN. Traditional IDSs are not suitable for cloud environment as network-based IDSs (NIDS) cannot detect encrypted node communication, also host-based IDSs (HIDS) are not able to find the hidden attack trail. The traditional intrusion detection is largely inefficient to be deployed in cloud computing environments due to their openness and specific essence. Accordingly, this proposed work consists of two modules namely clustering module and classification module. In clustering module, the input dataset is grouped into clusters with the use of possibilistic fuzzy C-means clustering (PFCM). In classification module, the centroid from the clusters is given to the recurrent neural network which is used to classify whether the data is intruded or not. For experimental evaluation, we use the benchmark database and the results clearly demonstrate the proposed technique outperformed conventional methods.",Low
"The metaverse, which is at the stage of innovation and exploration, faces the dilemma of data collection and the problem of private data leakage in the process of development. This can seriously hinder the widespread deployment of the metaverse. Fortunately, federated learning (FL) is a solution to the above problems. FL is a distributed machine learning paradigm with privacy-preserving features designed for a large number of edge devices. Federated learning for metaverse (FL4M) will be a powerful tool. Because FL allows edge devices to participate in training tasks locally using their own data, computational power, and model-building capabilities. Applying FL to the metaverse not only protects the data privacy of participants but also reduces the need for high computing power and high memory on servers. Until now, there have been many studies about FL and the metaverse, respectively. In this paper, we review some of the early advances of FL4M, which will be a research direction with unlimited development potential. We first introduce the concepts of metaverse and FL, respectively. Besides, we discuss the convergence of key metaverse technologies and FL in detail, such as big data, communication technology, the Internet of Things, edge computing, blockchain, and extended reality. Finally, we discuss some key challenges and promising directions of FL4M in detail. In summary, we hope that our up-to-date brief survey can help people better understand FL4M and build a fair, open, and secure metaverse.",Low
"The mining industry plays a crucial role in the development of the national economy. However, in the era of big data, traditional methods of mining analysis and management struggle to achieve efficient management and analysis. This paper proposed an intelligent analysis middle platform designed specifically for the mining field, aiming to assist mining enterprises in making accurate and efficient decisions in various aspects of their operations. The proposed middle platform system established an end-to-end AI production line, including feature engineering and model deployment, and integrated a knowledge center and an algorithm center. It enabled the organic fusion of domain expert knowledge and artificial intelligence algorithms, facilitating unified management and application of knowledge and algorithms.",Low
"The online sequential extreme learning machine (OS-ELM) algorithm is an on-line and incremental learning method, which can learn data one-by-one or chunk-by-chunk with a fixed or varying chunk size. And OS-ELM achieves the same learning performance as ELM trained by the complete set of data. However, in on-line learning environments, the concepts to be learned may change with time, a feature referred to as concept drift. To use ELMs in such non-stationary environments, a forgetting parameters extreme learning machine (FP-ELM) is proposed in this paper. The proposed FP-ELM can achieve incremental and on-line learning, just like OS-ELM. Furthermore, FP-ELM will assign a forgetting parameter to the previous training data according to the current performance to adapt to possible changes after a new chunk comes. The regularized optimization method is used to avoid estimator windup. Performance comparisons between FP-ELM and two frequently used ensemble approaches are carried out on several regression and classification problems with concept drift. The experimental results show that FP-ELM produces comparable or better performance with lower training time.",Medium
"The Open RAN architecture, featuring disaggregated and virtualized RAN functions communicating over standardized interfaces, promises a diverse, multi-vendor ecosystem. However, these features also increase operational complexity, complicating the troubleshooting of RAN performance issues and failures. Addressing this challenge requires a reliable, explainable anomaly detection method, which Open RAN currently lacks. To address this problem, we have developed SpotLight, a tailored distributed deep learning method running across the edge and cloud. SpotLight continuously detects and localizes anomalies by analyzing a diverse, fine-grained stream of metrics from the RAN and platform. It employs a novel multi-stage generative model to identify potential anomalies at the edge using a lightweight algorithm, followed by anomaly confirmation and an explainability phase in the cloud, which pinpoints the minimal set of KPIs responsible for the anomaly. In this demo, using a carrier-grade indoor Open RAN testbed with configurable anomaly event generation and replay, we highlight (1) the difficulty of troubleshooting problems in Open RAN and (2) accurate, efficient, and explainable online anomaly detection with SpotLight and corresponding visualization in comparison with prior art.",Medium
"The performance of Hidden Markov Models (HMMs) targeted for complex real-world applications are often degraded because they are designed a priori using limited training data and prior knowledge, and because the classification environment changes during operations. Incremental learning of new data sequences allows to adapt HMM parameters as new data becomes available, without having to retrain from the start on all accumulated training data. This paper presents a survey of techniques found in literature that are suitable for incremental learning of HMM parameters. These techniques are classified according to the objective function, optimization technique and target application, involving block-wise and symbol-wise learning of parameters. Convergence properties of these techniques are presented along with an analysis of time and memory complexity. In addition, the challenges faced when these techniques are applied to incremental learning is assessed for scenarios in which the new training data is limited and abundant. While the convergence rate and resource requirements are critical factors when incremental learning is performed through one pass over abundant stream of data, effective stopping criteria and management of validation sets are important when learning is performed through several iterations over limited data. In both cases managing the learning rate to integrate pre-existing knowledge and new data is crucial for maintaining a high level of performance. Finally, this paper underscores the need for empirical benchmarking studies among techniques presented in literature, and proposes several evaluation criteria based on non-parametric statistical testing to facilitate the selection of techniques given a particular application domain.",Medium
"The phenomenon of Big Data continues to present moving targets for the scientific and technological state-of-the-art. This work demonstrates that the solution space of these challenges has expanded with deep learning now moving beyond traditional applications in computer vision and natural language processing to diverse and core machine learning tasks such as learning with streaming and non-iid-data, partial supervision, and large volumes of distributed data while preserving privacy. We present a framework coalescing multiple deep methods and corresponding models as responses to specific Big Data challenges. First, we perform a detailed per-challenge review of existing techniques, with benchmarks and usage advice, and subsequently synthesize them together into one organic construct that we discover principally uses extensions of one underlying model, the autoencoder. This work therefore provides a synthesis where challenges at scale across the Vs of Big Data could be addressed by new algorithms and architectures being proposed in the deep learning community. The value being proposed to the reader from either community in terms of nomenclature, concepts, and techniques of the other would advance the cause of multi-disciplinary, transversal research and accelerate the advance of technology in both domains.",Medium
"The popularity of Machine Learning as a Service (MLaaS) has led to increased concerns about Model Stealing Attacks (MSA), which aim to craft a clone model by querying MLaaS. Currently, most research on MSA assumes that MLaaS can provide soft labels and that the attacker has a proxy dataset with a similar distribution. However, this fails to encapsulate the more practical scenario where only hard labels are returned by MLaaS and the data distribution remains elusive. Furthermore, most existing work focuses solely on stealing the model accuracy, neglecting the model robustness, while robustness is essential in security-sensitive scenarios, e.g., face-scan payment. Notably, improving model robustness often necessitates the use of expensive techniques such as adversarial training, thereby further making stealing robustness a more lucrative prospect. In response to these identified gaps, we introduce a novel Data-Free Hard-Label Robustness Stealing (DFHL-RS) attack in this paper, which enables the stealing of both model accuracy and robustness by simply querying hard labels of the target model without the help of any natural data. Comprehensive experiments demonstrate the effectiveness of our method. The clone model achieves a clean accuracy of 77.86% and a robust accuracy of 39.51% against AutoAttack, which are only 4.71% and 8.40% lower than the target model on the CIFAR-10 dataset, significantly exceeding the baselines. Our code is available at: https://github.com/LetheSec/DFHL-RS-Attack.",Low
"The primary challenge of intrusion detection systems (IDS) is to rapidly identify new attacks, learn from the adversary, and update the intrusion detection immediately. IDS operate in dynamic environments subjected to evolving data streams where data may come from different distributions. This is known as the problem of concept drift. Today's IDS though are equipped with deep learning algorithms most of the times fail to identify concept drift. This paper presents a technique to detect and adapt to concept drifts in streaming data with a large number of features often seen in IDS. The study modifies extreme gradient boosting (XGB) algorithm for adaptability of drifts and optimization for large datasets in IDS. The primary objective is to reduce the number of â€˜false positives' and â€˜false negatives' in the predictions. The method is tested on streaming data of smaller and larger sizes and compared against non-adaptive XGBoost and logistic regression.",Medium
"The primary objective of various data stream applications is to monitor the on-going variations of data elements newly generated in data streams, so that appropriate services can be delivered to users timely. Basically, such monitoring activities can be divided into three categories: instant event monitoring, frequent behavior monitoring and analytic OLAP measures monitoring. Several prototype systems for data streams have been introduced but they are designated to support only the operations of one or two categories. However, many real-world data stream applications often require mixed operations of the three categories. This demonstration introduces an Integrated Stream Execution Environment (i-SEE) that can support the mixed execution of the operations of the three categories seamlessly, so that an i-SEE system can provide the multifaceted analytic results of on-line data streams continuously for various emerging applications.",Low
"The proliferation of Internet of Things (IoT) technology across diverse domains has led to a surge in both the frequency and diversity of cyber-attacks targeting IoT infrastructures. In response to this escalating security challenge, several machine learning-based Intrusion Detection Systems (IDSs) have been developed. However, the machine-learning models used in IDSs are generally not designed with IoT infrastructureâ€™s resource constraints in mind. In response to that issue, we propose the utilization of online machine learning models to build efficient IDS for IoT. In this study, we present an extensive exploration of the implementation of online machine-learning algorithms to develop efficient IDSs for IoT. To evaluate the performance of the online machine-learning models, we tested several online machine-learning models using the TON-IoT dataset which was designed specifically for evaluating Artificial Intelligence (AI)-based security applications in IoT. The experimental results showed that the online machine-learning models exhibit performance on par with batch machine-learning models while saving significant computational resources. This notable benefit highlights the potential of online machine-learning models as promising candidates for developing machine learning-based IDSs for IoT infrastructure.",Medium
"The rapid development of the internet has changed the way people express their opinions and emotions. How to use high-performance sentiment analysis algorithms for practical auxiliary product recommendation and public opinion analysis has become a research hotspot. This research proposes a text data sentiment analysis algorithm that integrates transfer learning and hierarchical attention network, and conducts sentiment analysis on single domain and cross domain text data and verifies the effectiveness of the algorithm. The results show that the TLHANN algorithm has an accuracy of 0.85 in IMDB2 data samples, which is higher than other algorithms. In the field of books and DVDs, the accuracy of this algorithm is 0.83, while the other algorithms are 0.826 and 0.828, respectively, which are lower than the FMCSC algorithm. This verifies the effectiveness of the cross domain text sentiment analysis FMCSC algorithm and further verifies its optimal performance.",Low
"The rapid evolution of mobile networks from 5G to 6G has necessitated the development of autonomous network management systems, such as Zero-Touch Networks (ZTNs). However, the increased complexity and automation of these networks have also escalated cybersecurity risks. Existing Intrusion Detection Systems (IDSs) leveraging traditional Machine Learning (ML) techniques have shown effectiveness in mitigating these risks, but they often require extensive manual effort and expert knowledge. To address these challenges, this paper proposes an Automated Machine Learning (AutoML)-based autonomous IDS framework towards achieving autonomous cybersecurity for next-generation networks. To achieve autonomous intrusion detection, the proposed AutoML framework automates all critical procedures of the data analytics pipeline, including data pre-processing, feature engineering, model selection, hyperparameter tuning, and model ensemble. Specifically, it utilizes a Tabular Variational Auto-Encoder (TVAE) method for automated data balancing, tree-based ML models for automated feature selection and base model learning, Bayesian Optimization (BO) for hyperparameter optimization, and a novel Optimized Confidence-based Stacking Ensemble (OCSE) method for automated model ensemble. The proposed AutoML-based IDS was evaluated on two public benchmark network security datasets, CICIDS2017 and 5G-NIDD, and demonstrated improved performance compared to state-of-the-art cybersecurity methods. This research marks a significant step towards fully autonomous cybersecurity in next-generation networks, potentially revolutionizing network security applications.",Medium
"The rapid increase in the number of Internet of Things (IoT) has brought numerous benefits and conveniences, but it has also introduced significant cybersecurity challenges. IoT devices are often connected to networks and to the internet, making them potential targets for cyberattacks (e.g., Mirai attack). As the IoT ecosystem grows, thereâ€™s a pressing need to enhance cybersecurity measures to ensure the protection of these interconnected devices and the data they handle. On the other hand, Cyber Threat Intelligence (CTI) involves the collection, analysis, and distribution of information about potential and ongoing cyber threats. CTI helps organizations understand the threat landscape and take proactive measures to mitigate risks. In the context of IoT, having timely and accurate CTI is crucial for identifying emerging threats and vulnerabilities. While sharing CTI knowledge is important, doing so in a distributed IoT environment presents challenges. The data generated and collected by IoT devices can be sensitive and private, making it essential to find ways to share CTI knowledge without compromising data privacy. This research focuses on leveraging Federated Learning (FL) techniques to facilitate secure and privacy-preserving CTI knowledge sharing in IoT networks by proposing a solution called FedCTI. This paper explores the potential benefits, architecture, experiments, and challenges associated with using FL for sharing CTI knowledge to secure IoT systems.",Medium
"The recent advancements in the Internet of Medical Things (IoMT) have significantly contributed to improving personalized medicine and patient diagnosis and monitoring. Nonetheless, the implementation of IoMT may encounter obstacles due to security and privacy concerns. Federated learning emerges as a promising solution, enabling multiple devices to collaborate on training rich, heterogeneous datasets while preserving privacy. Despite its potential, traditional federated learning methods exhibit vulnerabilities such as single points of attack or failure and performance degradation with heterogeneous data. To this end, this paper proposes a blockchain federated learning system to address these limitations. In the proposed blockchain, a Proof-of-Contribution-Earned (PoCE) consensus protocol is designed for block propagation and minersâ€™ selection using an improved addition tic-tac-toe game. To overcome the challenge related to heterogeneous data, a reward system based on a cooperation strategy is proposed to ensure that high-quality data is shared among health institutions. We employ a Convolutional Neural Network (CNN) where we replace the fully connected layers with sparse ones to minimize the number of parameters using an exponential random graph while maintaining model accuracy. The experimental results on real-world heterogeneous data demonstrate that the proposed system outperforms existing state-of-the-art systems in terms of accuracy and convergence rate. Security analysis reveals that the proposed system is robust against existing security and privacy-related attacks.",Medium
"The Session Initiation Protocol (SIP) is the de facto standard for user's session control in the next generation Voice over Internet Protocol (VoIP) networks based on the IP Multimedia Subsystem (IMS) framework. In this paper, we first analyze the role of SIP based floods in the Denial of Service (DoS) attacks on the IMS. Afterwards, we present an online intrusion detection framework for detection of such attacks. We analyze the role of different evolutionary and non-evolutionary classifiers on the classification accuracy of the proposed framework. We have evaluated the performance of our intrusion detection framework on a traffic in which SIP floods of varying intensities are injected. The results of our study show that the evolutionary classifiers like sUpervised Classifier System (UCS) and Genetic clASSIfier sySTem (GAssist) can even detect low intensity SIP floods in realtime. Finally, we formulate a set of specific guidelines that can help VoIP service providers in customizing our intrusion detection framework by selecting an appropriate classifier-depending on their requirements in different service scenarios.",Medium
"The smart electricity grid enables a two-way flow of power and data between suppliers and consumers in order to facilitate the power flow optimization in terms of economic efficiency, reliability and sustainability. This infrastructure permits the consumers and the micro-energy producers to take a more active role in the electricity market and the dynamic energy management (DEM). The most important challenge in a smart grid (SG) is how to take advantage of the users' participation in order to reduce the cost of power. However, effective DEM depends critically on load and renewable production forecasting. This calls for intelligent methods and solutions for the real-time exploitation of large volumes of data generated by the vast amount of smart meters. Hence, robust data analytics, high performance computing, efficient data network management, and cloud computing techniques are critical towards the optimized operation of SGs. This research aims to highlight the big data issues and challenges faced by the DEM employed in SG networks. It also provides a brief description of the most commonly used data processing methods in the literature, and proposes a promising direction for future research in the field.",Low
"The Smart Grid (SG) heavily depends on the Advanced Metering Infrastructure (AMI) technology, which has shown its vulnerability to intrusions. To effectively monitor and raise alarms in response to anomalous activities, the Intrusion Detection System (IDS) plays a crucial role. However, existing intrusion detection models are typically trained on cloud servers, which exposes user data to significant privacy risks and extends the time required for intrusion detection. Training a high-quality IDS using Artificial Intelligence (AI) technologies on a single entity becomes particularly challenging when dealing with vast amounts of distributed data across the network. To address these concerns, this paper presents a novel approach: a fog-edge-enabled Support Vector Machine (SVM)-based federated learning (FL) IDS for SGs. FL is an AI technique for training Edge devices. In this system, only learning parameters are shared with the global model, ensuring the utmost data privacy while enabling collaborative learning to develop a high-quality IDS model. The test and validation results obtained from this proposed model demonstrate its superiority over existing methods, achieving an impressive percentage improvement of 4.17% accuracy, 13.19% recall, 9.63% precision, 13.19% F1 score when evaluated using the NSL-KDD dataset. Furthermore, the model performed exceptionally well on the CICIDS2017 dataset, with improved accuracy, precision, recall, and F1 scores reaching 6.03%, 6.03%, 7.57%, and 7.08%, respectively. This novel approach enhances intrusion detection accuracy and safeguards user data and privacy in SG systems, making it a significant advancement in the field.",Medium
"The speed of digital transformation has resulted in new challenges for job seekers to become lifelong learners and to develop new skills faster than before. In this paper, our main objective is to examine how online content can serve as indicators for changes to the Information Technology (IT) industry and its in-demand skills. To study this relationship, we collect Reddit posts to represent social media content and job postings to reflect the IT industry based on which we explore possible correlations between them. Further, we propose a methodology to quantitatively estimate the predictive power of social media content for future in-demand skills. Our results show that the frequency of skill-related conversations on Reddit correlates with the popularity of skills in job posting data. Additionally, our findings indicate that the number of social posts dedicated to a specific skill can be a strong indicator for future job requirements. This is an important finding because identifying what skills the labor force should acquire will assist job seekers to plan their lifelong learning objectives to (a) maximize their employability, (b) continuously update their skills to remain in demand, and (c) be informed and actively engaged in defining knowledge trends, rather than reactively becoming informed of the latest information.",Low
"The use of a well-known state-of-the-art classifier, Hoeffding Trees, is generally proposed in data stream mining (DSM) approaches. Most of these approaches generally address achieving improved accuracy when exceedingly complex drifts are present. Unfortunately, only a few minor DSM approaches have been proposed for anomaly-based Intrusion Detection Systems (IDS). Despite the common relation between anomalies and concept-drift. These approaches also validate with outdated cyber datasets. In this paper, we propose an enhanced IDS ensemble framework of distributed diverse Hoeffding Trees built on Spark Streaming. The pivotal component is an extensible framework to include additional Linear Classifiers and essential IDS components. To validate the efficiency of our approach, we perform several experiments using various up-to-date real-world, synthetic cyber-attack and concept-drift datasets. Our results demonstrate IDS evaluation metrics in the 80â€“90 percentile and an increase in speed and marginal increase in accuracy and Kappa Statistic, when compared to the current state-of-the-art DSM platform, MOA.",Medium
"The use of application media, gamming, entertainment, and healthcare engineering has expanded as a result of the rapid growth of mobile technologies. This technology overcomes the traditional computing methods in terms of communication delay and energy consumption, thereby providing high reliability and bandwidth for devices. In todayâ€™s world, mobile edge computing is improving in various forms so as to provide better output and there is no room for simple computing architecture for MEC. So, this paper proposed a secure and energy-efficient computational offloading scheme using LSTM. The prediction of the computational tasks is done using the LSTM algorithm, the strategy for computation offloading of mobile devices is based on the prediction of tasks, and the migration of tasks for the scheme of edge cloud scheduling helps to optimize the edge computing offloading model. Experiments show that our proposed architecture, which consists of an LSTM-based offloading technique and routing (LSTMOTR) algorithm, can efficiently decrease total task delay with growing data and subtasks, reduce energy consumption, and bring much security to the devices due to the firewall nature of LSTM.",Low
"The volumes and sophistication of cyber threats in todayâ€™s cyber threat landscape have risen to levels where automated quantitative tools for Cyber Threat Intelligence (CTI) have become an indispensable part in the cyber defense arsenals. The AI and cyber security research communities are producing novel automated tools for CTI that quickly find their ways into commercial products. However, the quality of such automated intelligence products is being questioned by the intelligence community. Cyber security operators are forced to complement the automated tools with costly and time-consuming human intelligence analysis in order to improve the quality of the end product. For improving the quality, it has been suggested that researchers should incorporate methods from traditional intelligence analysis into the quantitative algorithms. This article presents a novel approach to cyber intelligence analysis called AMBARGO, which takes the inherent ambiguity of evidence into account in the analysis, using the Choquet integral, in formalizing the re-evaluation of evidence and hypotheses made by human analysts. The development of AMBARGO revolves around a cyber attribution use case, one of the hardest problems in CTI. The results of our evaluating experiments show that the robustness of AMBARGO outperforms state-of-the-art quantitative approaches to CTI in the presence of ambiguous evidence and potentially deceptive threat actor tactics. AMBARGO has thus the potential to fill a gap in the CTI state-of-the-art, which currently handles ambiguity poorly. The findings are also confirmed in a large-scale realistic experimental setting based on data from an APT campaign obtained from the MITRE ATT&amp;CK Framework.",Low
"The worldwide process of converting most activities of both corporate and non-corporate entities into digital formats is now firmly established. Machine learning models are necessary to serve as a tool for preventing illegal intrusion onto different networks. The machine learning (ML) model's strengths and drawbacks pertain to intrusion detection (IDS) tasks. This study used an experimental methodology to assess the efficacy of various ML models, including linear SVC, LR, random forest (RF), decision tree (DT), and XGBoost, in detecting intrusion on the UNSW NB15 datasets. The objective is to compare the strengths and shortcomings of these models. Data exploration, Feature engineering, selection and a test set of 15%, a validation set of 15%, and a training set of 70% respectively were used for data splitting. Performance evaluation was carried out using accuracy, recall, precision F1-score and confusion matrix plotted. The outcome of the experiment shows a percentage of 92.71% (1, normal) and 7.29% (0, attack) for normal traffic and attack traffic respectively. Performance evaluation results showed that RF and XGBoost outperformed the other ML models. Hence, ML models can effectively be used to detect system attacks. We intend to expand this research in the future and use the paradigm in a real-world setting with further conclusions and justifications.",Low
This article is an enhancement of the chapter â€œAbout Digital Avatars for Control in Virtual Industriesâ€ in the book Big Data and Knowledge Sharing in Virtual Organizations. The article discusses the capabilities of the R language for modeling Levy processes that currently most closely correspond to the nature of the organizational learning movements in sliding mode. The efficient algorithm of the CGMY process simulation as a difference of the tempered stable independent Levy is processed and programmed at R language. The efficient algorithm of variance gamma process simulation using variance gamma random variables is processed and programmed at R language. Overview of CGMY process simulation in practice is use for human capital management in the context of the implementation of digital intelligent decision support systems and knowledge management and for digital intelligent design of avatar-based control with application to human capital management.,Low
"This paper aims to explore unsupervised cross-lingual word representation learning methods with the specific task of acquiring a bilingual translation lexicon on a monolingual corpus. Specifically, an unsupervised cross-lingual word representation co-training scheme based on different word embedding models is first designed and outperforms the baseline model. In this paper, we adeptly tackles the obstacles encountered in higher education foreign language teaching and underscores the necessity for inventive teaching methods, and design and implement a linear self-encoder-based principal component acquisition scheme for the interpoint mutual information matrix obtained from a monolingual corpus. And on top of this, a collaborative training scheme based on linear self-encoder for cross-language word representation is designed to improve the learning effect of cross-language word embedding. The results of the study show that the most obvious rise in the pre and post tests of the experimental class in the practical application of the foreign language teaching model based on the method of this paper is the word sense guessing, which rose by 23.12%. Sentence meaning comprehension increased by 23.39%, main idea by 16.61%, factual details by 15.47%, and inferential judgment by 10.28%. Thus, the feasibility of the unsupervised cross-linguistic word representation learning collaborative training method is further verified.",Low
"This paper divides the application of AI in education into three categories, namely, students-oriented AI, teachers-oriented AI and school mangers -oriented AI, which focuses on the individualized self-adaptive learning of students, the assisted teaching of teachers and the service management efficiency of schools respectively. With the continuous integration of AI technology and education, although we see a bright future in the field of AI in education, it can be seen that many obstacles will still exist in the future by reviewing the tortuous and difficult development history of AI in education for decades. Due to the dilemmas such as the inexplicability of algorithm, the limitations of algorithm, data bias, privacy leakage, etc., the application of AI in the education may face difficulties and obstacles in technology, effect, law, ethics, and system level, etc. Logically proposing the countermeasures to meet the development of AI education will be very helpful in dealing well with the difficulties and obstacles effectively. Abiding by the principles such as transparency, integration, diversification, popularization, fairness, accountability, security, privacy protection, humanistic education and other principlesand ways is to effectively apply AI in education, thereby promoting the technological integration in the development of educational innovation safely and effectively.",Low
"This paper introduces a robust zero-trust architecture (ZTA) tailored for the decentralized system that empowers efficient remote work and collaboration within IoT networks. Using blockchain-based federated learning principles, our proposed framework includes a robust aggregation mechanism designed to counteract malicious updates from compromised clients, enhancing the security of the global learning process. Moreover, secure and reliable trust computation is essential for remote work and collaboration. The robust ZTA framework integrates anomaly detection and trust computation, ensuring secure and reliable device collaboration in a decentralized fashion. We introduce an adaptive algorithm that dynamically adjusts to varying user contexts, using unsupervised clustering to detect novel anomalies, like zero-day attacks. To ensure a reliable and scalable trust computation, we develop an algorithm that dynamically adapts to varying user contexts by employing incremental anomaly detection and clustering techniques to identify and share local and global anomalies between nodes.",Medium
"This paper introduces a technique for big data classification using an optimisation algorithm. Here, the classification of big data is performed in a Hadoop MapReduce framework, wherein the map and reduce functions are based on the proposed dragonfly rider optimisation algorithm (DROA), which is designed by integrating the dragonfly algorithm (DA) and rider optimisation algorithm (ROA). The mapper uses the proposed optimisation as a mapper function for selecting the optimal features from the input big-data, for which the fitness function is based on Renyi entropy. Then, the selected features are subjected to the reducer phase, where the classification of the big data is performed using the DROA-based recurrent neural network (RNN), in which the RNN is trained by the proposed DROA. The result proves that the proposed method acquired a maximal accuracy of 0.996, the sensitivity of 0.995, and specificity of 0.995, respectively.",Low
"This paper introduces the active learning strategy to the classical back-propagation neural network algorithm and proposes punishing-characterized active learning Back-Propagation BP Algorithm PCAL-BP to adapt to big data conditions. The PCAL-BP algorithm selects samples and punishments based on the absolute value of the prediction error to improve the efficiency of learning complex data. This approach involves reducing learning time and provides high precision. Numerical analysis shows that the PCAL-BP algorithm is superior to the classical BP neural network algorithm in both learning efficiency and precision. This advantage is more prominent in the case of extensive sample data. In addition, the PCAL-BP algorithm is compared with 16 types of classical classification algorithms. It performs better than 14 types of algorithms in the classification experiment used here. The experimental results also indicate that the prediction accuracy of the PCAL-BP algorithm can continue to increase with an increase in sample size.",Low
"This paper proposes a privacy preserving-aware-based approach over Big data in clouds using GSA and MapReduce framework. It consists of two modules such as; MapReduce module and evaluation module. In MR module, convolution process is applied to the dataset and creates a new kernel matrix. The convolution process is correctly done; the utility and privacy information of the data is well secured. Once the convolution process is over, the privacy-persevering framework over big data in cloud systems is performed based on the evaluation module. In Evaluation module, the neural-network is trained based on the Gravitational Search Algorithm with Scaled conjugate gradient (GSA-SCG) algorithm which is improving the utility of the privacy data. Finally, the reduced privacy data's are stored in the service provider (CSP). The MapReduce framework is to ensure the private data, which is in charge for anonymising original data sets as per privacy requirements.",Low
"This paper will examine the ""intelligent +"" society from the perspective of digital literacy education, so as to propose the promotion of talent digital literacy to cope with a new round of high-tech digital technology competition.&nbsp;Firstly, it analyzes the urgency of the information revolution globalization to the demand for digital talents, and then compares and analyzes the root causes of the lack of digital talents in China and the problems that need to be solved by digital literacy education by combining the changes in the demand structure of digital talents in advanced countries and the development of digital talent training strategy in recent years.&nbsp; Furthermore, it puts forward the basic path to promote the comprehensive governance system of ""intelligent +"" digital literacy education, and puts forward specific suggestions on lifelong education of digital literacy for all people, cultivation of comprehensive digital talents with legal ethics and data analysis, as well as promoting the aggregation of industry-university-research resources.",Low
"This review article looks at recent research on Federated Learning (FL) for Collaborative Intrusion Detection Systems (CIDS) to establish a taxonomy and survey. The motivation behind this review comes from the difficulty of detecting coordinated cyberattacks in large-scale distributed networks. Collaborative anomalies are one of the network anomalies that need to be detected through robust collaborative learning methods. FL is promising collaborative learning method in recent research. This review aims to offer insights and lesson learn for creating a taxonomy of collaborative anomaly detection in CIDS using FL as a collaborative learning method. Our findings suggest that a taxonomy is required to map the discussion area, including an algorithm for training the learning model, the dataset, global aggregation model, system architecture, security, and privacy. Our results indicate that FL is a promising approach for collaborative anomaly detection in CIDS, and the proposed taxonomy could be useful for future research in this area. Overall, this review contributes to the growing knowledge of FL for CIDS, providing insights and lessons for researchers and practitioners. This research also concludes significant challenges, opportunities, and future directions in CIDS based on collaborative anomaly detection using FL.",Medium
"This study analysed two internationally recognised measurements of professional skills: the Programme for the International Assessment of Adult Competencies (PIAAC) and the Digital Competence Framework for Citizens (DigComp 2.2). This analysis aimed to establish a baseline and reference point for a comprehensive understanding of these two scales with the objective of exploring the most relevant competencies and assessing their potential benefits in fostering a technology-driven environment in higher education institutions in Saudi Arabia. The research yielded significant findings regarding the essential competencies that university students should possess, drawing insights from the two assessments and identifying gaps in existing manuals. The research outcomes include valuable insights and best practices that can empower a new generation of university learners to effectively engage with the requirements of Education 4.0.",Low
"This study presents an innovative anomaly detection framework for enterprise information systems, integrating deep learning and multi-model ensemble techniques. The framework has been evaluated on a large-scale real-world dataset, demonstrating superior performance. Experimental results show significant achievements in key metrics such as accuracy, F1 score, and AUC value. Compared to traditional methods, this framework exhibits notable improvements in detection precision and processing speed. It excels in identifying critical anomalies such as system crashes and network attacks, effectively reducing false positive and false negative rates. The framework also shows excellent scalability and adaptability, capable of handling new anomalies and dynamically changing system environments. The research findings provide robust support for enhancing the security and reliability of enterprise information systems, offering broad practical application prospects.",Low
"This work presents a novel method called dense projection for unsupervised anomaly detection (DPAD). The main idea is maximizing the local density of (normal) training data and then determining whether a test data is anomalous or not by evaluating its density. Specifically, DPAD uses a deep neural network to learn locally dense representations of normal data. Since density estimation is computationally expensive, we minimize the local distances of the representations in an iteratively reweighting manner, where the weights are updated adaptively and the parameters are regularized to avoid model collapse (all representations collapse to a single point). Compared with many state-of-the-art methods of anomaly detection, our DPAD does not rely on any assumption about the distribution or spatial structure of the normal data and representations. Moreover, we provide theoretical guarantees for the effectiveness of DPAD. The experiments show that our method DPAD is effective not only in traditional one-class classification problems but also in scenarios with complex normal data composed of multiple classes.",Low
"Time series anomaly detection is critical in various domains, including stock markets, network traffic monitoring, and industrial systems, as it identifies deviations from expected patterns in data, enabling real-time analysis and timely responses to potential issues such as system failures or fraudulent activities. In the realm of industrial intelligent operation and maintenance, effective anomaly detection is essential for ensuring product quality and maintaining the safety of production systems. However, challenges such as large-scale data influx during testing and evolving data patterns, known as concept drift, pose significant challenges to the adaptability and accuracy of traditional deep learning methods. To address these challenges, we propose a novel plug-and-play Temporal Adaptation Anomaly Detection (TAAD) framework. This framework can seamlessly integrates with existing deep anomaly detection models, enhancing their capability to manage rapidly changing data patterns and concept drift. It comprises two key modules: a multi-scale prediction correction module that provides accurate predictions of future values based on historical data, and a memory comparison monitoring module that continuously updates memory vectors to detect segment anomalies and adapt to shifts in data distributions. Our TAAD framework substantially improves detection accuracy and efficiency, offering a robust and adaptable solution for real-time anomaly detection in industrial scenarios. We conducted experiments on the KPI datasets and demonstrated that our framework can effectively address concept drift.",Medium
"Time series anomaly detection is important for a wide range of research fields and applications, including financial markets, economics, earth sciences, manufacturing, and healthcare. The presence of anomalies can indicate novel or unexpected events, such as production faults, system defects, and heart palpitations, and is therefore of particular interest. The large size and complexity of patterns in time series data have led researchers to develop specialised deep learning models for detecting anomalous patterns. This survey provides a structured and comprehensive overview of state-of-the-art deep learning for time series anomaly detection. It provides a taxonomy based on anomaly detection strategies and deep learning models. Aside from describing the basic anomaly detection techniques in each category, their advantages and limitations are also discussed. Furthermore, this study includes examples of deep anomaly detection in time series across various application domains in recent years. Finally, it summarises open issues in research and challenges faced while adopting deep anomaly detection models to time series data.",Low
"To address the difficulties in detecting anomalous data in underwater sensor networks and the issue of concept drift in streaming data, a method by constructing an LSTM-CNN Fusion network is proposed. The Long Short-Term Memory neural network and the One-Dimensional Convolutional Neural Network are used as the two base learners of the LSCF network. The Adaptive Sliding Window drift detector is utilized to detect whether data drift occurs. The two base learners are trained independently, and their prediction results are combined through soft voting. The performance of the model is validated using two public datasets, with results showing detection accuracies of 97% and 95% for anomalous data, significantly higher than the average detection accuracy of other models.",Medium
"To detect unknown attack traffic, anomaly-based network intrusion detection systems (NIDSs) are widely used in Internet infrastructure. However, the security communities realize some limitations when they put most existing proposals into practice. The challenges are mainly concerned with (i) fine-grained emerging attack detection and (ii) incremental updates/adaptations. To tackle these problems, we propose to decouple the need for model capabilities by transforming known/new class identification issues into multiple independent one-class learning tasks. Based on the above core ideas, we develop Trident, a universal framework for fine-grained unknown encrypted traffic detection. It consists of three main modules, i.e., tSieve, tScissors, and tMagnifier are used for profiling traffic, determining outlier thresholds, and clustering respectively, each of which supports custom configuration. Using four popular datasets of network traces, we show that Trident significantly outperforms 16 state-of-the-art (SOTA) methods. Furthermore, a series of experiments (concept drift, overhead/parameter evaluation) demonstrate the stability, scalability, and practicality of Trident.",Medium
"To ensure reliability and service availability, next-generation networks are expected to rely on automated anomaly detection systems powered by advanced machine learning methods with the capability of handling multi-dimensional data. Such multi-dimensional, heterogeneous data occurs mostly in todayâ€™s Industrial Internet of Things (IIoT), where real-time detection of anomalies is critical to prevent impending failures and resolve them in a timely manner. However, existing anomaly detection methods often fall short of effectively coping with the complexity and dynamism of multi-dimensional data streams in IIoT. In this paper, we propose an adaptive method for detecting anomalies in IIoT streaming data utilizing a multi-source prediction model and concept drift adaptation. The proposed anomaly detection algorithm merges a prediction model into a novel drift adaptation method resulting in accurate and efficient anomaly detection that exhibits improved scalability. Our trace-driven evaluations indicate that the proposed method outperforms the state-of-the-art anomaly detection methods by achieving up to an 89.71% accuracy (in terms of Area under the Curve (AUC)) while meeting the given efficiency and scalability requirements.",Medium
"Today, Machine Learning (ML) techniques are increasingly used to detect abnormal behaviours of industrial applications. Since many of these applications are moving to the cloud environments, classical ML approaches are facing new challenges in accurately identifying abnormal behaviours due to the highly dynamic and heterogeneous nature of the cloud. In this paper, we propose a novel framework, DistAppGaurd, for profiling simultaneously the behaviour of all microservice components of a distributed application in the cloud. The framework can therefore, detect complex attacks that are not observable by monitoring a single process or a single microservice. DistAppGaurd utilizes the system calls executed by all the processes of an application to build a graph consisting of data exchanges among different application entities (e.g., processes and files) representing the behaviour of the application. This representation is then used by our novel miroservice-aware Autoencoder model to perform anomaly detection at runtime. The efficiency and feasibility of our approach is shown by implementing several different real-world attacks, which yields high detection rates (94%-97%) at 0.01% false alarm rate.",Medium
"Traditional deep learning algorithms are difficult to deploy on most IoT terminal devices due to their limited computing power. To solve this problem, this paper proposes a novel ensemble fuzziness-based online sequential learning approach to support the local update of terminal intelligent models and improve their prediction performance. Our method consists of two modules: server module and terminal module. The latter uploads the data collected in real-time to the server module, then the server module selects the most valuable samples and sends them back to the terminal module for the local update. Specifically, the server module uses the ensemble learning mechanism to filter data through multiple fuzzy classifiers, while the terminal module uses the online neural networks with random weights to update the local model. Extensive experimental results on ten benchmark data sets show that the proposed method outperforms other similar algorithms in prediction. Moreover, we apply the proposed method to solve the network intrusion detection problem, and the corresponding experimental results show that our method has better generalization ability than other existing solutions.",Medium
"Traffic congestion is a major challenge in modern transportation systems, leading to increased travel time, greater fuel consumption, and higher levels of harmful emissions. Accurate traffic congestion forecasting is crucial for effective traffic control, such as managing traffic lights. To generate reliable predictions, it is crucial to precisely detect abnormal traffic patterns in the data, particularly in densely populated urban areas. This study presents a novel approach that integrates anomaly detection and ensemble learning for traffic congestion forecasting. As an initial step in the learning process, we evaluate different anomaly detection techniques to identify unusual traffic patterns in different locations over time. After predicting the anomaly, the data pattern is cleaned accordingly, and a set of baseline learner models is trained as a secondary learning process. The top-performing models are chosen and undergo an ensemble process to combine their results, evaluating both stacking and voting ensemble methods as a third learning process. We evaluate the efficacy of the proposed strategy by employing a real-world traffic dataset and diverse evaluation metrics. The dataset undergoes several preprocessing techniques, including the windowing process with various settings, to convert the time series data into frequency patterns and produce a more generalized model. The results show that the multilevel learning approach improves prediction accuracy compared to baseline models, highlighting its effectiveness. This study highlights the utilization of anomaly detection and ensemble learning to enhance the precision of traffic congestion prediction, thereby promoting further exploration of this approach in intelligent transportation systems.",Low
"Training from pre-trained models (PTM) is a popular approach for fast machine learning (ML) service deployment. Recent studies on hardware security have revealed that ML systems could be compromised through flipping bits in model parameters (e.g., weights) with memory faults. In this paper, we introduce WBP (i.e., weight bit poisoning), a novel task-agnostic backdoor attack that manifests during the victimâ€™s training time (i.e., fine-tuning from a public and clean PTM) by inducing hardware-based weight bit flips. WBP utilizes a novel distance-aware algorithm that identifies bit flips to maximize the distance between the distribution of poisoned output representations (ORs) and clean ORs based on the public PTM. This unique set of bit flips can be applied to backdoor any victim model during the fine-tuning of the same public PTM, regardless of the downstream tasks. We evaluate WBP on state-of-the-art CNNs and Vision Transformer models with representative downstream tasks. The results show that WBP can compromise a wide range of PTMs and downstream tasks with an average 99.3% attack success rate by flipping as few as 11 model weight bits. WBP can be effective in various training configurations with respect to learning rate, optimizer, and fine-tuning duration. We investigate limitations of existing backdoor protection techniques against WBP and discuss potential future mitigation. (Our code can be accessed at: ).",Low
"Under the background of new engineering construction, engineering majors are facing new opportunities and challenges. It is an urgent matter and a long-term strategy to update the teaching concept in time and cultivate new talents with core competence. Ultrasonic diagnosis equipment and maintenance is an important core course in the major of medical equipment application technology. This paper introduces the major and course, analyzes the current teaching situation and existing problems in the course, and proposes to use PBL (Problem-Based Learning) teaching mode commonly used in medical colleges to design the course content. The practice shows that the organic integration of science and engineering courses and PBL teaching mode is conducive to the cultivation of students' independent thinking ability, innovation ability and team cooperation ability. This will help to cultivate students' ability of independent thinking, innovation and teamwork. And stimulate their subjective initiative, the humanistic quality has also been improved, which is highly consistent with the training needs of medical equipment talents under the background of the new engineering construction.",Low
"Unsupervised anomaly detection aims to construct a model that effectively detects invisible anomalies by training and reconstruct normal data. While a significant amount of reconstruction-based methods has made effective progress for time series anomaly detection, challenges still exist in aspects such as temporal feature extraction and generalization ability. Firstly, temporal features of data are subject to local information interference in reconstruction methods, which limits the long-term signal reconstruction methods. Secondly, the training dataset collector is subject to information nourishment such as collection methods, collection periods and locations, and data patterns are diverse, requiring the model to rebuild normal data according to different patterns. These issues hinder the anomaly detection capability of reconstruction-based methods. We propose an unsupervised anomaly detection model based on a diffusion model, which learns normal data pattern learning through noisy forward diffusion and reverse noise regression. By using a cascaded structure and combining it with a structured state space layer, long-term time series signal feature can be well extracted. Different collection signals are distinguished by introducing collector entity ID embedding. The method proposed in this article significantly improves performance in experimental tests on three public datasets. Innovative aspects: (1) Utilizing the S4 method to capture long-term dependencies; (2) Employing a diffusion model for reconstruction learning; (3) Leveraging embedding techniques to enhance different pattern learning.",Low
"Unsupervised Deep Learning (DL) techniques have been widely used in various security-related anomaly detection applications, owing to the great promise of being able to detect unforeseen threats and superior performance provided by Deep Neural Networks (DNN). However, the lack of interpretability creates key barriers to the adoption of DL models in practice. Unfortunately, existing interpretation approaches are proposed for supervised learning models and/or non-security domains, which are unadaptable for unsupervised DL models and fail to satisfy special requirements in security domains.In this paper, we propose DeepAID, a general framework aiming to (1) interpret DL-based anomaly detection systems in security domains, and (2) improve the practicality of these systems based on the interpretations. We first propose a novel interpretation method for unsupervised DNNs by formulating and solving well-designed optimization problems with special constraints for security domains. Then, we provide several applications based on our Interpreter as well as a model-based extension Distiller to improve security systems by solving domain-specific problems. We apply DeepAID over three types of security-related anomaly detection systems and extensively evaluate our Interpreter with representative prior works. Experimental results show that DeepAID can provide high-quality interpretations for unsupervised DL models while meeting the special requirements of security domains. We also provide several use cases to show that DeepAID can help security operators to understand model decisions, diagnose system mistakes, give feedback to models, and reduce false positives.",Low
"Video models on federated learning (FL) enable continual learning of the involved models for video tasks on end-user devices while protecting the privacy of end-user data. As a result, the security issues on FL, e.g., the backdoor attacks on FL and their defense have increasingly become the domains of extensive research in recent years. The backdoor attacks on FL are a class of poisoning attacks, in which an attacker, as one of the training participants, submits poisoned parameters and thus injects the backdoor into the global model after aggregation. Existing backdoor attacks against videos based on FL only poison RGB frames, which makes it that the attack could be easily mitigated by two-stream model neutralization. Therefore, it is a big challenge to manipulate the most advanced two-stream video model with a high success rate by poisoning only a small proportion of training data in the framework of FL. In this paper, a new backdoor attack scheme incorporating the rich spatial and temporal structures of video data is proposed, which injects the backdoor triggers into both the optical flow and RGB frames of video data through multiple rounds of model aggregations. In addition, the adversarial attack is utilized on the RGB frames to further boost the robustness of the attacks. Extensive experiments on real-world datasets verify that our methods outperform the state-of-the-art backdoor attacks and show better performance in terms of stealthiness and persistence.",Medium
"Virtual coordinate systems provide an accurate and efficient service that allows hosts on the Internet to determine the latency to arbitrary hosts without actively monitoring all nodes in the network. Many of the proposed virtual coordinate systems were designed with the assumption that all of the nodes in the system are altruistic. However, this assumption may be violated by compromised nodes acting maliciously to degrade the accuracy of the coordinate system. As numerous peer-to-peer applications rely on virtual coordinate systems to achieve good performance, it is critical to address the security of such systems.In this work, we demonstrate the vulnerability of decentralized virtual coordinate systems to insider (or Byzantine) attacks. We propose techniques to make the coordinate assignment robust to malicious attackers without increasing the communication cost. We demonstrate the attacks and mitigation techniques in the context of a well-known distributed virtual coordinate system using simulations based on three representative, real-life Internet topologies of hosts and corresponding round trip times (RTT).",Low
"Virtual coordinate systems provide an accurate and efficient service that allows hosts on the Internet to determine the latency to arbitrary hosts without actively monitoring all of the nodes in the network. Many of the proposed systems were designed with the assumption that all of the nodes are altruistic. However, this assumption may be violated by compromised nodes acting maliciously to degrade the accuracy of the coordinate system. As numerous peer-to-peer applications come to rely on virtual coordinate systems to achieve good performance, it is critical to address the security of such systems.In this work, we demonstrate the vulnerability of decentralized virtual coordinate systems to insider (or Byzantine) attacks. We propose techniques to make the coordinate assignment robust to malicious attackers without increasing the communication cost. We use both spatial and temporal correlations to perform context-sensitive outlier analysis to reject malicious updates and prevent unnecessary and erroneous adaptations. We demonstrate the attacks and mitigation techniques in the context of a well-known virtual coordinate system using simulations based on three representative, real-life Internet topologies of hosts and corresponding Round Trip Times (RTT). We show the effects of the attacks and the utility of the mitigation techniques on the virtual coordinate system as seen by higher-level applications, elucidating the utility of deploying robust virtual coordinate systems as network services.",Low
"Virtual reality (VR), while enhancing user experiences, introduces significant privacy risks. This paper reveals a novel vulnerability in VR systems that allows attackers to capture VR privacy through obstacles utilizing millimeter-wave (mmWave) signals without physical intrusion and virtual connection with the VR devices. We propose mmSpyVR, a novel attack on VR user's privacy via mmWave radar. The mmSpyVR framework encompasses two main parts: (i) A transfer learning-based feature extraction model to achieve VR feature extraction from mmWave signal. (ii) An attention-based VR privacy spying module to spy VR privacy information from the extracted feature. The mmSpyVR demonstrates the capability to extract critical VR privacy from the mmWave signals that have penetrated through obstacles. We evaluate mmSpyVR through IRB-approved user studies. Across 22 participants engaged in four experimental scenes utilizing VR devices from three different manufacturers, our system achieves an application recognition accuracy of 98.5% and keystroke recognition accuracy of 92.6%. This newly discovered vulnerability has implications across various domains, such as cybersecurity, privacy protection, and VR technology development. We also engage with VR manufacturer Meta to discuss and explore potential mitigation strategies. Data and code are publicly available for scrutiny and research.1",Low
"Virtualization-layer has opened new doors for attacks and raised serious security concerns. The traditional intrusion detection mechanisms are less efficient in detecting emerging attacks in virtualization. The detection of evolving malicious activities is important to prevent the spread of any harmful operations, especially among co-located VMs. Moreover, the evolving behavior of the attacks may affect the decision boundaries of the static-trained machine learning models, leading to a â€˜concept driftâ€™ problem. In this paper, we have proposed a concept-drift driven virtual network security framework, called CDDN that detects malicious network activities by analyzing virtual machine (VM) traffic profile and provides a solution to the problem of concept drift. CDDN is deployed at the privilege domain of hypervisor and initially performs the network introspection, to capture the virtual network traffic of the monitored VM. The traffic is pre-processed and various important features are extracted. CDDN employs a combination of supervised and unsupervised machine learning techniques to distinguish between benign and attack network flows. In addition, it employs a drift detection method to identify drifted samples from the streaming data. Furthermore, incremental learning is implemented to update the model with evolving attack patterns, representing drift (representing drift).The approach has been validated with both publicly available and self-generated network attack datasets. The results seem to be promising.",Medium
"We are traversing the growing emerging technology paradigms in todayâ€™s advanced technological world. In this present era, the Internet of Things (IoT) is extensively used in all sectors. IoT is the ecosystem of smart devices which contains sensors, smart objects, networking, and processing units. These integrated devices provide better services to the end user. IoT is impacting our environment and is becoming one of the most popular technologies. The leading use of IoT in human life is to track activities anywhere at any time. The utmost utilities achieved by IoT applications are decision-making and monitoring for efficient and effective management. In this paper, an extensive literature review on IoT has been done using the systematic literature review (SLR) technique. The main focus areas include commercial, environmental, healthcare, industrial, and smart cities. The issues related to the IoT are also discussed in detail. The purpose of this review is to identify the major areas of applications, different popular architectures, and their challenges. The various IoT applications are compared in accordance with technical features such as quality of service and environmental evaluation. This study can be utilized by the researchers to understand the concept of IoT and provides a roadmap to develop strategies for their future research work.",Low
"We explore the capabilities of Large Language Models (LLMs) to assist or substitute devices (i.e., firewalls) and humans (i.e., security experts) respectively in the detection and analysis of security incidents. We leverage transformer-based technologies, from relatively small to foundational sizes, to address the problem of correctly identifying the attack severity (and accessorily identifying and explaining the attack type). We contrast a broad range of LLM techniques (prompting, retrieval augmented generation, and fine-tuning of several models) using state-of-the-art machine learning models as a baseline. Using proprietary data from commercial deployment, our study provides an unbiased picture of the strengths and weaknesses of LLM for intrusion detection.",Medium
"We propose an incremental nonparametric Bayesian approach for clustering. Our approach is based on a Dirichlet process mixture of generalized Dirichlet GD distributions. Unlike classic clustering approaches, our model does not require the number of clusters to be pre-defined. Moreover, an unsupervised feature selection scheme is integrated into the proposed nonparametric framework to improve clustering performance. By learning the proposed model using an incremental variational framework, the number of clusters as well as the features weights can be automatically and simultaneously computed. The effectiveness and merits of the proposed approach are investigated on a challenging application namely anomaly intrusion detection.",Medium
"We show that knowledge of wallet addresses from the current time state of a blockchain network, such as Bitcoin, increases the performance of illicit activity detection. Based on this finding we introduce two new methods for the sampling of classifier training data so that precedence is given to transaction information from the recent past and the current time state. This sampling enables streaming classification in which a decision on the class of a transaction needs to be made based on data seen to date. Our new approach provides insight into how the dynamics of the blockchain network plays a central role in the detection of illicit transactions, and is independent of the classifier choice. Our proposed sampling methods enable graph convolution network (GCN) and random forest (RF) classifiers to better adapt to changes in the network due to significant events, such as the closure of a large â€˜Darknetâ€™ marketplace. We introduce Graphlet spectral correlation analysis for exposing the effect of such network re-organisation due to major events. Finally, based on our analysis, we propose a new two-stage random forest classifier that feeds back intermediate predictions of neighbours to improve the classification decision. Our methodology enables practical streaming classification, even in the scenario of very limited information on the feature space of each transaction.",Medium
"Wild animal poaching, particular rhinos, and elephants in Africa, is a serious destruction for biodiversity and eco-tourism. Governments and numerous Non â€“ Government Organizations (NGOs) spent a great amount of human labor and money every year in preventing poaching. Recently, advanced techniques, like intelligent video surveillance and multimedia data mining, have been adopted to help more efficiently mitigate wild animals poaching. In this paper, we provide a detailed review of the state-of-the-art video surveillance and multimedia data mining techniques for mitigating wild animal poaching from four aspects according to processing steps, namely object detection, object classification, object behavior analysis and invader analysis. More specifically, different algorithms in each aspect are further subdivided into sub-categories and compared in terms of pros, cons, efficiency, and complexity. While these techniques have been thoroughly researched separately, such topics have not been superimposed in the paradigm of wild animals poaching.&nbsp;To the best of our knowledge, this is the first such comprehensive review of the recent advances of the intelligent video understanding and multimedia data mining for mitigating wild animals poaching and hopefully it would help the improvement, implementation, and applications of advanced techniques in preventing wild animal poaching and protecting diverse especially endangered species for the one and only one home for us human being.",Low
"With the advancement of technology, real-world networks have become vulnerable to many attacks such as cyber-crimes, terrorist attacks, and financial frauds. Accuracy and scalability are the two principal but contrary characteristics for algorithms detecting such attacks (or events) in these time-varying networks. However, existing approaches confirm to either of these two prerequisites. Hence, we propose two algorithms designated as GraphAnomaly and GraphAnomaly-CS, both satisfying these two requirements together. Given a stream of time-evolving real-world network edges, the proposed algorithms first extract the local structure of network graphs by identifying the relationship between egonets and their properties, and then use this information in Chi-square statistics to discover (1) anomalous time-points at which many network nodes deviate from their normal behavior and (2) those nodes and features that majorly contribute to the change. The proposed algorithms are (a) accurate: upto 7 to 12% more accurate than state-of-the-art methods; (b) speedy: process millions of edges within a few minutes; (c) scalable: scale linearly with the number of edges and nodes in the network graph; (d) theoretically sound: providing theoretical guarantees on the false positive probability of algorithms; We show theoretically and experimentally that the proposed algorithms successfully detect anomalies in time-evolving edge streams. We have selected six baselines, five evaluation metrics, and six real-world network datasets from three different network classes for empirical analysis. The experimental results show that both algorithms are efficient at detecting anomalies in networks that reduce false positives and false negatives in the results, especially in successive time-points. Furthermore, algorithms discover the maximum number of critical events from real-world networks, demonstrating their effectiveness over baselines.",Low
"With the advent of the multimedia era, the identification of sensitive information in social data of online social network users has become critical for maintaining the security of network community information. Currently, traditional sensitive information identification techniques in online social networks cannot acquire the full semantic knowledge of multimodal data and cannot learn cross-information between data modalities. Therefore, it is urgent to study a new multimodal deep learning model that considers semantic relationships. This paper presents an improved multimodal dual-channel reasoning mechanism (MDR), which deeply mines semantic information and implicit association relationships between modalities based on the consideration of multimodal data fusion. In addition, we propose a multimodal adaptive spatial attention mechanism (MAA) to improve the accuracy and flexibility of the decoder. We manually annotated real social data of 50 users to train and test our model. The experimental results show that the proposed method significantly outperforms simple multimodal fusion deep learning models in terms of sensitive information prediction accuracy and adaptability and verifies the feasibility and effectiveness of a multimodal deep model considering semantic strategies in social network sensitive information identification.",Low
"With the development of cryptocurrenciesâ€™ market, the problem of cryptojacking, which is an unauthorized control of someone elseâ€™s computer to mine cryptocurrency, has been more and more serious. Existing cryptojacking detection methods require to install anti-virus software on the host or load plug-in in the browser, which are difficult to deploy on enterprise or campus networks with a large number of hosts and servers. To bridge the gap, we propose MineHunter, a practical cryptomining traffic detection algorithm based on time series tracking. Instead of being deployed at the hosts, MineHunter detects the cryptomining traffic at the entrance of enterprise or campus networks. Minehunter has taken into account the challenges faced by the actual deployment environment, including extremely unbalanced datasets, controllable alarms, traffic confusion, and efficiency. The accurate network-level detection is achieved by analyzing the network traffic characteristics of cryptomining and investigating the association between the network flow sequence of cryptomining and the block creation sequence of cryptocurrency. We evaluate our algorithm at the entrance of a large office building in a campus network for a month. The total volumes exceed 28 TeraBytes. Our experimental results show that MineHunter can achieve precision of 97.0% and recall of 99.7%.",Low
"With the development of network communication technology and multimedia audio-visual technology, in the era of shared economy, the hot topics of education such as ""MOOC"" and ""network course platform"" are getting more and more from the Ministry of Education. The attention and favor of colleges and universities at all levels as well as teaching-related groups, however, because ""online education"" started relatively late, the concept of education has not been updated in time, and the understanding of technology and education is not correct. The construction of ""network teaching platform"" in colleges and universities has failed to achieve the expected effect, and even led to the erroneous cognition of ""information-based education"" and ""shared education"" at one time. Taking Chinese University MOOC as an example, this paper uses the methods of case analysis, literature research, online investigation and so on to summarize and sort out the advantages of Chinese university MOOC and its enlightenment to the network teaching platform of colleges and universities. Then the development trend of network teaching platform in colleges and universities under the background of Internet is analyzed. The construction of network teaching platform in colleges and universities should be fully aware of their strengths and weaknesses, make them keep up with the pace of the times under the support of new technological trends and policies, and look forward to their future development under the new situation of ""Internet"". The network teaching platform of colleges and universities can only grasp its own position, carry on its own reform and strengthen the learning and application of new technologies (cloud computing, big data analysis, AI artificial intelligence, VR virtual reality).",Low
"With the economic development, smart communities have been widely studied and applied. However, the system in this field is not perfect, and there are still a series of problems, such as high construction cost, low level of intelligence, mutual independence of different systems, difficulty in unified management, and so on. To solve the above problems, this paper proposes the smart community big data dynamic analysis model based on logistic regression model. First, this paper constructs the big data research architecture of smart community based on IOT technology, including IAAs, DAAS, PAAS, and SaaS layers and the virtual service layer of resource scheduling of spatiotemporal information cloud platform optimized by spatiotemporal law. And the IoT platform is designed to collect data to lay the foundation for research. Second, this paper is oriented to the big data application requirements with distribution and mobility as the main technical characteristics. Based on the distributed data flow, this paper designs mining operator to provide technical support for the data mining algorithm; at the same time, this paper constructs a high-dimensional random matrix model for measuring big data and then deduces its abnormal data detection theory and method to detect high-dimensional abnormal data. Finally, this paper uses logistic regression model to predict the development trend of smart community and provide guarantee for smart community service. The simulation results show the efficiency and accuracy of prediction can be improved based on logistic regression model. Furthermore, it effectively avoid repeated construction and waste of resources in the community and form a new community management model based on intelligent and information-based social management and service.",Low
"With the escalating degree of networking within the computerized numerical control (CNC) industry, the information security of NC code is encountering severe challenges. In order to prevent reference errors, this paper designs an anomaly detection scheme of NC code based on incremental learning, and proposes a two-layer type recognition model of NC code based on ITI incremental decision tree and Bayes. Text preprocessing of NC code is carried out, and feature selection of processing result is carried out. The NC code data set is used to construct a basic decision tree learner with attribute statistics. The learning process of incremental decision tree, model recognition and transformation segmentation scoring process are designed. Finally, the validity of the incremental learning type recognition model is tested. The results show that the recognition accuracy of the proposed model can gradually increase with the increase of samples, which can reach more than 95%, and the misjudgment rate is less than 5%, which is better than the decision tree and Bayesian model.",Medium
"With the evolution of internet and computer networks, security has become a major concern over the years. Security is a focal aspect of every computer system and so the quality of these systems depends on the provided functionalities as well as the degree of their security. Generally, we trust the used networks when using our personal and sensitive information. However, several threats and attacks of stealing our information and harming our computers are possible. Therefore, intrusion detection system is one of the most widely used systems to diagnose various threats and malicious activity on computer networks. There are a lot of works that have proposed MAS-based intrusion diagnostic techniques to handle attacks. In this paper, we proposed an approach for intrusion detection system that uses a set of mobile agents to ensure the protection of the whole data and machines from attackers. Moreover, to detect possible attacks, we use the scenario method that is based on the comparison of the packets received in the network with the information stored in the attacks signature database.",Low
"With the growing demand for computility, the reliability of computility services has become increasingly crucial. Due to the escalating volume and complexity of tasks processed, computility services often need to operate under high load, which can easily lead to issues such as resource shortages and service interruptions. Logs in computility services meticulously record the operational information of each component; therefore, anomaly detection based on logs can effectively ensure the stable operation of computility services. This study aims to address two challenges in the field of log anomaly detection. First, this study addresses the previously overlooked issue of class-imbalanced log data. Second, given the massive volumes of log data, the time required for model training poses a significant challenge. To address these issues, we propose EDSLog, a novel efficient log anomaly detection framework based on dataset partitioning. Initially, EDSLog processes log sequences through the Weight-Based K-fold Sub Hold-out Method (WKHM), effectively alleviating the class-imbalance problem. Subsequently, EDSLog leverages Simple Recurrent Units (SRU) enhanced by a self-attention mechanism to extract features from log sequences. Finally, EDSLog determines whether the predicted log data are anomalous. Experiments show that EDSLog achieves the best evaluation metrics in class-imbalanced datasets while having the shortest total model runtime. Specifically, EDSLog achieved the highest F1 scores of 100 and 99.96 respectively on the BGL and HDFS datasets, where abnormal logs account for 0.1% of the data. Additionally, EDSLogâ€™s training speed was 35.62% faster than the model with the second shortest training duration among all models compared.",Low
"With the increase in cyber threats in recent years, there have been more forms of demand for network security protection measures. Network traffic classification technology is used to adapt to the dynamic threat environment. However, network traffic has a natural unbalanced class distribution problem, and the single model leads to the low accuracy and high false-positive rate of the traditional detection model. Given the above two problems, this paper proposes a new dataset balancing method named SD sampling based on the SMOTE algorithm. Different from the SMOTE algorithm, this method divides the sample into two types that are easy and difficult to classify and only balances the difficult-to-classify sample, which not only overcomes the SMOTEâ€™s overgeneralization but also combines the idea of oversampling and undersampling. In addition, a two-layer structure combined with XGBoost and the random forest is proposed for multiclassification of anomalous traffic, since using a hierarchical structure can better classify minority abnormal traffic. This paper conducts experiments on the CICIDS2017 dataset. The results show that the classification accuracy of the proposed model is more than 99.70% and that the false-positive rate is less than 0.34%, indicating that the proposed model is better than traditional models.",Low
"With the increasing miniaturization of smartphones, computers, and sensors in the Internet of Things (IoT) paradigm, strengthening the security and preventing ransomware attacks have become key concerns. Traditional security mechanisms are no longer applicable because of the involvement of resource-constrained devices, which require more computation power and resources. This paper presents the ransomware attacks and security concerns in IoT. We initially discuss the rise of ransomware attacks and outline the associated challenges. Then, we investigate, report, and highlight the state-of-the-art research efforts directed at IoT from a security perspective. A taxonomy is devised by classifying and categorizing the literature based on important parameters (e.g., threats, requirements, IEEE standards, deployment level, and technologies). Furthermore, a few credible case studies are outlined to alert people regarding how seriously IoT devices are vulnerable to threats. We enumerate the requirements that need to be met for securing IoT. Several indispensable open research challenges (e.g., data integrity, lightweight security mechanisms, lack of security softwares upgradability and patchability features, physical protection of trillions of devices, privacy, and trust) are identified and discussed. Several prominent future research directions are provided.",Low
"With the increasing number of Internet users, cybersecurity is becoming more and more critical. Denial of service (DoS) and distributed denial of service (DDoS) attacks are two of the most common types of attacks that can severely affect a website or a server and make them unavailable to other users. The number of DDoS attacks increased by 55% between the period January 2020 and March 2021. Some approaches for detecting the DoS and DDoS attacks employing different machine learning and deep learning techniques are reported in the literature. Recently, it is also observed that the attackers have started leveraging state-of-the-art AI tools such as generative models for generating synthetic attacks which fool the standard detectors. No concrete approach is reported for developing and training the models which are not only robust in the detection of standard DDoS attacks but which can also detect adversarial attacks which are created synthetically by the attackers with harmful intentions. To that end, in this work, we employ a generative adversarial network (GAN) to develop such a robust detector. The proposed framework can generate and classify the synthetic benign (normal) and malignant (DDoS) instances which are very similar to the corresponding real instances as evaluated by similarity scores. The GAN-based model also demonstrates how effectively the malicious actors can generate adversarial DDoS network traffic instances which look like normal instances using feature modification which are very difficult for the classifier to detect. An approach on how to make the classifiers robust enough to detect such kinds of deliberate adversarial attacks via modifying some specific attack features manually is also proposed. This work provides the first step towards developing a generic and robust detector for DDoS attacks originating from various sources.",Low
"With the increasingly growing internal and external attacks on computer systems and online services, cybersecurity has become a vibrant research area. Countering intrusive attacks is a daunting task with no universal magic solution that can successfully handle all scenarios. A variety of machine-learning and computational intelligence techniques have been applied extensively to detect and classify these attacks. However, the effectiveness of these techniques greatly depends on the adopted data preprocessing methods for feature extraction and engineering. This paper presents an extended taxonomy of the work related to intrusion detection and reviews the state-of-the-art techniques for data preprocessing. It offers a critical up-to-date survey which can be an instrumental pedagogy to help junior researchers conceive the vast amount of research work and gain a holistic view and awareness of various contemporary research directions in this domain.",Low
"With the outbreak of the new crown epidemic, the world economy has been severely tested, making predictions more difficult. Wireless sensors have the advantages of low cost, ease of use, high reliability, and high safety and have been widely used in the tourism economy. In order to understand the ability of wireless sensors to predict the regional economy, this article uses an example to construct a nonlinear model of wireless sensors to predict the regional economy. With the continuous development of the concept of circular economy, circular economy has gradually been recognized by Chinese scholars and practitioners. After domestic scholars continue to study the theory of circular economy, practicing the concept of circular economy and taking the road of sustainable development have become one of the important directions of the development of industrial theory. Literature analysis and other methods were used to conduct research on databases such as CNKI, Wan fang Database, and SSCI. Literature was collected, and GIS spatial analysis technology was used to analyze different areas and finally get a prediction model. The phenomenon is nonlinearity (such as saturation nonlinearity in the magnetic circuit), and some are caused by the nonlinear relationship between system variables (such as linear resistance and squared nonlinearity between current and power) and some artificially introduced nonlinear links (such as the hysteresis nonlinearity of relays). Experiments have proved that there is a certain error between the prediction model and the actual result; the error value is about 9%, which is less than the value of other prediction models. This shows that the output results of the nonlinear model of wireless sensor regional economic prediction should be processed reasonably. This result has a certain reference value, and its output should be combined with the actual situation. Related research found that under the nonlinear model, the more accurate and comprehensive the input value is, the closer the output result is to the actual value.",Low
"With the proliferation of mobile sensing techniques, huge amounts of time series data are generated and accumulated in various domains, fueling plenty of real-world applications. In this setting, time series anomaly detection is practically important. It endeavors to identify deviant samples from the normal sample distribution in time series. Existing approaches generally assume that all the time series is available at a central location. However, we are witnessing the decentralized collection of time series due to the deployment of various edge devices. To bridge the gap between the decentralized time series data and the centralized anomaly detection algorithms, we propose a &lt;u&gt;P&lt;/u&gt;arameter-&lt;u&gt;e&lt;/u&gt;fficient &lt;u&gt;F&lt;/u&gt;ederated &lt;u&gt;A&lt;/u&gt;nomaly &lt;u&gt;D&lt;/u&gt;etection framework named PeFAD with the increasing privacy concerns. PeFAD for the first time employs the pre-trained language model (PLM) as the body of the client's local model, which can benefit from its cross-modality knowledge transfer capability. To reduce the communication overhead and local model adaptation cost, we propose a parameter-efficient federated training module such that clients only need to fine-tune small-scale parameters and transmit them to the server for update. PeFAD utilizes a novel anomaly-driven mask selection strategy to mitigate the impact of neglected anomalies during training. A knowledge distillation operation on a synthetic privacy-preserving dataset that is shared by all the clients is also proposed to address the data heterogeneity issue across clients. We conduct extensive evaluations on four real datasets, where PeFAD outperforms existing state-of-the-art baselines by up to 28.74%.",Low
"With the rapid development of digital technology, its application in the field of education has become more and more extensive. Since 2020, digital education has become an important way for international students coming to China to receive education. Digital technology provides a new platform for the teaching of international students in China, and this study explores and analyzes the current value and dilemma of digital technology in the education of international students in China, and explores the integration path of integrating digital technology such as Big Data, Artificial Intelligence, and Virtual Reality into the national education of international students in China, in order to promote the quality of international students' education and increase its efficiency.",Low
"With the rapid development of the Internet since the beginning of the 21st century, social networks have provided a significant amount of convenience for work, study, and entertainment. Specifically, because of the irreplaceable superiority of social platforms in disseminating information, criminals have thus updated the main methods of social engineering attacks. Detecting abnormal accounts on social networks in a timely manner can effectively prevent the occurrence of malicious Internet events. Different from previous research work, in this work, a method of anomaly detection called Hurst of Interest Distribution is proposed based on the stability of user interest quantifiable from the content of usersâ€™ tweets, so as to detect abnormal accounts. In detail, the Latent Dirichlet Allocation model is adopted to classify blog content on Twitter into topics to calculate and obtain the topic distribution of tweets sent by a single user within a period of time. Then, the stability degree of the userâ€™s tweet topic preference is calculated according to the Hurst index to determine whether the account is compromised. Through experiments, the Hurst indexes of normal and abnormal accounts are found to be significantly different, and the detection rate of abnormal accounts using the proposed method can reach up to 97.93%.",Low
"With the rapid proliferation of Internet of Things (IoT) devices, surveillance systems have become ubiquitous, providing crucial monitoring capabilities across various domains such as security, healthcare, and manufacturing. However, efficiently detecting abnormal activities within the vast streams of surveillance data remains a significant challenge. Traditional methods often struggle to adapt to dynamic environments and diverse anomalies, necessitating more sophisticated approaches. This paper proposes a novel framework for abnormal activity identification in IoT surveillance systems leveraging Deep Q Network (DQN) architecture. DQN, a form of reinforcement learning, has shown remarkable success in learning optimal strategies from high-dimensional sensory inputs. By integrating DQN into IoT surveillance systems, we aim to enhance their anomaly detection capabilities.",Low
"With the swift progression of industrial automation and Internet of Things technologies, the importance of multivariate time series anomaly detection has markedly increased, serving as a vital tool for identifying abnormal behaviors within complex datasets to prevent potential risks. Traditional anomaly detection methods often struggle to deal with multivariable and unlabeled data environments, especially in the context of real-time dynamic data streams, where traditional models require frequent retraining to adapt to new anomaly patterns. To address this challenge, our work proposes an online anomaly detection model for multivariate time series based on a contrastive learning framework:ODAnomaly, utilizing a dual autocorrelation mechanism to effectively extract features of normal data and distinguish anomalous data. The model features an online learner that uses gradient updates and Pearson correlation coefficients to rapidly adapt to new anomaly patterns, boosting its real-time learning efficiency. A contrastive loss function, informed by homoscedastic uncertainty, aids in anomaly detection through data representation. This approach reduces reliance on extensively labeled data and enhances the modelâ€™s adaptability and accuracy in real-time data streams. It provides an efficient and cost-effective solution for advancing multivariate time series anomaly detection in both research and practical applications.",Medium
"With the widespread use of the industrial internet, industrial control systems (ICS) are increasingly vulnerable to network intrusions, making their security challenges more prominent. Current intrusion detection methods primarily focus on closed-set scenarios; however, in open-set scenarios, numerous unknown classes of intrusions lead to poor detection performance. To address this issue, this paper proposes a novel open-set industrial network intrusion detection method. The method begins with a feature extractor, termed GPL (BiGRU-PL), which integrates bidirectional gated recurrent units (BiGRU) and prototype learning (PL) through joint training. This model is trained using Distance-Based Cross-Entropy (DCE) loss and Prototype Loss, effectively enhancing intra-class compactness for known classes in the open set, increasing inter-class distance, and achieving separability between unknown and known classes in the feature space. Following this, the Weibull model is employed to construct an Extreme Value Theory (EVT) discriminant module that fits each known class and calculates an outlier probability threshold for each known class. Finally, the method computes the outlier probability value of each input sample for each known category and compares the test sampleâ€™s outlier probability value against the threshold to recognize unknown classes. Experimental results on the CICIDS2017 dataset and Gas Pipeline dataset demonstrate the effectiveness of the proposed method. The method achieves high accuracy in recognizing known classes. Additionally, it effectively identifies unknown class intrusions in open-set scenarios with varying degrees of openness.",Medium
"World Wide Web (WWW) is one of the most popular applications currently running on the Internet and web server is a crucial component for this application. However, network anomalies especially Distributed Denial-of-Service (DDoS) attacks bombard web server, degrade its Quality of Service (QoS) and even deny the legitimate users' requests. Traditional network anomaly detection methods often lead to high false positives and expensive computational cost, thus unqualified for real-time web server anomaly detection. To solve these problems, in this paper we first propose an efficient network anomaly detection method based on Transductive Confidence Machines for K-Nearest Neighbors (TCM-KNN) algorithm. Secondly, we integrate a lot of objective and efficient anomalies impact metrics from the perceptions of the end users into TCM-KNN algorithm to build a robust web sever anomaly detection mechanism. Finally, Genetic Algorithm (GA) based instance selection method is introduced to boost the real-time detection performance of our method. We evaluate our method on a series of experiments both on well-known KDD Cup 1999 dataset and concrete dataset collected from real network traffic. The results demonstrate our methods are actually effective and lightweight for real-time web server anomaly detection.",Low
