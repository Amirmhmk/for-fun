asessor,Search Type,Source,Http,Title,Year,Type,Venue,Core,Abstract,Keywords,Citation,,,,,
HELLDFJDSA;LFJSD;LKFJDSDFHELLOLWOROLD WE ARE HERE TO FIND OUYT77777FF,Initial hypotheses,Google Scholar,https://ieeexplore.ieee.org/abstract/document/9953983,Analysis of Continual Learning Models for Intrusion Detection System,2022,Review,IEEE Access,A*,"Deep Learning based Intrusion Detection Systems (IDSs) have received significant attention  from the research community for their capability to handle modern-day security systems in large-scale  networks. Despite their considerable improvement in performance over machine learning-based techniques  and conventional statistical models, deep neural networks (DNN) suffer from catastrophic forgetting: the  model forgets previously learned information when trained on newer data points. This vulnerability is  specifically exaggerated in large scale systems due to the frequent changes in network architecture and  behaviours, which leads to changes in data distribution and the introduction of zero-day attacks; this  phenomenon is termed as covariate shift. Due to these constant changes in the data distribution, the DNN  models will not be able to consistently perform at high accuracy and low false positive rate (FPR) rates  without regular updates. However, before we update the DNN models, it is essential to understand the  magnitude and nature of the drift in the data distribution. In this paper, to analyze the drift in data distribution,  we propose an eight-stage statistics and machine learning guided implementation framework that objectively  studies and quantifies the changes. Further, to handle the changes in data distribution, most IDS solutions  collect the network packets and store them to retrain the DNN models periodically, but when the network’s  size and complexity increase, those tasks become expensive. To efficiently solve this problem, we explore  the potential of continual learning models to incrementally learn new data patterns while also retaining  their previous knowledge. We perform an experimental and analytical study of advanced intrusion detection  systems using three major continual learning approaches: learning without forgetting, experience replay, and  dark experience replay on the NSL-KDD and the CICIDS 2017 dataset. Through extensive experimentation,  we show that our continual learning models achieve improved accuracy and lower FPR rates when compared  to the state-of-the-art works while also being able to incrementally learn newer data patterns. Finally,  we highlight the drawbacks of traditional statistical and non-gradient based machine learning approaches  in handling the covariate shift problem.","Intrusion detection systems, catastrophic forgetting, covariate shift, continual learning.",22,,,,"Deep Learning based Intrusion Detection Systems (IDSs) have received significant attention  from the research community for their capability to handle modern-day security systems in large-scale  networks. Despite their considerable improvement in performance over machine learning-based techniques  and conventional statistical models, deep neural networks (DNN) suffer from catastrophic forgetting: the  model forgets previously learned information when trained on newer data points. This vulnerability is  specifically exaggerated in large scale systems due to the frequent changes in network architecture and  behaviours, which leads to changes in data distribution and the introduction of zero-day attacks; this  phenomenon is termed as covariate shift. Due to these constant changes in the data distribution, the DNN  models will not be able to consistently perform at high accuracy and low false positive rate (FPR) rates  without regular updates. However, before we update the DNN models, it is essential to understand the  magnitude and nature of the drift in the data distribution. In this paper, to analyze the drift in data distribution,  we propose an eight-stage statistics and machine learning guided implementation framework that objectively  studies and quantifies the changes. Further, to handle the changes in data distribution, most IDS solutions  collect the network packets and store them to retrain the DNN models periodically, but when the network’s  size and complexity increase, those tasks become expensive. To efficiently solve this problem, we explore  the potential of continual learning models to incrementally learn new data patterns while also retaining  their previous knowledge. We perform an experimental and analytical study of advanced intrusion detection  systems using three major continual learning approaches: learning without forgetting, experience replay, and  dark experience replay on the NSL-KDD and the CICIDS 2017 dataset. Through extensive experimentation,  we show that our continual learning models achieve improved accuracy and lower FPR rates when compared  to the state-of-the-art works while also being able to incrementally learn newer data patterns. Finally,  we highlight the drawbacks of traditional statistical and non-gradient based machine learning approaches  in handling the covariate shift problem.",High
-H--SD00(,Initial hypotheses,Google Scholar,https://ieeexplore.ieee.org/abstract/document/9064134,Continuous and Adaptive Learning over Big Streaming Data for Network Security,2019,Research Paper,IEEE International Conference on Cloud Networking (CloudNet),N/A,"Continuous and adaptive learning is an effective learning approach when dealing with highly dynamic and changing scenarios, where concept drift often happens. In a continuous, stream or adaptive learning setup, new measurements arrive continuously and there are no boundaries for learning, meaning that the learning model has to decide how and when to (re)learn from these new data constantly. We address the problem of adaptive and continual learning for network security, building dynamic models to detect network attacks in real network traffic. The combination of fast and big network measurements data with the re-training paradigm of adaptive learning imposes complex challenges in terms of data processing speed, which we tackle by relying on big data platforms for parallel stream processing. We build and benchmark different adaptive learning models on top of a novel big data analytics platform for network traffic monitoring and analysis tasks, and show that high speed-up computations (as high as × 6) can be achieved by parallelizing off-the-shelf stream learning approaches.","Stream Machine Learning; Network Security; Big-Data.",8,,,,"Continuous and adaptive learning is an effective learning approach when dealing with highly dynamic and changing scenarios, where concept drift often happens. In a continuous, stream or adaptive learning setup, new measurements arrive continuously and there are no boundaries for learning, meaning that the learning model has to decide how and when to (re)learn from these new data constantly. We address the problem of adaptive and continual learning for network security, building dynamic models to detect network attacks in real network traffic. The combination of fast and big network measurements data with the re-training paradigm of adaptive learning imposes complex challenges in terms of data processing speed, which we tackle by relying on big data platforms for parallel stream processing. We build and benchmark different adaptive learning models on top of a novel big data analytics platform for network traffic monitoring and analysis tasks, and show that high speed-up computations (as high as × 6) can be achieved by parallelizing off-the-shelf stream learning approaches.",High
Amir Bidaki,Initial hypotheses,Google Scholar,https://dl.acm.org/doi/abs/10.1145/3229607.3229612,Stream-based Machine Learning for Network Security and Anomaly Detection,2018,Research Paper,SIGCOMM,A*,"Continuous and adaptive learning is an effective  learning approach when dealing with highly dynamic and changing scenarios, where concept drift often happens. In a continuous,  stream or adaptive learning setup, new measurements arrive  continuously and there are no boundaries for learning, meaning  that the learning model has to decide how and when to (re)learn  from these new data constantly. We address the problem of  adaptive and continual learning for network security, building  dynamic models to detect network attacks in real network traffic.  The combination of fast and big network measurements data with  the re-training paradigm of adaptive learning imposes complex  challenges in terms of data processing speed, which we tackle by  relying on big data platforms for parallel stream processing. We  build and benchmark different adaptive learning models on top of  a novel big data analytics platform for network traffic monitoring  and analysis tasks, and show that high speed-up computations  (as high as × 6) can be achieved by parallelizing off-the-shelf  stream learning approaches.","Data Stream mining; Machine Learning; Network Attacks; HighDimensional Data.",58,,,,"Continuous and adaptive learning is an effective  learning approach when dealing with highly dynamic and changing scenarios, where concept drift often happens. In a continuous,  stream or adaptive learning setup, new measurements arrive  continuously and there are no boundaries for learning, meaning  that the learning model has to decide how and when to (re)learn  from these new data constantly. We address the problem of  adaptive and continual learning for network security, building  dynamic models to detect network attacks in real network traffic.  The combination of fast and big network measurements data with  the re-training paradigm of adaptive learning imposes complex  challenges in terms of data processing speed, which we tackle by  relying on big data platforms for parallel stream processing. We  build and benchmark different adaptive learning models on top of  a novel big data analytics platform for network traffic monitoring  and analysis tasks, and show that high speed-up computations  (as high as × 6) can be achieved by parallelizing off-the-shelf  stream learning approaches.",High
Amir Bidaki,Initial hypotheses,Google Scholar,https://repository.rit.edu/theses/11758/,Continual Learning for an Ever Evolving and Intelligent Malware Classification System,2024,PHD Dissertation,-,N/A,"Malware classification poses unique challenges for continual learning (CL) systems, driven by  the daily influx of new samples and the evolving nature of malware threats that exploit new  vulnerabilities. Antivirus vendors encounter hundreds of thousands of unique software pieces  daily, encompassing both malicious and benign files. Over its operational life, a malware  classifier can accumulate more than a billion samples. Training malware classification system  with only new samples and classes leads to catastrophic forgetting (CF), where the system  forgets previously learned data distribution. While retraining with all old and new samples  effectively combats CF, it is computationally expensive and necessitates storing vast amounts  of older software and malware samples. Employing sequential training with CL strategies  offers a potential solution to mitigate these challenges by reducing both training and storage  demands. However, the adoption of CL for malware classification has not been extensively  explored. This work represents the first in-depth examination of CL not just in the realm of  malware classification, but also more broadly within the cybersecurity domain.",Malware (Computer software)--Classification; Transfer learning (Machine learning); Cyberterrorism--Prevention,0,,,,"Malware classification poses unique challenges for continual learning (CL) systems, driven by  the daily influx of new samples and the evolving nature of malware threats that exploit new  vulnerabilities. Antivirus vendors encounter hundreds of thousands of unique software pieces  daily, encompassing both malicious and benign files. Over its operational life, a malware  classifier can accumulate more than a billion samples. Training malware classification system  with only new samples and classes leads to catastrophic forgetting (CF), where the system  forgets previously learned data distribution. While retraining with all old and new samples  effectively combats CF, it is computationally expensive and necessitates storing vast amounts  of older software and malware samples. Employing sequential training with CL strategies  offers a potential solution to mitigate these challenges by reducing both training and storage  demands. However, the adoption of CL for malware classification has not been extensively  explored. This work represents the first in-depth examination of CL not just in the realm of  malware classification, but also more broadly within the cybersecurity domain.",High
Amir Bidaki,Initial hypotheses,Google Scholar,https://www.mdpi.com/2504-4990/6/4/135,Continual Semi-Supervised Malware Detection,2024,Research Paper,Machine Learning and Knowledge Extraction,N/A,"Detecting malware has become extremely important with the increasing exposure of computational systems and mobile devices to online services. However, the rapidly evolving nature of malicious software makes this task particularly challenging. Despite the significant number of machine learning works for malware detection proposed in the last few years, limited interest has been devoted to continual learning approaches, which could allow models to showcase effective performance in challenging and dynamic scenarios while being computationally efficient. Moreover, most of the research works proposed thus far adopt a fully supervised setting, which relies on fully labelled data and appears to be impractical in a rapidly evolving malware landscape. In this paper, we address malware detection from a continual semi-supervised one-class learning perspective, which only requires normal",continual learning; malware detection; semi supervised learning; one-class learning; anomaly detection,0,,,,"Detecting malware has become extremely important with the increasing exposure of computational systems and mobile devices to online services. However, the rapidly evolving nature of malicious software makes this task particularly challenging. Despite the significant number of machine learning works for malware detection proposed in the last few years, limited interest has been devoted to continual learning approaches, which could allow models to showcase effective performance in challenging and dynamic scenarios while being computationally efficient. Moreover, most of the research works proposed thus far adopt a fully supervised setting, which relies on fully labelled data and appears to be impractical in a rapidly evolving malware landscape. In this paper, we address malware detection from a continual semi-supervised one-class learning perspective, which only requires normal",Medium
Amir Bidaki,Initial hypotheses,Google Scholar,https://proceedings.mlr.press/v199/rahman22a.html,On the Limitations of Continual Learning for Malware Classification,2022,Research Paper,PMLR ,N/A,"Malicious software (malware) classification offers a unique challenge for continual learning (CL) regimes due to the volume of new samples received on a daily basis and the evolution of malware to exploit new vulnerabilities. On a typical day, antivirus vendors receive hundreds of thousands of unique pieces of software, both malicious and benign, and over the course of the lifetime of a malware classifier, more than a billion samples can easily accumulate. Given the scale of the problem, sequential training using continual learning techniques could provide substantial benefits in reducing training and storage overhead. To date, however, there has been no exploration of CL applied to malware classification tasks. In this paper, we study 11 CL techniques applied to three malware tasks covering common incremental learning scenarios, including task, class, and domain incremental learning (IL). Specifically, using two realistic, large-scale malware datasets, we evaluate the performance of the CL methods on both binary malware classification (Domain-IL) and multi-class malware family classification (Task-IL and Class-IL) tasks. To our surprise, continual learning methods significantly underperformed naive {\em Joint} replay of the training data in nearly all settings – in some cases reducing accuracy by more than 70 percentage points. A simple approach of selectively replaying 20% of the stored data achieves better performance, with 50% of the training time compared to {\em Joint} replay. Finally, we discuss potential reasons for the unexpectedly poor performance of the CL techniques, with the hope that it spurs further research on developing techniques that are more effective in the malware classification domain.",,9,,,,"Malicious software (malware) classification offers a unique challenge for continual learning (CL) regimes due to the volume of new samples received on a daily basis and the evolution of malware to exploit new vulnerabilities. On a typical day, antivirus vendors receive hundreds of thousands of unique pieces of software, both malicious and benign, and over the course of the lifetime of a malware classifier, more than a billion samples can easily accumulate. Given the scale of the problem, sequential training using continual learning techniques could provide substantial benefits in reducing training and storage overhead. To date, however, there has been no exploration of CL applied to malware classification tasks. In this paper, we study 11 CL techniques applied to three malware tasks covering common incremental learning scenarios, including task, class, and domain incremental learning (IL). Specifically, using two realistic, large-scale malware datasets, we evaluate the performance of the CL methods on both binary malware classification (Domain-IL) and multi-class malware family classification (Task-IL and Class-IL) tasks. To our surprise, continual learning methods significantly underperformed naive {\em Joint} replay of the training data in nearly all settings – in some cases reducing accuracy by more than 70 percentage points. A simple approach of selectively replaying 20% of the stored data achieves better performance, with 50% of the training time compared to {\em Joint} replay. Finally, we discuss potential reasons for the unexpectedly poor performance of the CL techniques, with the hope that it spurs further research on developing techniques that are more effective in the malware classification domain.",High
Amir Bidaki,Initial hypotheses,Google Scholar,https://dl.acm.org/doi/abs/10.1145/3702990,Temporal-Incremental Learning for Android Malware Detection,2024,Research Paper,ACM Transactions on Software Engineering and Methodology,A*,"Malware classification is a specific and refined task within the broader malware detection problem. Effective classification aids in understanding attack techniques and developing robust defenses, ensuring application security and timely mitigation of software vulnerabilities. The dynamic nature of malware demands adaptive classification techniques that can handle the continuous emergence of new families. Traditionally, this is done by retraining models on all historical samples, which requires significant resources in terms of time and storage. An alternative approach is Class-Incremental Learning (CIL), which focuses on progressively learning new classes (malware families) while preserving knowledge from previous training steps. However, CIL assumes that each class appears only once in training and is not revisited, an assumption that does not hold for malware families, which often persist across multiple time intervals. This leads to shifts in the data distribution for the same family over time, a challenge that is not addressed by traditional CIL methods. We formulate this problem as Temporal-Incremental Malware Learning (TIML), which adapts to these shifts and effectively classifies new variants. To support this, we organize the MalNet dataset, consisting of over a million entries of Android malware data collected over a decade, in chronological order. We first adapt state-of-the-art CIL approaches to meet TIML's requirements, serving as baseline methods. Then, we propose a novel multimodal TIML approach that leverages multiple malware modalities for improved performance. Extensive evaluations show that our TIML approaches outperform traditional CIL methods and demonstrate the feasibility of periodically updating malware classifiers at a low cost. This process is efficient and requires minimal storage and computational resources, with only a slight dip in performance compared to full retraining with historical data.","Malware Classification, Incremental Learning",1,,,,"Malware classification is a specific and refined task within the broader malware detection problem. Effective classification aids in understanding attack techniques and developing robust defenses, ensuring application security and timely mitigation of software vulnerabilities. The dynamic nature of malware demands adaptive classification techniques that can handle the continuous emergence of new families. Traditionally, this is done by retraining models on all historical samples, which requires significant resources in terms of time and storage. An alternative approach is Class-Incremental Learning (CIL), which focuses on progressively learning new classes (malware families) while preserving knowledge from previous training steps. However, CIL assumes that each class appears only once in training and is not revisited, an assumption that does not hold for malware families, which often persist across multiple time intervals. This leads to shifts in the data distribution for the same family over time, a challenge that is not addressed by traditional CIL methods. We formulate this problem as Temporal-Incremental Malware Learning (TIML), which adapts to these shifts and effectively classifies new variants. To support this, we organize the MalNet dataset, consisting of over a million entries of Android malware data collected over a decade, in chronological order. We first adapt state-of-the-art CIL approaches to meet TIML's requirements, serving as baseline methods. Then, we propose a novel multimodal TIML approach that leverages multiple malware modalities for improved performance. Extensive evaluations show that our TIML approaches outperform traditional CIL methods and demonstrate the feasibility of periodically updating malware classifiers at a low cost. This process is efficient and requires minimal storage and computational resources, with only a slight dip in performance compared to full retraining with historical data.",Medium
Amir Bidaki,Initial hypotheses,Google Scholar,https://www.mdpi.com/2076-3417/13/22/12255,SSCL-TransMD: Semi-Supervised Continual Learning Transformer for Malicious Software Detection,2023,Research Paper,Applied Sciences,N/A,"Machine learning-based malware (malicious software) detection methods have a wide range of real-world applications. However, these types of approaches suffer from the fatal problem of “model aging”, in which the validity of the model decreases rapidly as the malware continues to evolve and variants emerge continuously. The model aging problem is usually solved by model retraining, which relies on lots of labeled samples obtained at great expense. To address this challenge, this paper proposes a semi-supervised continuous learning malware detection model based on Transformer. Firstly, this model improves the lifelong semi-supervised mixture algorithm to dynamically adjust the weighted combination of new sample sequences and historical ones to solve the imbalance problem. Secondly, the Learning with Local and Global Consistency algorithm is used to iteratively compute similarity scores for the unlabeled samples in the mixed samples to obtain pseudo-labels. Lastly, the Multilayer Perceptron is applied for malware classification. To validate the effectiveness of the model, this paper conducts experiments on the CICMalDroid2020 dataset. The experimental results show that the proposed model performs better than existing deep learning detection models. The F1 score has an average improvement of 1.27% compared to other models when conducting binary classification. And, after inputting hybrid samples, including historical data and new data, four times, the F1 score is still 1.96% higher than other models.",android malware detection; deep learning; transformer; semi-supervised continual learning,2,,,,"Machine learning-based malware (malicious software) detection methods have a wide range of real-world applications. However, these types of approaches suffer from the fatal problem of “model aging”, in which the validity of the model decreases rapidly as the malware continues to evolve and variants emerge continuously. The model aging problem is usually solved by model retraining, which relies on lots of labeled samples obtained at great expense. To address this challenge, this paper proposes a semi-supervised continuous learning malware detection model based on Transformer. Firstly, this model improves the lifelong semi-supervised mixture algorithm to dynamically adjust the weighted combination of new sample sequences and historical ones to solve the imbalance problem. Secondly, the Learning with Local and Global Consistency algorithm is used to iteratively compute similarity scores for the unlabeled samples in the mixed samples to obtain pseudo-labels. Lastly, the Multilayer Perceptron is applied for malware classification. To validate the effectiveness of the model, this paper conducts experiments on the CICMalDroid2020 dataset. The experimental results show that the proposed model performs better than existing deep learning detection models. The F1 score has an average improvement of 1.27% compared to other models when conducting binary classification. And, after inputting hybrid samples, including historical data and new data, four times, the F1 score is still 1.96% higher than other models.",Medium
Amir Bidaki,Automatic,Springer,http://link.springer.com/article/10.1007/s13042-024-02486-9,ExBCIL: an exemplar-based class incremental learning for intrusion detection system,2024,Type,International Journal of Machine Learning and Cybernetics,Core,"Classical Network intrusion detection systems (N-IDSs) produce fairly accurate attack detection rates. However, these models operate in a static environment, which restricts the model’s update once they are deployed. The use of Continual Learning (CL) approaches provides a mechanism to accumulate old knowledge while continually adapting to the latest intrusions and better handling the dynamic environment. Replay-based continual learning proves to be a practical approach when working in a dynamic environment, and thus, our work extends the existing replay-based approach. However, the major challenge is to keep informative candidate vectors in episodic memory for replay purposes to mitigate catastrophic forgetting. It is essential to capture diverse samples for better class representation. In this work, we present a novel exemplar-based class incremental learning (ExBCIL) for intrusion detection, which updates the model in increments by keeping a small subset of past attack samples. However, it is challenging to select the most appropriate attack samples for the replay. While marking the importance of a sample, we consider both the discrimination and representation power of it. To improve the overall performance of learning and to detect emerging intrusions, we aim to keep a more diverse set of attack samples. We use the CICIDS2017, CICIDS2018, and UNSWNB-15 datasets to evaluate the model performance and compare the performance with other existing algorithms. The ExBCIL witnesses significant improvement in intrusion classification, with an average incremental accuracy of 95.6875±3.27%95.6875 \pm 3.27\%, an average score of 95.5±3.04%95.5 \pm 3.04\%, and a performance (accuracy) drop of 7.91%7.91\%.",Artificial Intelligence,0,,,,"Classical Network intrusion detection systems (N-IDSs) produce fairly accurate attack detection rates. However, these models operate in a static environment, which restricts the model’s update once they are deployed. The use of Continual Learning (CL) approaches provides a mechanism to accumulate old knowledge while continually adapting to the latest intrusions and better handling the dynamic environment. Replay-based continual learning proves to be a practical approach when working in a dynamic environment, and thus, our work extends the existing replay-based approach. However, the major challenge is to keep informative candidate vectors in episodic memory for replay purposes to mitigate catastrophic forgetting. It is essential to capture diverse samples for better class representation. In this work, we present a novel exemplar-based class incremental learning (ExBCIL) for intrusion detection, which updates the model in increments by keeping a small subset of past attack samples. However, it is challenging to select the most appropriate attack samples for the replay. While marking the importance of a sample, we consider both the discrimination and representation power of it. To improve the overall performance of learning and to detect emerging intrusions, we aim to keep a more diverse set of attack samples. We use the CICIDS2017, CICIDS2018, and UNSWNB-15 datasets to evaluate the model performance and compare the performance with other existing algorithms. The ExBCIL witnesses significant improvement in intrusion classification, with an average incremental accuracy of 95.6875±3.27%95.6875 \pm 3.27\%, an average score of 95.5±3.04%95.5 \pm 3.04\%, and a performance (accuracy) drop of 7.91%7.91\%.",High
Amir Bidaki,Automatic,Springer,http://link.springer.com/article/10.1007/s11416-024-00536-y,Experts still needed: boosting long-term android malware detection with active learning,2024,Type,Journal of Computer Virology and Hacking Techniques,Core,"The continuous evolution of cyber threats imposes a critical challenge to malware detection systems, so operational detection solutions in real-world settings must keep up-to-date malware knowledge databases. Machine learning-based solutions are not exempt from this requirement as handling concept drift constitutes the primary building block for keeping high detection performance in the long term. However, maintaining non-stationary malware detection models is highly demanding due to the high cost of labeling. This study applies several active learning-based approaches for maintaining a non-stationary model for Android malware detection in a 7-year-long time frame and conducts a comprehensive analysis to understand the impact of feature space selection, different data balancing techniques, and timestamping methods, utilized for locating the instances along the historical timeline, on the model’s detection performance over time. The detection accuracy and labeling costs are compared with various baselines. Additionally, the study investigates the resilience of such models against noisy labeling, a common problem in production environments due to unintentional expert errors and adversarial attacks. This research fills a significant gap in the literature by conducting a comprehensive analysis of active learning approaches to address concept drift in non-stationary settings established for mobile malware detection.","Mobile malware
Android
Concept drift
Active learning
Malware detection
Machine learning
Data labeling",0,,,,"The continuous evolution of cyber threats imposes a critical challenge to malware detection systems, so operational detection solutions in real-world settings must keep up-to-date malware knowledge databases. Machine learning-based solutions are not exempt from this requirement as handling concept drift constitutes the primary building block for keeping high detection performance in the long term. However, maintaining non-stationary malware detection models is highly demanding due to the high cost of labeling. This study applies several active learning-based approaches for maintaining a non-stationary model for Android malware detection in a 7-year-long time frame and conducts a comprehensive analysis to understand the impact of feature space selection, different data balancing techniques, and timestamping methods, utilized for locating the instances along the historical timeline, on the model’s detection performance over time. The detection accuracy and labeling costs are compared with various baselines. Additionally, the study investigates the resilience of such models against noisy labeling, a common problem in production environments due to unintentional expert errors and adversarial attacks. This research fills a significant gap in the literature by conducting a comprehensive analysis of active learning approaches to address concept drift in non-stationary settings established for mobile malware detection.",Medium
Amir Bidaki,Automatic,Springer,http://link.springer.com/article/10.1007/s10586-018-2835-2,Strategies for data stream mining method applied in anomaly detection,2019,Type,Cluster Computing,Core,"Anomaly detection, which is a method of intrusion detection, detects anomaly behaviors and protects network security. Data mining technology has been integrated to improve the performance of anomaly detection and some algorithms have been improved for anomaly detection field. We think that most data mining algorithms are analyzed on static data sets and ignore the influence of dynamic data streams. Data stream is the potentially unbounded, ordered sequence of data objects which arrive over time. The entire data objects cannot be stored and they need to be handled in one-time scanning. The data distribution of data stream may change over time and this phenomenon is called concept drift. The properties of data stream make analysis method different from the method based on data set and the analysis model is required to be updated immediately when concept drift occurs. In this paper, we summarize the characteristics of data stream, compare the difference between data stream and data set, discuss the problems of data stream mining and propose some corresponding strategies.","Anomaly detection
Data stream
Clustering
Concept drift",0,,,,"Anomaly detection, which is a method of intrusion detection, detects anomaly behaviors and protects network security. Data mining technology has been integrated to improve the performance of anomaly detection and some algorithms have been improved for anomaly detection field. We think that most data mining algorithms are analyzed on static data sets and ignore the influence of dynamic data streams. Data stream is the potentially unbounded, ordered sequence of data objects which arrive over time. The entire data objects cannot be stored and they need to be handled in one-time scanning. The data distribution of data stream may change over time and this phenomenon is called concept drift. The properties of data stream make analysis method different from the method based on data set and the analysis model is required to be updated immediately when concept drift occurs. In this paper, we summarize the characteristics of data stream, compare the difference between data stream and data set, discuss the problems of data stream mining and propose some corresponding strategies.",Medium
Amir Bidaki,Automatic,Springer,http://link.springer.com/article/10.1007/s00500-017-2708-2,Improved clustering algorithm based on high-speed network data stream,2018,Type,Soft Computing,Core,"With the continuous development of network technology, the attack has become the biggest threat to the stable operation of the network. Intrusion detection technology is a proactive safety protection measure which provides real-time monitoring of internal attacks, external attacks, and misuse. Traditional intrusion detection system is short of adaptability due to the complication and scale of the network. The main problem is that the real-time performance of the network is poor and the reliability is not high. This paper designs the intrusion detection mechanism combined with data stream clustering algorithm and intrusion detection system to solve the problem in processing a large amount of high-speed data streams. The performance of processing data streams is improved through the clustering algorithm based on density and the sliding window and the experiments show that the intrusion detection efficiency is higher than DenStream algorithm.",Artificial Intelligence,29 Citations,,,,"With the continuous development of network technology, the attack has become the biggest threat to the stable operation of the network. Intrusion detection technology is a proactive safety protection measure which provides real-time monitoring of internal attacks, external attacks, and misuse. Traditional intrusion detection system is short of adaptability due to the complication and scale of the network. The main problem is that the real-time performance of the network is poor and the reliability is not high. This paper designs the intrusion detection mechanism combined with data stream clustering algorithm and intrusion detection system to solve the problem in processing a large amount of high-speed data streams. The performance of processing data streams is improved through the clustering algorithm based on density and the sliding window and the experiments show that the intrusion detection efficiency is higher than DenStream algorithm.",Low
Amir Bidaki,Automatic,Springer,http://link.springer.com/article/10.1007/s12243-024-01046-0,A distributed platform for intrusion detection system using data stream mining in a big data environment,2024,Type,Annals of Telecommunications,Core,"With the growth of computer networks worldwide, there has been a greater need to protect local networks from malicious data that travel over the network. The increase in volume, speed, and variety of data requires a more robust, accurate intrusion detection system capable of analyzing a huge amount of data. This work proposes the creation of an intrusion detection system using stream classifiers and three classification layers—with and without a reduction in the number of features of the records and three classifiers in parallel with a voting system. The results obtained by the proposed system are compared against other models proposed in the literature, using two datasets to validate the proposed system. In all cases, gains in accuracy of up to 18.52% and 3.55% were obtained, using the datasets NSL-KDD and CICIDS2017, respectively. Reductions in classification time up to 35.51% and 94.90% were also obtained using the NSL-KDD and CICIDS2017 datasets, respectively.","Big data
Deep learning
Intrusion detection system
Machine learning
Real-time",0,,,,"With the growth of computer networks worldwide, there has been a greater need to protect local networks from malicious data that travel over the network. The increase in volume, speed, and variety of data requires a more robust, accurate intrusion detection system capable of analyzing a huge amount of data. This work proposes the creation of an intrusion detection system using stream classifiers and three classification layers—with and without a reduction in the number of features of the records and three classifiers in parallel with a voting system. The results obtained by the proposed system are compared against other models proposed in the literature, using two datasets to validate the proposed system. In all cases, gains in accuracy of up to 18.52% and 3.55% were obtained, using the datasets NSL-KDD and CICIDS2017, respectively. Reductions in classification time up to 35.51% and 94.90% were also obtained using the NSL-KDD and CICIDS2017 datasets, respectively.",Medium
Amir Bidaki,Automatic,Springer,http://link.springer.com/article/10.1007/s11432-020-3419-4,Communication-efficient federated continual learning for distributed learning system with Non-IID data,2022,Type,Science China Information Sciences,Core,"Due to the privacy preserving capabilities and the low communication costs, federated learning has emerged as an efficient technique for distributed deep learning/machine learning training. However, given the typical heterogeneous data distributions in the realistic scenario, federated learning faces the challenge of performance degradation on non-independent identically distributed (Non-IID) data across clients. Therefore, we propose federated continual learning to improve the performance on Non-IID data by introducing the knowledge of the other local models. Specifically, we propose a novel federated continual learning method called FedSI, adapting the synaptic intelligence method to the federated learning scenario. Furthermore, in order to reduce the communication overheads, we propose the bidirectional compression and error compensation (BCEC) algorithm to produce the communication-efficient federated continual learning method, called CFedSI. Specifically, the proposed BCEC algorithm compresses both the uplink and the downlink transmission data and utilizes the error compensation locally to ensure training divergence. Experiments show that CFedSI improves the accuracy on Non-IID data by up to 46% with KDDCUP’99 dataset, 23% with CICIDS2017 dataset, 22% with MNIST dataset, and 8% with FashionMNIST dataset, along with the reduced communication overheads.","distributed learning system
federated learning
continual learning
model compression
error compensation",21 Citations,,,,"Due to the privacy preserving capabilities and the low communication costs, federated learning has emerged as an efficient technique for distributed deep learning/machine learning training. However, given the typical heterogeneous data distributions in the realistic scenario, federated learning faces the challenge of performance degradation on non-independent identically distributed (Non-IID) data across clients. Therefore, we propose federated continual learning to improve the performance on Non-IID data by introducing the knowledge of the other local models. Specifically, we propose a novel federated continual learning method called FedSI, adapting the synaptic intelligence method to the federated learning scenario. Furthermore, in order to reduce the communication overheads, we propose the bidirectional compression and error compensation (BCEC) algorithm to produce the communication-efficient federated continual learning method, called CFedSI. Specifically, the proposed BCEC algorithm compresses both the uplink and the downlink transmission data and utilizes the error compensation locally to ensure training divergence. Experiments show that CFedSI improves the accuracy on Non-IID data by up to 46% with KDDCUP’99 dataset, 23% with CICIDS2017 dataset, 22% with MNIST dataset, and 8% with FashionMNIST dataset, along with the reduced communication overheads.",Medium
Amir Bidaki,Automatic,Springer,http://link.springer.com/article/10.1186/s40537-024-00957-y,Advancing cybersecurity: a comprehensive review of AI-driven detection techniques,2024,Type,Journal of Big Data,Core,"As the number and cleverness of cyber-attacks keep increasing rapidly, it's more important than ever to have good ways to detect and prevent them. Recognizing cyber threats quickly and accurately is crucial because they can cause severe damage to individuals and businesses. This paper takes a close look at how we can use artificial intelligence (AI), including machine learning (ML) and deep learning (DL), alongside metaheuristic algorithms to detect cyber-attacks better. We've thoroughly examined over sixty recent studies to measure how effective these AI tools are at identifying and fighting a wide range of cyber threats. Our research includes a diverse array of cyberattacks such as malware attacks, network intrusions, spam, and others, showing that ML and DL methods, together with metaheuristic algorithms, significantly improve how well we can find and respond to cyber threats. We compare these AI methods to find out what they're good at and where they could improve, especially as we face new and changing cyber-attacks. This paper presents a straightforward framework for assessing AI Methods in cyber threat detection. Given the increasing complexity of cyber threats, enhancing AI methods and regularly ensuring strong protection is critical. We evaluate the effectiveness and the limitations of current ML and DL proposed models, in addition to the metaheuristic algorithms. Recognizing these limitations is vital for guiding future enhancements. We're pushing for smart and flexible solutions that can adapt to new challenges. The findings from our research suggest that the future of protecting against cyber-attacks will rely on continuously updating AI methods to stay ahead of hackers' latest tricks.",Artificial Intelligence,0,,,,"As the number and cleverness of cyber-attacks keep increasing rapidly, it's more important than ever to have good ways to detect and prevent them. Recognizing cyber threats quickly and accurately is crucial because they can cause severe damage to individuals and businesses. This paper takes a close look at how we can use artificial intelligence (AI), including machine learning (ML) and deep learning (DL), alongside metaheuristic algorithms to detect cyber-attacks better. We've thoroughly examined over sixty recent studies to measure how effective these AI tools are at identifying and fighting a wide range of cyber threats. Our research includes a diverse array of cyberattacks such as malware attacks, network intrusions, spam, and others, showing that ML and DL methods, together with metaheuristic algorithms, significantly improve how well we can find and respond to cyber threats. We compare these AI methods to find out what they're good at and where they could improve, especially as we face new and changing cyber-attacks. This paper presents a straightforward framework for assessing AI Methods in cyber threat detection. Given the increasing complexity of cyber threats, enhancing AI methods and regularly ensuring strong protection is critical. We evaluate the effectiveness and the limitations of current ML and DL proposed models, in addition to the metaheuristic algorithms. Recognizing these limitations is vital for guiding future enhancements. We're pushing for smart and flexible solutions that can adapt to new challenges. The findings from our research suggest that the future of protecting against cyber-attacks will rely on continuously updating AI methods to stay ahead of hackers' latest tricks.",Medium
Amir Bidaki,Automatic,Springer,http://link.springer.com/article/10.1007/s11831-020-09496-0,"A Review on Machine Learning and Deep Learning Perspectives of IDS for IoT: Recent Updates, Security Issues, and Challenges",2021,Type,Archives of Computational Methods in Engineering,Core,"Internet of Things (IoT) is widely accepted technology in both industrial as well as academic field. The objective of IoT is to combine the physical environment with the cyber world and create one big intelligent network. This technology has been applied to various application domains such as developing smart home, smart cities, healthcare applications, wireless sensor networks, cloud environment, enterprise network, web applications, and smart grid technologies. These wide emerging applications in variety of domains raise many security issues such as protecting devices and network, attacks in IoT networks, and managing resource-constrained IoT networks. To address the scalability and resource-constrained security issues, many security solutions have been proposed for IoT such as web application firewalls and intrusion detection systems. In this paper, a comprehensive survey on Intrusion Detection System (IDS) for IoT is presented for years 2015–2019. We have discussed various IDS placement strategies and IDS analysis strategies in IoT architecture. The paper discusses various intrusions in IoT, along with Machine Learning (ML) and Deep Learning (DL) techniques for detecting attacks in IoT networks. The paper also discusses security issues and challenges in IoT.",Artificial Intelligence,0,,,,"Internet of Things (IoT) is widely accepted technology in both industrial as well as academic field. The objective of IoT is to combine the physical environment with the cyber world and create one big intelligent network. This technology has been applied to various application domains such as developing smart home, smart cities, healthcare applications, wireless sensor networks, cloud environment, enterprise network, web applications, and smart grid technologies. These wide emerging applications in variety of domains raise many security issues such as protecting devices and network, attacks in IoT networks, and managing resource-constrained IoT networks. To address the scalability and resource-constrained security issues, many security solutions have been proposed for IoT such as web application firewalls and intrusion detection systems. In this paper, a comprehensive survey on Intrusion Detection System (IDS) for IoT is presented for years 2015–2019. We have discussed various IDS placement strategies and IDS analysis strategies in IoT architecture. The paper discusses various intrusions in IoT, along with Machine Learning (ML) and Deep Learning (DL) techniques for detecting attacks in IoT networks. The paper also discusses security issues and challenges in IoT.",Low
Amir Bidaki,Automatic,Springer,http://link.springer.com/article/10.1007/s10462-024-10890-4,Explainable deep learning approach for advanced persistent threats (APTs) detection in cybersecurity: a review,2024,Type,Artificial Intelligence Review,Core,"In recent years, Advanced Persistent Threat (APT) attacks on network systems have increased through sophisticated fraud tactics. Traditional Intrusion Detection Systems (IDSs) suffer from low detection accuracy, high false-positive rates, and difficulty identifying unknown attacks such as remote-to-local (R2L) and user-to-root (U2R) attacks. This paper addresses these challenges by providing a foundational discussion of APTs and the limitations of existing detection methods. It then pivots to explore the novel integration of deep learning techniques and Explainable Artificial Intelligence (XAI) to improve APT detection. This paper aims to fill the gaps in the current research by providing a thorough analysis of how XAI methods, such as Shapley Additive Explanations (SHAP) and Local Interpretable Model-agnostic Explanations (LIME), can make black-box models more transparent and interpretable. The objective is to demonstrate the necessity of explainability in APT detection and propose solutions that enhance the trustworthiness and effectiveness of these models. It offers a critical analysis of existing approaches, highlights their strengths and limitations, and identifies open issues that require further research. This paper also suggests future research directions to combat evolving threats, paving the way for more effective and reliable cybersecurity solutions. Overall, this paper emphasizes the importance of explainability in enhancing the performance and trustworthiness of cybersecurity systems.",Artificial Intelligence,2 Citations,,,,"In recent years, Advanced Persistent Threat (APT) attacks on network systems have increased through sophisticated fraud tactics. Traditional Intrusion Detection Systems (IDSs) suffer from low detection accuracy, high false-positive rates, and difficulty identifying unknown attacks such as remote-to-local (R2L) and user-to-root (U2R) attacks. This paper addresses these challenges by providing a foundational discussion of APTs and the limitations of existing detection methods. It then pivots to explore the novel integration of deep learning techniques and Explainable Artificial Intelligence (XAI) to improve APT detection. This paper aims to fill the gaps in the current research by providing a thorough analysis of how XAI methods, such as Shapley Additive Explanations (SHAP) and Local Interpretable Model-agnostic Explanations (LIME), can make black-box models more transparent and interpretable. The objective is to demonstrate the necessity of explainability in APT detection and propose solutions that enhance the trustworthiness and effectiveness of these models. It offers a critical analysis of existing approaches, highlights their strengths and limitations, and identifies open issues that require further research. This paper also suggests future research directions to combat evolving threats, paving the way for more effective and reliable cybersecurity solutions. Overall, this paper emphasizes the importance of explainability in enhancing the performance and trustworthiness of cybersecurity systems.",Low
Amir Bidaki,Automatic,Springer,http://link.springer.com/article/10.1186/s42400-023-00199-0,A multi-agent adaptive deep learning framework for online intrusion detection,2024,Type,Cybersecurity,Core,"The network security analyzers use intrusion detection systems (IDSes) to distinguish malicious traffic from benign ones. The deep learning-based (DL-based) IDSes are proposed to auto-extract high-level features and eliminate the time-consuming and costly signature extraction process. However, this new generation of IDSes still needs to overcome a number of challenges to be employed in practical environments. One of the main issues of an applicable IDS is facing traffic concept drift, which manifests itself as new (i.e. , zero-day) attacks, in addition to the changing behavior of benign users/applications. Furthermore, a practical DL-based IDS needs to be conformed to a distributed (i.e. , multi-sensor) architecture in order to yield more accurate detections, create a collective attack knowledge based on the observations of different sensors, and also handle big data challenges for supporting high throughput networks. This paper proposes a novel multi-agent network intrusion detection framework to address the above shortcomings, considering a more practical scenario (i.e., online adaptable IDSes). This framework employs continual deep anomaly detectors for adapting each agent to the changing attack/benign patterns in its local traffic. In addition, a federated learning approach is proposed for sharing and exchanging local knowledge between different agents. Furthermore, the proposed framework implements sequential packet labeling for each flow, which provides an attack probability score for the flow by gradually observing each flow packet and updating its estimation. We evaluate the proposed framework by employing different deep models (including CNN-based and LSTM-based) over the CIC-IDS2017 and CSE-CIC-IDS2018 datasets. Through extensive evaluations and experiments, we show that the proposed distributed framework is well adapted to the traffic concept drift. More precisely, our results indicate that the CNN-based models are well suited for continually adapting to the traffic concept drift (i.e. , achieving an average detection rate of above 95% while needing just 128 new flows for the updating phase), and the LSTM-based models are a good candidate for sequential packet labeling in practical online IDSes (i.e. , detecting intrusions by just observing their first 15 packets).",Artificial Intelligence,4 Citations,,,,"The network security analyzers use intrusion detection systems (IDSes) to distinguish malicious traffic from benign ones. The deep learning-based (DL-based) IDSes are proposed to auto-extract high-level features and eliminate the time-consuming and costly signature extraction process. However, this new generation of IDSes still needs to overcome a number of challenges to be employed in practical environments. One of the main issues of an applicable IDS is facing traffic concept drift, which manifests itself as new (i.e. , zero-day) attacks, in addition to the changing behavior of benign users/applications. Furthermore, a practical DL-based IDS needs to be conformed to a distributed (i.e. , multi-sensor) architecture in order to yield more accurate detections, create a collective attack knowledge based on the observations of different sensors, and also handle big data challenges for supporting high throughput networks. This paper proposes a novel multi-agent network intrusion detection framework to address the above shortcomings, considering a more practical scenario (i.e., online adaptable IDSes). This framework employs continual deep anomaly detectors for adapting each agent to the changing attack/benign patterns in its local traffic. In addition, a federated learning approach is proposed for sharing and exchanging local knowledge between different agents. Furthermore, the proposed framework implements sequential packet labeling for each flow, which provides an attack probability score for the flow by gradually observing each flow packet and updating its estimation. We evaluate the proposed framework by employing different deep models (including CNN-based and LSTM-based) over the CIC-IDS2017 and CSE-CIC-IDS2018 datasets. Through extensive evaluations and experiments, we show that the proposed distributed framework is well adapted to the traffic concept drift. More precisely, our results indicate that the CNN-based models are well suited for continually adapting to the traffic concept drift (i.e. , achieving an average detection rate of above 95% while needing just 128 new flows for the updating phase), and the LSTM-based models are a good candidate for sequential packet labeling in practical online IDSes (i.e. , detecting intrusions by just observing their first 15 packets).",High
Amir Bidaki,Automatic,Springer,http://link.springer.com/article/10.1007/s10586-024-04424-4,Advances in deep learning intrusion detection over encrypted data with privacy preservation: a systematic review,2024,Type,Cluster Computing,Core,"Many sensitive applications require that data remain confidential and undisclosed, even for intrusion detection objectives. For this purpose, the detection of anomalies in encrypted data has become increasingly vital. Deep learning models are becoming good tools to detect anomalies in encrypted data without the need to pass through data decryption. This paper presents a systematic review focusing on the advancements made in deep learning models for intrusion detection over encrypted data with privacy preservation. This study aims to guide researchers on how to select the right tools to set up an intrusion detection system over encrypted data with privacy preservation. The study presented the context and challenges of intrusion detection on encrypted data and how machine learning-based solutions can circumvent these challenges. The paper looks at recently proposed solutions, examines metrics for assessing model performance, and evaluates frequently used reference datasets. Deep learning models are also evaluated with statistics on the most frequent models, datasets, and encryption tools. The performance metrics of the studied solutions are investigated as a function of the encryption tools, the deployed deep learning models, the privacy preservation tools, the deployed datasets, and the eventual additional tools and algorithms. Our recommendations help researchers evaluate their proposals for preserving privacy and detecting intrusions on encrypted data using deep learning techniques.","Deep learning
Dataset
Privacy preservation
Encryption
Evaluation metrics",1 Citation,,,,"Many sensitive applications require that data remain confidential and undisclosed, even for intrusion detection objectives. For this purpose, the detection of anomalies in encrypted data has become increasingly vital. Deep learning models are becoming good tools to detect anomalies in encrypted data without the need to pass through data decryption. This paper presents a systematic review focusing on the advancements made in deep learning models for intrusion detection over encrypted data with privacy preservation. This study aims to guide researchers on how to select the right tools to set up an intrusion detection system over encrypted data with privacy preservation. The study presented the context and challenges of intrusion detection on encrypted data and how machine learning-based solutions can circumvent these challenges. The paper looks at recently proposed solutions, examines metrics for assessing model performance, and evaluates frequently used reference datasets. Deep learning models are also evaluated with statistics on the most frequent models, datasets, and encryption tools. The performance metrics of the studied solutions are investigated as a function of the encryption tools, the deployed deep learning models, the privacy preservation tools, the deployed datasets, and the eventual additional tools and algorithms. Our recommendations help researchers evaluate their proposals for preserving privacy and detecting intrusions on encrypted data using deep learning techniques.",Low
Amir Bidaki,Automatic,Springer,http://link.springer.com/article/10.1007/s10462-021-10037-9,"A survey on intrusion detection system: feature selection, model, performance measures, application perspective, challenges, and future research directions",2022,Type,Artificial Intelligence Review,Core,"With the increase in the usage of the Internet, a large amount of information is exchanged between different communicating devices. The data should be communicated securely between the communicating devices and therefore, network security is one of the dominant research areas for the current network scenario. Intrusion detection systems (IDSs) are therefore widely used along with other security mechanisms such as firewall and access control. Many research ideas have been proposed pertaining to the IDS using machine learning (ML) techniques, deep learning (DL) techniques, and swarm and evolutionary algorithms (SWEVO). These methods have been tested on the datasets such as DARPA, KDD CUP 99, and NSL-KDD using network features to classify attack types. This paper surveys the intrusion detection problem by considering algorithms from areas such as ML, DL, and SWEVO. The survey is a representative research work carried out in the field of IDS from the year 2008 to 2020. The paper focuses on the methods that have incorporated feature selection in their models for performance evaluation. The paper also discusses the different datasets of IDS and a detailed description of recent dataset CIC IDS-2017. The paper presents applications of IDS with challenges and potential future research directions. The study presented, can serve as a pedestal for research communities and novice researchers in the field of network security for understanding and developing efficient IDS models.",Artificial Intelligence,135 Citations,,,,"With the increase in the usage of the Internet, a large amount of information is exchanged between different communicating devices. The data should be communicated securely between the communicating devices and therefore, network security is one of the dominant research areas for the current network scenario. Intrusion detection systems (IDSs) are therefore widely used along with other security mechanisms such as firewall and access control. Many research ideas have been proposed pertaining to the IDS using machine learning (ML) techniques, deep learning (DL) techniques, and swarm and evolutionary algorithms (SWEVO). These methods have been tested on the datasets such as DARPA, KDD CUP 99, and NSL-KDD using network features to classify attack types. This paper surveys the intrusion detection problem by considering algorithms from areas such as ML, DL, and SWEVO. The survey is a representative research work carried out in the field of IDS from the year 2008 to 2020. The paper focuses on the methods that have incorporated feature selection in their models for performance evaluation. The paper also discusses the different datasets of IDS and a detailed description of recent dataset CIC IDS-2017. The paper presents applications of IDS with challenges and potential future research directions. The study presented, can serve as a pedestal for research communities and novice researchers in the field of network security for understanding and developing efficient IDS models.",Low
Amir Bidaki,Automatic,Springer,http://link.springer.com/article/10.1186/s40537-020-00318-5,Cybersecurity data science: an overview from machine learning perspective,2020,Type,Journal of Big Data,Core,"In a computing context, cybersecurity is undergoing massive shifts in technology and its operations in recent days, and data science is driving the change. Extracting security incident patterns or insights from cybersecurity data and building corresponding data-driven model, is the key to make a security system automated and intelligent. To understand and analyze the actual phenomena with data, various scientific methods, machine learning techniques, processes, and systems are used, which is commonly known as data science. In this paper, we focus and briefly discuss on cybersecurity data science, where the data is being gathered from relevant cybersecurity sources, and the analytics complement the latest data-driven patterns for providing more effective security solutions. The concept of cybersecurity data science allows making the computing process more actionable and intelligent as compared to traditional ones in the domain of cybersecurity. We then discuss and summarize a number of associated research issues and future directions. Furthermore, we provide a machine learning based multi-layered framework for the purpose of cybersecurity modeling. Overall, our goal is not only to discuss cybersecurity data science and relevant methods but also to focus the applicability towards data-driven intelligent decision making for protecting the systems from cyber-attacks.","Cybersecurity
Machine learning
Data science
Decision making
Cyber-attack
Security modeling
Intrusion detection
Cyber threat intelligence",328 Citations,,,,"In a computing context, cybersecurity is undergoing massive shifts in technology and its operations in recent days, and data science is driving the change. Extracting security incident patterns or insights from cybersecurity data and building corresponding data-driven model, is the key to make a security system automated and intelligent. To understand and analyze the actual phenomena with data, various scientific methods, machine learning techniques, processes, and systems are used, which is commonly known as data science. In this paper, we focus and briefly discuss on cybersecurity data science, where the data is being gathered from relevant cybersecurity sources, and the analytics complement the latest data-driven patterns for providing more effective security solutions. The concept of cybersecurity data science allows making the computing process more actionable and intelligent as compared to traditional ones in the domain of cybersecurity. We then discuss and summarize a number of associated research issues and future directions. Furthermore, we provide a machine learning based multi-layered framework for the purpose of cybersecurity modeling. Overall, our goal is not only to discuss cybersecurity data science and relevant methods but also to focus the applicability towards data-driven intelligent decision making for protecting the systems from cyber-attacks.",Low
Amir Bidaki,Automatic,Springer,http://link.springer.com/article/10.1007/s10586-017-1066-2,Anomaly detection model based on data stream clustering,2019,Type,Cluster Computing,Core,"Intrusion detection provides important protection for network security and anomaly detection as a type of intrusion detection, which can recognize the pattern of normal behaviors and label the behaviors which departure from normal pattern as anomaly behaviors. The updating of network equipment and broadband speed makes the data mining object change from static data sets to dynamic data streams. We think that the traditional methods based on data set do not satisfy the needs of dynamic network environment. The network data stream is temporal and cannot be treated as static data set. The concept and distribution of data objects is variety in different time stamps and the changing is unpredictable. Therefore, we propose an improved data stream clustering algorithm and design the anomaly detection model according to the improved algorithm. The established model can be modified with the changing of data stream and detect anomaly behaviors in time.","Intrusion detection
Anomaly detection
Data stream
Clustering",0,,,,"Intrusion detection provides important protection for network security and anomaly detection as a type of intrusion detection, which can recognize the pattern of normal behaviors and label the behaviors which departure from normal pattern as anomaly behaviors. The updating of network equipment and broadband speed makes the data mining object change from static data sets to dynamic data streams. We think that the traditional methods based on data set do not satisfy the needs of dynamic network environment. The network data stream is temporal and cannot be treated as static data set. The concept and distribution of data objects is variety in different time stamps and the changing is unpredictable. Therefore, we propose an improved data stream clustering algorithm and design the anomaly detection model according to the improved algorithm. The established model can be modified with the changing of data stream and detect anomaly behaviors in time.",Medium
Amir Bidaki,Automatic,Springer,http://link.springer.com/article/10.1007/s41870-024-01727-y,Ensemble adaptive online machine learning in data stream: a case study in cyber intrusion detection system,2024,Type,International Journal of Information Technology,Core,"Adaptive online machine learning using data streams is an emerging research area in which algorithms learn dynamically from live data and update regularly for future predictions. On the contrary, traditional machine learning and deep learning approaches follow the batch learning paradigm, which requires the entire dataset to be available in memory for model training and testing purposes. The model is deployed only once with few updates. This approach is not feasible where data arrive in real-time and streaming manner (e.g., modern network traffic data stream, IoT and sensor data stream, etc.). This necessitates the need for the deployment of real-time and adaptive online machine learning systems. In this research study, we have proposed a case study of ensemble adaptive online machine learning in the cyber intrusion detection system (EnsAdp_CIDS) to detect and classify cyber intrusion in network traffic data. The proposed EnsAdp_CIDS algorithm learns instance by instance and fine-tunes its parameters on the fly. We have conducted extensive experimentation on the three benchmark cybersecurity datasets, namely, CICIDS-2017, CIC-IoT-2023 and CIC-MalMem-2022, to evaluate their performance in terms of accuracy, precision, recall and f1-score. The obtained results are compared with recent state-of-the-art machine learning and deep learning algorithms to show the novelty and efficiency of the proposed research work. The obtained accuracy over three cyber security datasets is 99.77%, 98.93% and 99.85%, respectively. The potential applicability of the proposed work is not limited to the cybersecurity domain; in fact, it can be further extended to other domains, such as video analytics and surveillance systems.",Artificial Intelligence,10 Citations,,,,"Adaptive online machine learning using data streams is an emerging research area in which algorithms learn dynamically from live data and update regularly for future predictions. On the contrary, traditional machine learning and deep learning approaches follow the batch learning paradigm, which requires the entire dataset to be available in memory for model training and testing purposes. The model is deployed only once with few updates. This approach is not feasible where data arrive in real-time and streaming manner (e.g., modern network traffic data stream, IoT and sensor data stream, etc.). This necessitates the need for the deployment of real-time and adaptive online machine learning systems. In this research study, we have proposed a case study of ensemble adaptive online machine learning in the cyber intrusion detection system (EnsAdp_CIDS) to detect and classify cyber intrusion in network traffic data. The proposed EnsAdp_CIDS algorithm learns instance by instance and fine-tunes its parameters on the fly. We have conducted extensive experimentation on the three benchmark cybersecurity datasets, namely, CICIDS-2017, CIC-IoT-2023 and CIC-MalMem-2022, to evaluate their performance in terms of accuracy, precision, recall and f1-score. The obtained results are compared with recent state-of-the-art machine learning and deep learning algorithms to show the novelty and efficiency of the proposed research work. The obtained accuracy over three cyber security datasets is 99.77%, 98.93% and 99.85%, respectively. The potential applicability of the proposed work is not limited to the cybersecurity domain; in fact, it can be further extended to other domains, such as video analytics and surveillance systems.",High
Amir Bidaki,Automatic,Springer,http://link.springer.com/article/10.1007/s10115-024-02219-y,ARP spoofing detection using machine learning classifiers: an experimental study,2024,Type,Knowledge and Information Systems,Core,"Recent university data breaches highlight the need to protect sensitive information and enhance centralized security systems like Software-Defined Networking and Intrusion Detection Systems by providing timely data for traffic analysis and attack detection. ARP spoofing, which can facilitate Man-in-the-Middle (MITM) attacks, is a key threat responsible for such breaches. Our work focuses on real-time anomaly detection within host-based systems to improve protection against ARP spoofing-based MITM attacks. The existing intrusion detection methods generally exhibit a gap where ML-based methods often overlook network metrics crucial for assessing real-world impact and system performance, while non-ML approaches struggle with adapting to new attack patterns. Our study introduces a dynamic ARP spoofing detection approach that addresses vulnerabilities in victim ARP caches by continuously updating references and verifying source IP and MAC addresses. The algorithm also cross-verifies gateway values to maintain accurate network integrity. Our research optimizes ML classifiers for ARP spoofing detection in host-based networks by selecting features based on expert insights and literature, utilizing a real-time dataset from our institute’s lab with 12 features, including 6 identified as optimal via PCA, to ensure accuracy and relevance in our specific network conditions. Additionally, we evaluated our models across various machine learning classifiers-including K-Nearest Neighbors, Decision Tree, Random Forest, Artificial Neural Network, Deep Neural Network, Convolutional Neural Network, and hybrid classifiers from continual learning approaches-achieving remarkable performance with 99% F1-Score and accuracy during training, and increased 99.26% F1-Score for real-time attack detection using CNN.","ARP spoofing
Network security
Machine learning
Real-time detection
Generative adversarial nets
Cybersecurity",0,,,,"Recent university data breaches highlight the need to protect sensitive information and enhance centralized security systems like Software-Defined Networking and Intrusion Detection Systems by providing timely data for traffic analysis and attack detection. ARP spoofing, which can facilitate Man-in-the-Middle (MITM) attacks, is a key threat responsible for such breaches. Our work focuses on real-time anomaly detection within host-based systems to improve protection against ARP spoofing-based MITM attacks. The existing intrusion detection methods generally exhibit a gap where ML-based methods often overlook network metrics crucial for assessing real-world impact and system performance, while non-ML approaches struggle with adapting to new attack patterns. Our study introduces a dynamic ARP spoofing detection approach that addresses vulnerabilities in victim ARP caches by continuously updating references and verifying source IP and MAC addresses. The algorithm also cross-verifies gateway values to maintain accurate network integrity. Our research optimizes ML classifiers for ARP spoofing detection in host-based networks by selecting features based on expert insights and literature, utilizing a real-time dataset from our institute’s lab with 12 features, including 6 identified as optimal via PCA, to ensure accuracy and relevance in our specific network conditions. Additionally, we evaluated our models across various machine learning classifiers-including K-Nearest Neighbors, Decision Tree, Random Forest, Artificial Neural Network, Deep Neural Network, Convolutional Neural Network, and hybrid classifiers from continual learning approaches-achieving remarkable performance with 99% F1-Score and accuracy during training, and increased 99.26% F1-Score for real-time attack detection using CNN.",Medium
Amir Bidaki,Automatic,Springer,http://link.springer.com/article/10.1007/s10462-024-11082-w,Advancements in securing federated learning with IDS: a comprehensive review of neural networks and feature engineering techniques for malicious client detection,2025,Type,Artificial Intelligence Review,Core,"Federated Learning (FL) is a technique that can learn a global machine-learning model at a central server by aggregating locally trained models. This distributed machine-learning approach preserves the privacy of local models. However, FL systems are inherently vulnerable to significant security challenges such as cyber-attacks, handling non-independent and identically distributed (non-IID) data, and data privacy concerns. This systematic literature review addresses these issues by examining advanced neural network models, feature engineering methods, and privacy-preserving techniques within intrusion detection systems (IDS) for FL environments. These are key elements for improving the security of FL systems. To the best of our knowledge, this review is among the first to comprehensively explore the combined impacts of these technologies. We analyzed 88 studies published between 2021 and October 2024. This study offers valuable insights for future research directions, including scaling FL in a real-world environment.",Artificial Intelligence,0,,,,"Federated Learning (FL) is a technique that can learn a global machine-learning model at a central server by aggregating locally trained models. This distributed machine-learning approach preserves the privacy of local models. However, FL systems are inherently vulnerable to significant security challenges such as cyber-attacks, handling non-independent and identically distributed (non-IID) data, and data privacy concerns. This systematic literature review addresses these issues by examining advanced neural network models, feature engineering methods, and privacy-preserving techniques within intrusion detection systems (IDS) for FL environments. These are key elements for improving the security of FL systems. To the best of our knowledge, this review is among the first to comprehensively explore the combined impacts of these technologies. We analyzed 88 studies published between 2021 and October 2024. This study offers valuable insights for future research directions, including scaling FL in a real-world environment.",Medium
Amir Bidaki,Automatic,Springer,http://link.springer.com/article/10.1007/s13369-022-07412-1,Intrusion Detection Systems: A State-of-the-Art Taxonomy and Survey,2023,Type,Arabian Journal for Science and Engineering,Core,"Intrusion Detection Systems (IDSs) have become essential to the sound operations of networks. These systems have the potential to identify and report deviations from normal behaviors, which is crucial for the sustainability and resilience of networks. A large amount of IDSs have been proposed in the literature, but only few of them found success in real-world environments. This study illustrates a taxonomy and a survey on state-of-the-art intrusion detection systems. It also depicts the characteristics of successful IDSs and sheds light on the gaps that need to be resolved for future IDSs to become fit for deployment in realistic environments.",CRISPR-Cas systems,14 Citations,,,,"Intrusion Detection Systems (IDSs) have become essential to the sound operations of networks. These systems have the potential to identify and report deviations from normal behaviors, which is crucial for the sustainability and resilience of networks. A large amount of IDSs have been proposed in the literature, but only few of them found success in real-world environments. This study illustrates a taxonomy and a survey on state-of-the-art intrusion detection systems. It also depicts the characteristics of successful IDSs and sheds light on the gaps that need to be resolved for future IDSs to become fit for deployment in realistic environments.",Low
Amir Bidaki,Automatic,Springer,http://link.springer.com/article/10.1007/s10207-024-00959-0,Artificial intelligence for system security assurance: A systematic literature review,2024,Type,International Journal of Information Security,Core,"System Security Assurance (SSA) has emerged as a critical methodology for organizations to verify the trustworthiness of their systems by evaluating security measures against industry standards, legal requirements, and best practices to identify any weakness and demonstrate compliance. In recent years, the role of Artificial Intelligence (AI) in enhancing cybersecurity has received increased attention, with an increasing number of literature reviews highlighting its diverse applications. However, there remains a significant gap in comprehensive reviews that specifically address the integration of AI within SSA frameworks. This systematic literature review seeks to fill this research gap by assessing the current state of AI in SSA, identifying key areas where AI contributes to improve SSA processes, highlighting the limitations of current methodologies, and providing the guidance for future advancements in the field of AI-driven SSA.",Artificial Intelligence,0,,,,"System Security Assurance (SSA) has emerged as a critical methodology for organizations to verify the trustworthiness of their systems by evaluating security measures against industry standards, legal requirements, and best practices to identify any weakness and demonstrate compliance. In recent years, the role of Artificial Intelligence (AI) in enhancing cybersecurity has received increased attention, with an increasing number of literature reviews highlighting its diverse applications. However, there remains a significant gap in comprehensive reviews that specifically address the integration of AI within SSA frameworks. This systematic literature review seeks to fill this research gap by assessing the current state of AI in SSA, identifying key areas where AI contributes to improve SSA processes, highlighting the limitations of current methodologies, and providing the guidance for future advancements in the field of AI-driven SSA.",Low
Amir Bidaki,Automatic,Springer,http://link.springer.com/article/10.1007/s10462-022-10143-2,"The role of artificial intelligence and machine learning in wireless networks security: principle, practice and challenges",2022,Type,Artificial Intelligence Review,Core,"Security is one of the biggest challenges concerning networks and communications. The problem becomes aggravated with the proliferation of wireless devices. Artificial Intelligence (AI) has emerged as a promising solution and a volume of literature exists on the methodological studies of AI to resolve the security challenge. In this survey, we present a taxonomy of security threats and review distinct aspects and the potential of AI to resolve the challenge. To the best of our knowledge, this is the first comprehensive survey to review the AI solutions for all possible security types and threats. We also present the lessons learned from the existing AI techniques and contributions of up-to-date literature, future directions of AI in security, open issues that need to be investigated further through AI, and discuss how AI can be more effectively used to overcome the upcoming advanced security threats.",Artificial Intelligence,0,,,,"Security is one of the biggest challenges concerning networks and communications. The problem becomes aggravated with the proliferation of wireless devices. Artificial Intelligence (AI) has emerged as a promising solution and a volume of literature exists on the methodological studies of AI to resolve the security challenge. In this survey, we present a taxonomy of security threats and review distinct aspects and the potential of AI to resolve the challenge. To the best of our knowledge, this is the first comprehensive survey to review the AI solutions for all possible security types and threats. We also present the lessons learned from the existing AI techniques and contributions of up-to-date literature, future directions of AI in security, open issues that need to be investigated further through AI, and discuss how AI can be more effectively used to overcome the upcoming advanced security threats.",Low
Amir Bidaki,Automatic,Springer,http://link.springer.com/article/10.1007/s12650-018-0525-z,A real-time network security visualization system based on incremental learning (ChinaVis 2018),2019,Type,Journal of Visualization,Core,"The real-time analysis of network data is of great significance to network security. Visualization technology and machine learning can assist in network data analysis from different aspects. However, there is little research regarding combining these two methods to process real-time network data. This paper proposes a novel real-time network security system. Combining unsupervised learning and visualization technology, it can identify network behavior patterns and provide a visualization module to adjust models interactively. The system is primarily divided into three parts. In the feature extraction part, we train a deep auto-encoder to compress the feature dimension. In the behavior pattern recognition part, normal and abnormal pattern SOINNs are trained incrementally. In visualization part, analysts can use multiple views to judge recognition results rapidly and adjust models so that the identification accuracy can be increased. We use the data in VAST Challenge 2013 to show that our system can identify network behavior patterns in real time and find the correlations between them.","Real-time analysis
Network security visualization
Machine learning
Incremental learning
Pattern recognition",0,,,,"The real-time analysis of network data is of great significance to network security. Visualization technology and machine learning can assist in network data analysis from different aspects. However, there is little research regarding combining these two methods to process real-time network data. This paper proposes a novel real-time network security system. Combining unsupervised learning and visualization technology, it can identify network behavior patterns and provide a visualization module to adjust models interactively. The system is primarily divided into three parts. In the feature extraction part, we train a deep auto-encoder to compress the feature dimension. In the behavior pattern recognition part, normal and abnormal pattern SOINNs are trained incrementally. In visualization part, analysts can use multiple views to judge recognition results rapidly and adjust models so that the identification accuracy can be increased. We use the data in VAST Challenge 2013 to show that our system can identify network behavior patterns in real time and find the correlations between them.",Medium
Amir Bidaki,Automatic,Springer,http://link.springer.com/article/10.1007/s11235-018-0475-8,A comprehensive survey on network anomaly detection,2019,Type,Telecommunication Systems,Core,"Nowadays, there is a huge and growing concern about security in information and communication technology among the scientific community because any attack or anomaly in the network can greatly affect many domains such as national security, private data storage, social welfare, economic issues, and so on. Therefore, the anomaly detection domain is a broad research area, and many different techniques and approaches for this purpose have emerged through the years. In this study, the main objective is to review the most important aspects pertaining to anomaly detection, covering an overview of a background analysis as well as a core study on the most relevant techniques, methods, and systems within the area. Therefore, in order to ease the understanding of this survey’s structure, the anomaly detection domain was reviewed under five dimensions: (1) network traffic anomalies, (2) network data types, (3) intrusion detection systems categories, (4) detection methods and systems, and (5) open issues. The paper concludes with an open issues summary discussing presently unsolved problems, and final remarks.",Artificial Intelligence,0,,,,"Nowadays, there is a huge and growing concern about security in information and communication technology among the scientific community because any attack or anomaly in the network can greatly affect many domains such as national security, private data storage, social welfare, economic issues, and so on. Therefore, the anomaly detection domain is a broad research area, and many different techniques and approaches for this purpose have emerged through the years. In this study, the main objective is to review the most important aspects pertaining to anomaly detection, covering an overview of a background analysis as well as a core study on the most relevant techniques, methods, and systems within the area. Therefore, in order to ease the understanding of this survey’s structure, the anomaly detection domain was reviewed under five dimensions: (1) network traffic anomalies, (2) network data types, (3) intrusion detection systems categories, (4) detection methods and systems, and (5) open issues. The paper concludes with an open issues summary discussing presently unsolved problems, and final remarks.",Low
Amir Bidaki,Automatic,Springer,http://link.springer.com/article/10.1007/s10462-023-10437-z,Zero-day attack detection: a systematic literature review,2023,Type,Artificial Intelligence Review,Core,"With the continuous increase in cyberattacks over the past few decades, the quest to develop a comprehensive, robust, and effective intrusion detection system (IDS) in the research community has gained traction. Many of the recently proposed solutions lack a holistic IDS approach due to explicitly relying on attack signature repositories, outdated datasets or the lack of considering zero-day (unknown) attacks while developing, training, or testing the machine learning (ML) or deep learning (DL)-based models. Overlooking these factors makes the proposed IDS less robust or practical in real-time environments. On the other hand, detecting zero-day attacks is a challenging subject, despite the many solutions proposed over the past many years. One of the goals of this systematic literature review (SLR) is to provide a research asset to future researchers on various methodologies, techniques, ML and DL algorithms that researchers used for the detection of zero-day attacks. The extensive literature review on the recent publications reveals exciting future research trends and challenges in this particular field. With all the advances in technology, the availability of large datasets, and the strong processing capabilities of DL algorithms, detecting a completely new or unknown attack remains an open research area. This SLR is an effort towards completing the gap in providing a single repository of finding ML and DL-based tools and techniques used by researchers for the detection of zero-day attacks.",Artificial Intelligence,0,,,,"With the continuous increase in cyberattacks over the past few decades, the quest to develop a comprehensive, robust, and effective intrusion detection system (IDS) in the research community has gained traction. Many of the recently proposed solutions lack a holistic IDS approach due to explicitly relying on attack signature repositories, outdated datasets or the lack of considering zero-day (unknown) attacks while developing, training, or testing the machine learning (ML) or deep learning (DL)-based models. Overlooking these factors makes the proposed IDS less robust or practical in real-time environments. On the other hand, detecting zero-day attacks is a challenging subject, despite the many solutions proposed over the past many years. One of the goals of this systematic literature review (SLR) is to provide a research asset to future researchers on various methodologies, techniques, ML and DL algorithms that researchers used for the detection of zero-day attacks. The extensive literature review on the recent publications reveals exciting future research trends and challenges in this particular field. With all the advances in technology, the availability of large datasets, and the strong processing capabilities of DL algorithms, detecting a completely new or unknown attack remains an open research area. This SLR is an effort towards completing the gap in providing a single repository of finding ML and DL-based tools and techniques used by researchers for the detection of zero-day attacks.",Low
Amir Bidaki,Automatic,Springer,http://link.springer.com/article/10.1007/s00521-024-10437-2,"Unlocking the black box: an in-depth review on interpretability, explainability, and reliability in deep learning",2024,Type,Neural Computing and Applications,Core,"Deep learning models have revolutionized numerous fields, yet their decision-making processes often remain opaque, earning them the characterization of “black-box” models due to their lack of transparency and comprehensibility. This opacity presents significant challenges to understanding the rationale behind their decisions, thereby impeding their interpretability, explainability, and reliability. This review examines 718 studies published between 2015 and 2024 in high-impact journals indexed in SCI, SCI-E, SSCI, and ESCI, providing a crucial reference for researchers investigating methodologies and techniques in related domains. In this exploration, we evaluate a wide array of interpretability and explainability (XAI) strategies, including visual and feature-based explanations, local approach-based techniques, and Bayesian methods. These strategies are assessed for their effectiveness and applicability using a comprehensive set of evaluation metrics. Moving beyond traditional analyses, we propose a novel taxonomy of XAI methods, addressing gaps in the literature and offering a structured classification that elucidates the roles and interactions of these methods. Moreover, we explore the intricate relationship between interpretability and explainability, examining potential conflicts and highlighting the necessity for interpretability in practical applications. Through detailed comparative analysis, we underscore the strengths and limitations of various XAI methods across different data types, ensuring a thorough understanding of their practical performance and real-world utility. The review also examines model robustness against adversarial attacks, emphasizing the critical importance of transparency, reliability, and ethical considerations in model development. A significant emphasis is placed on identifying and mitigating biases in deep learning systems, providing insights into future research directions that aim to enhance fairness and reduce bias. By thoroughly reviewing current challenges and emerging research directions, this article equips researchers with the knowledge and tools to advance the development of more transparent, fair, and reliable deep learning systems. Ultimately, this work aims to bridge existing literature gaps by offering a forward-looking perspective that fosters innovation and progress in the field. This comprehensive review not only illuminates the current state of XAI methodologies but also contributes to the broader understanding and enhancement of deep learning systems, ensuring their ethical and equitable application across various domains.",Artificial Intelligence,0,,,,"Deep learning models have revolutionized numerous fields, yet their decision-making processes often remain opaque, earning them the characterization of “black-box” models due to their lack of transparency and comprehensibility. This opacity presents significant challenges to understanding the rationale behind their decisions, thereby impeding their interpretability, explainability, and reliability. This review examines 718 studies published between 2015 and 2024 in high-impact journals indexed in SCI, SCI-E, SSCI, and ESCI, providing a crucial reference for researchers investigating methodologies and techniques in related domains. In this exploration, we evaluate a wide array of interpretability and explainability (XAI) strategies, including visual and feature-based explanations, local approach-based techniques, and Bayesian methods. These strategies are assessed for their effectiveness and applicability using a comprehensive set of evaluation metrics. Moving beyond traditional analyses, we propose a novel taxonomy of XAI methods, addressing gaps in the literature and offering a structured classification that elucidates the roles and interactions of these methods. Moreover, we explore the intricate relationship between interpretability and explainability, examining potential conflicts and highlighting the necessity for interpretability in practical applications. Through detailed comparative analysis, we underscore the strengths and limitations of various XAI methods across different data types, ensuring a thorough understanding of their practical performance and real-world utility. The review also examines model robustness against adversarial attacks, emphasizing the critical importance of transparency, reliability, and ethical considerations in model development. A significant emphasis is placed on identifying and mitigating biases in deep learning systems, providing insights into future research directions that aim to enhance fairness and reduce bias. By thoroughly reviewing current challenges and emerging research directions, this article equips researchers with the knowledge and tools to advance the development of more transparent, fair, and reliable deep learning systems. Ultimately, this work aims to bridge existing literature gaps by offering a forward-looking perspective that fosters innovation and progress in the field. This comprehensive review not only illuminates the current state of XAI methodologies but also contributes to the broader understanding and enhancement of deep learning systems, ensuring their ethical and equitable application across various domains.",Low
Amir Bidaki,Automatic,Springer,http://link.springer.com/article/10.1007/s10207-023-00682-2,A systematic literature review for network intrusion detection system (IDS),2023,Type,International Journal of Information Security,Core,"With the recent increase in internet usage, the number of important, sensitive, confidential individual and corporate data passing through internet has increasingly grown. With gaps in the security systems, attackers have attempted to intrude the network, thereby gaining access to essential and confidential information, which may cause harm to the operation of the systems, and also affect the confidentiality of the data. To counter these possible attacks, intrusion detection systems (IDSs), which is an essential branch of cybersecurity, were employed to monitor and analyze network traffic thereby detects and reports malicious activities. A large number of review papers have covered different approaches for intrusion detection in networks, most of which follow a non-systematic approach, merely made a comparison of the existing techniques without reflecting an in-depth analytical synthesis of the methodologies and performances of the approaches to give a complete understanding of the state of IDS. Nonetheless, many of these reviews investigated more about the anomaly-based IDS with more emphasis on deep-learning models, while signature, hybrid-based (signature + anomaly-based) have received minimal focus. Hence, by adhering to the principles of Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA), this work reviewed existing contributions on anomaly-, signature-, and hybrid-based approaches to provide a comprehensive overview of network IDS's state of the art. The articles were retrieved from seven databases (ScienceDirect, SpringerNature, IEEE, MDPI, Hindawi, PeerJ, and Taylor & Francis) which cut across various reputable journals and conference Proceedings. Among the 776 pieces of the literature identified, 71 were selected for analysis and synthesis to answer the research questions. Based on the research findings, we identified unexplored study areas and unresolved research challenges. In order to create a better IDS model, we conclude by presenting promising, high-impact future research areas.","Network intrusion detection system
Signature-based
Anomaly-based
Artificial intelligence
Rule-based
Pattern matching",0,,,,"With the recent increase in internet usage, the number of important, sensitive, confidential individual and corporate data passing through internet has increasingly grown. With gaps in the security systems, attackers have attempted to intrude the network, thereby gaining access to essential and confidential information, which may cause harm to the operation of the systems, and also affect the confidentiality of the data. To counter these possible attacks, intrusion detection systems (IDSs), which is an essential branch of cybersecurity, were employed to monitor and analyze network traffic thereby detects and reports malicious activities. A large number of review papers have covered different approaches for intrusion detection in networks, most of which follow a non-systematic approach, merely made a comparison of the existing techniques without reflecting an in-depth analytical synthesis of the methodologies and performances of the approaches to give a complete understanding of the state of IDS. Nonetheless, many of these reviews investigated more about the anomaly-based IDS with more emphasis on deep-learning models, while signature, hybrid-based (signature + anomaly-based) have received minimal focus. Hence, by adhering to the principles of Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA), this work reviewed existing contributions on anomaly-, signature-, and hybrid-based approaches to provide a comprehensive overview of network IDS's state of the art. The articles were retrieved from seven databases (ScienceDirect, SpringerNature, IEEE, MDPI, Hindawi, PeerJ, and Taylor & Francis) which cut across various reputable journals and conference Proceedings. Among the 776 pieces of the literature identified, 71 were selected for analysis and synthesis to answer the research questions. Based on the research findings, we identified unexplored study areas and unresolved research challenges. In order to create a better IDS model, we conclude by presenting promising, high-impact future research areas.",Low
Amir Bidaki,Automatic,Springer,http://link.springer.com/article/10.1007/s13042-020-01264-7,"A scalable network intrusion detection system towards detecting, discovering, and learning unknown attacks",2021,Type,International Journal of Machine Learning and Cybernetics,Core,"Network intrusion detection systems (IDSs) based on deep learning have reached fairly accurate attack detection rates. But these deep learning approaches usually have been performed in a closed-set protocol that only known classes appear in training are considered during classification, the existing IDSs will fail to detect the unknown attacks and misclassify them as the training known classes, hence are not scalable. Furthermore, these IDSs are not efficient for updating the deep detection model once new attacks are discovered. To address those problems, we propose a scalable IDS towards detecting, discovering, and learning unknown attacks, it has three components. Firstly, we propose the open-set classification network (OCN) to detect unknown attacks, OCN based on the convolutional neural network adopts the nearest class mean (NCM) classifier, two new loss are designed to jointly optimize it, including Fisher loss and maximum mean discrepancy (MMD) loss. Subsequently, the semantic embedding clustering method is proposed to discover the hidden unknown attacks from all unknown instances detected by OCN. Then we propose the incremental nearest cluster centroid (INCC) method for learning the discovered unknown attacks through updating the NCM classifier. Extensive experiments on KDDCUP’99 dataset and CICIDS2017 dataset indicate that our OCN outperforms the state-of-the-art comparison methods in detecting multiple types of unknown attacks. Our experiments also verify the feasibility of the semantic embedding clustering method and INCC in discovering and learning unknown attacks.",Artificial Intelligence,37 Citations,,,,"Network intrusion detection systems (IDSs) based on deep learning have reached fairly accurate attack detection rates. But these deep learning approaches usually have been performed in a closed-set protocol that only known classes appear in training are considered during classification, the existing IDSs will fail to detect the unknown attacks and misclassify them as the training known classes, hence are not scalable. Furthermore, these IDSs are not efficient for updating the deep detection model once new attacks are discovered. To address those problems, we propose a scalable IDS towards detecting, discovering, and learning unknown attacks, it has three components. Firstly, we propose the open-set classification network (OCN) to detect unknown attacks, OCN based on the convolutional neural network adopts the nearest class mean (NCM) classifier, two new loss are designed to jointly optimize it, including Fisher loss and maximum mean discrepancy (MMD) loss. Subsequently, the semantic embedding clustering method is proposed to discover the hidden unknown attacks from all unknown instances detected by OCN. Then we propose the incremental nearest cluster centroid (INCC) method for learning the discovered unknown attacks through updating the NCM classifier. Extensive experiments on KDDCUP’99 dataset and CICIDS2017 dataset indicate that our OCN outperforms the state-of-the-art comparison methods in detecting multiple types of unknown attacks. Our experiments also verify the feasibility of the semantic embedding clustering method and INCC in discovering and learning unknown attacks.",Medium
Amir Bidaki,Automatic,Springer,http://link.springer.com/article/10.1007/s10462-024-11055-z,Staying ahead of phishers: a review of recent advances and emerging methodologies in phishing detection,2024,Type,Artificial Intelligence Review,Core,"The escalating threat of phishing attacks poses significant challenges to cybersecurity, necessitating innovative approaches for detection and mitigation. This paper addresses this need by presenting a comprehensive review of state-of-the-art methodologies for phishing detection, spanning traditional machine learning techniques to cutting-edge deep learning frameworks. The review encompasses a diverse range of methods, including list-based approaches, machine learning algorithms, graph-based analysis, deep learning models, network embedding techniques, and generative adversarial networks (GANs). Each method is meticulously scrutinized, highlighting its rationale, advantages, and empirical results. For instance, deep learning models, particularly convolutional neural networks (CNNs) and recurrent neural networks (RNNs), demonstrate superior detection performance, leveraging their ability to extract complex patterns from phishing data. Ensemble learning techniques and GANs offer additional benefits by enhancing detection accuracy and resilience against adversarial attacks. The impact of this review extends beyond academic discourse, informing practitioners and policymakers about the evolving landscape of phishing detection. By elucidating the strengths and limitations of existing methods, this paper guides the development of more robust and effective cybersecurity solutions. Moreover, the insights gleaned from this review lay the groundwork for future research endeavors, such as integrating contextual information, user behavior analysis, and explainable AI techniques into phishing detection systems. Ultimately, this work contributes to the collective effort to fortify digital defenses against sophisticated phishing threats, safeguarding the integrity of online ecosystems.",Artificial Intelligence,0,,,,"The escalating threat of phishing attacks poses significant challenges to cybersecurity, necessitating innovative approaches for detection and mitigation. This paper addresses this need by presenting a comprehensive review of state-of-the-art methodologies for phishing detection, spanning traditional machine learning techniques to cutting-edge deep learning frameworks. The review encompasses a diverse range of methods, including list-based approaches, machine learning algorithms, graph-based analysis, deep learning models, network embedding techniques, and generative adversarial networks (GANs). Each method is meticulously scrutinized, highlighting its rationale, advantages, and empirical results. For instance, deep learning models, particularly convolutional neural networks (CNNs) and recurrent neural networks (RNNs), demonstrate superior detection performance, leveraging their ability to extract complex patterns from phishing data. Ensemble learning techniques and GANs offer additional benefits by enhancing detection accuracy and resilience against adversarial attacks. The impact of this review extends beyond academic discourse, informing practitioners and policymakers about the evolving landscape of phishing detection. By elucidating the strengths and limitations of existing methods, this paper guides the development of more robust and effective cybersecurity solutions. Moreover, the insights gleaned from this review lay the groundwork for future research endeavors, such as integrating contextual information, user behavior analysis, and explainable AI techniques into phishing detection systems. Ultimately, this work contributes to the collective effort to fortify digital defenses against sophisticated phishing threats, safeguarding the integrity of online ecosystems.",Low
Amir Bidaki,Automatic,Springer,http://link.springer.com/article/10.1007/s11227-023-05403-z,Intrusion detection of manifold regularized broad learning system based on LU decomposition,2023,Type,The Journal of Supercomputing,Core,"Broad Learning System (BLS) is proposed as an alternative to deep learning. It has a fast adaptive model selection and online incremental learning capability, which has been successfully applied in many fields. In this paper, the BLS model is introduced into intrusion detection, and considering the weakness of the BLS model in mining the internal structural information of samples, this paper proposes a Manifold Regularized Broad Learning System based on LU decomposition (LU-MRBLS) intrusion detection. Based on the manifold hypothesis, the LU-MRBLS model firstly constructs the graph Laplacian operator in the data input space to mine the potential information of the data. Then, under the manifold regularized framework, the feature nodes, enhancement nodes, and Laplacian matrix are combined to construct the objective function to regularize and optimize the BLS model to avoid the model falling into local optimization. Finally, the LU decomposition method is used to solve the output weight matrix of the MRBLS model, shorten the training time of the MRBLS model, avoid singular value problems of the solution process, and improve the intrusion detection performance of the model. In this paper, we use the KDD Cup99 dataset for parameter selection and apply it to other network models. Through rigorous experiments, the LU-MRBLS model is applied to KDD Cup99, NSL-KDD, UNSW-NB15, and CIDDS-001 datasets with better detection results than the classical machine learning models and the latest intrusion detection models.","Intrusion detection
Broad learning system
Machine learning
Manifold regularized
LU decomposition",0,,,,"Broad Learning System (BLS) is proposed as an alternative to deep learning. It has a fast adaptive model selection and online incremental learning capability, which has been successfully applied in many fields. In this paper, the BLS model is introduced into intrusion detection, and considering the weakness of the BLS model in mining the internal structural information of samples, this paper proposes a Manifold Regularized Broad Learning System based on LU decomposition (LU-MRBLS) intrusion detection. Based on the manifold hypothesis, the LU-MRBLS model firstly constructs the graph Laplacian operator in the data input space to mine the potential information of the data. Then, under the manifold regularized framework, the feature nodes, enhancement nodes, and Laplacian matrix are combined to construct the objective function to regularize and optimize the BLS model to avoid the model falling into local optimization. Finally, the LU decomposition method is used to solve the output weight matrix of the MRBLS model, shorten the training time of the MRBLS model, avoid singular value problems of the solution process, and improve the intrusion detection performance of the model. In this paper, we use the KDD Cup99 dataset for parameter selection and apply it to other network models. Through rigorous experiments, the LU-MRBLS model is applied to KDD Cup99, NSL-KDD, UNSW-NB15, and CIDDS-001 datasets with better detection results than the classical machine learning models and the latest intrusion detection models.",Medium
Amir Bidaki,Automatic,Springer,http://link.springer.com/article/10.1007/s00500-020-05373-x,Improving security using SVM-based anomaly detection: issues and challenges,2021,Type,Soft Computing,Core,"Security is one of the main requirements of the current computer systems, and recently it gains much importance as the number and severity of malicious attacks increase dramatically. Anomaly detection is one of the main branches of the intrusion detection systems which enables to recognize the newer variants of the security attacks. This paper focuses on the anomaly detection schemes (ADS), which have applied support vector machine (SVM) for detecting intrusions and security attacks. For this purpose, it first presents the required concepts about the SVM classifier and intrusion detection systems. It then classifies the ADS approaches and discusses the various machine learning and artificial intelligence techniques that have been applied in combination with the SVM classifier to detect anomalies. Besides, it specifies the primary capabilities, possible limitations, or advantages of the ADS approaches. Furthermore, a comparison of the studied ADS schemes is provided to illuminate their various technical details.",Artificial Intelligence,55 Citations,,,,"Security is one of the main requirements of the current computer systems, and recently it gains much importance as the number and severity of malicious attacks increase dramatically. Anomaly detection is one of the main branches of the intrusion detection systems which enables to recognize the newer variants of the security attacks. This paper focuses on the anomaly detection schemes (ADS), which have applied support vector machine (SVM) for detecting intrusions and security attacks. For this purpose, it first presents the required concepts about the SVM classifier and intrusion detection systems. It then classifies the ADS approaches and discusses the various machine learning and artificial intelligence techniques that have been applied in combination with the SVM classifier to detect anomalies. Besides, it specifies the primary capabilities, possible limitations, or advantages of the ADS approaches. Furthermore, a comparison of the studied ADS schemes is provided to illuminate their various technical details.",Low
Amir Bidaki,Automatic,Springer,http://link.springer.com/article/10.1186/s40537-021-00514-x,A literature review on one-class classification and its potential applications in big data,2021,Type,Journal of Big Data,Core,"In severely imbalanced datasets, using traditional binary or multi-class classification typically leads to bias towards the class(es) with the much larger number of instances. Under such conditions, modeling and detecting instances of the minority class is very difficult. One-class classification (OCC) is an approach to detect abnormal data points compared to the instances of the known class and can serve to address issues related to severely imbalanced datasets, which are especially very common in big data. We present a detailed survey of OCC-related literature works published over the last decade, approximately. We group the different works into three categories: outlier detection, novelty detection, and deep learning and OCC. We closely examine and evaluate selected works on OCC such that a good cross section of approaches, methods, and application domains is represented in the survey. Commonly used techniques in OCC for outlier detection and for novelty detection, respectively, are discussed. We observed one area that has been largely omitted in OCC-related literature is its application context for big data and its inherently associated problems, such as severe class imbalance, class rarity, noisy data, feature selection, and data reduction. We feel the survey will be appreciated by researchers working in these areas of big data.","One-class classification
Big data
Outlier detection
Novelty detection
Deep learning
Class imbalance",111 Citations,,,,"In severely imbalanced datasets, using traditional binary or multi-class classification typically leads to bias towards the class(es) with the much larger number of instances. Under such conditions, modeling and detecting instances of the minority class is very difficult. One-class classification (OCC) is an approach to detect abnormal data points compared to the instances of the known class and can serve to address issues related to severely imbalanced datasets, which are especially very common in big data. We present a detailed survey of OCC-related literature works published over the last decade, approximately. We group the different works into three categories: outlier detection, novelty detection, and deep learning and OCC. We closely examine and evaluate selected works on OCC such that a good cross section of approaches, methods, and application domains is represented in the survey. Commonly used techniques in OCC for outlier detection and for novelty detection, respectively, are discussed. We observed one area that has been largely omitted in OCC-related literature is its application context for big data and its inherently associated problems, such as severe class imbalance, class rarity, noisy data, feature selection, and data reduction. We feel the survey will be appreciated by researchers working in these areas of big data.",Low
Amir Bidaki,Automatic,Springer,http://link.springer.com/article/10.1007/s11390-014-1416-y,On Density-Based Data Streams Clustering Algorithms: A Survey,2014,Type,Journal of Computer Science and Technology,Core,"Clustering data streams has drawn lots of attention in the last few years due to their ever-growing presence. Data streams put additional challenges on clustering such as limited time and memory and one pass clustering. Furthermore, discovering clusters with arbitrary shapes is very important in data stream applications. Data streams are infinite and evolving over time, and we do not have any knowledge about the number of clusters. In a data stream environment due to various factors, some noise appears occasionally. Density-based method is a remarkable class in clustering data streams, which has the ability to discover arbitrary shape clusters and to detect noise. Furthermore, it does not need the number of clusters in advance. Due to data stream characteristics, the traditional density-based clustering is not applicable. Recently, a lot of density-based clustering algorithms are extended for data streams. The main idea in these algorithms is using density-based methods in the clustering process and at the same time overcoming the constraints, which are put out by data stream’s nature. The purpose of this paper is to shed light on some algorithms in the literature on density-based clustering over data streams. We not only summarize the main density-based clustering algorithms on data streams, discuss their uniqueness and limitations, but also explain how they address the challenges in clustering data streams. Moreover, we investigate the evaluation metrics used in validating cluster quality and measuring algorithms’ performance. It is hoped that this survey will serve as a steppingstone for researchers studying data streams clustering, particularly density-based algorithms.",Artificial Intelligence,0,,,,"Clustering data streams has drawn lots of attention in the last few years due to their ever-growing presence. Data streams put additional challenges on clustering such as limited time and memory and one pass clustering. Furthermore, discovering clusters with arbitrary shapes is very important in data stream applications. Data streams are infinite and evolving over time, and we do not have any knowledge about the number of clusters. In a data stream environment due to various factors, some noise appears occasionally. Density-based method is a remarkable class in clustering data streams, which has the ability to discover arbitrary shape clusters and to detect noise. Furthermore, it does not need the number of clusters in advance. Due to data stream characteristics, the traditional density-based clustering is not applicable. Recently, a lot of density-based clustering algorithms are extended for data streams. The main idea in these algorithms is using density-based methods in the clustering process and at the same time overcoming the constraints, which are put out by data stream’s nature. The purpose of this paper is to shed light on some algorithms in the literature on density-based clustering over data streams. We not only summarize the main density-based clustering algorithms on data streams, discuss their uniqueness and limitations, but also explain how they address the challenges in clustering data streams. Moreover, we investigate the evaluation metrics used in validating cluster quality and measuring algorithms’ performance. It is hoped that this survey will serve as a steppingstone for researchers studying data streams clustering, particularly density-based algorithms.",Medium
Amir Bidaki,Automatic,Springer,http://link.springer.com/article/10.1007/s13198-024-02522-5,Network security situational awareness and early warning architecture based on big data,2024,Type,International Journal of System Assurance Engineering and Management,Core,"As the scale of the internet continues to expand and complex attack methods such as Advanced Persistent Threats (APTs) emerge, traditional Intrusion Detection Systems (IDS) and Intrusion Prevention Systems (IPS) face high rates of false positives and false negatives, creating an urgent need for a more robust network protection mechanism. To address these challenges, this paper proposes a big data-driven network security situational awareness and early warning architecture. The implementation steps include: building a data storage system using Hadoop Distributed File System (HDFS), Hive, and HBase, with HBase responsible for fast retrieval. MapReduce is used for large-scale data processing, combined with data mining techniques and Long Short-Term Memory (LSTM) networks, and Apache Mahout is employed to encapsulate traditional algorithms. A flexible situational awareness platform is designed, integrating various security devices and covering information integration, data analysis, multidimensional visualization, and warning processing. Data is stored in HDFS, Hive, and HBase, analyzed using LSTM networks, and real-time information is correlated to predict threats and generate warnings. This big data-driven network security architecture aims to enhance protection capabilities and response speed. Comparative evaluation with traditional protection systems showed that the big data-based security system increased network port traffic by approximately 50%, reduced memory usage by 36%, significantly shortened response time, and improved the security posture score by 0.19. The big data system effectively isolates external malicious information, ensuring public information security and reducing losses. This study provides significant progress in the field of network protection by offering a more robust and proactive defense mechanism against emerging threats, ultimately reducing potential risks and enhancing overall network security.","Early warning architecture
Security posture
Cyber security
Big data
Situational awareness",0,,,,"As the scale of the internet continues to expand and complex attack methods such as Advanced Persistent Threats (APTs) emerge, traditional Intrusion Detection Systems (IDS) and Intrusion Prevention Systems (IPS) face high rates of false positives and false negatives, creating an urgent need for a more robust network protection mechanism. To address these challenges, this paper proposes a big data-driven network security situational awareness and early warning architecture. The implementation steps include: building a data storage system using Hadoop Distributed File System (HDFS), Hive, and HBase, with HBase responsible for fast retrieval. MapReduce is used for large-scale data processing, combined with data mining techniques and Long Short-Term Memory (LSTM) networks, and Apache Mahout is employed to encapsulate traditional algorithms. A flexible situational awareness platform is designed, integrating various security devices and covering information integration, data analysis, multidimensional visualization, and warning processing. Data is stored in HDFS, Hive, and HBase, analyzed using LSTM networks, and real-time information is correlated to predict threats and generate warnings. This big data-driven network security architecture aims to enhance protection capabilities and response speed. Comparative evaluation with traditional protection systems showed that the big data-based security system increased network port traffic by approximately 50%, reduced memory usage by 36%, significantly shortened response time, and improved the security posture score by 0.19. The big data system effectively isolates external malicious information, ensuring public information security and reducing losses. This study provides significant progress in the field of network protection by offering a more robust and proactive defense mechanism against emerging threats, ultimately reducing potential risks and enhancing overall network security.",Low
Amir Bidaki,Automatic,Springer,http://link.springer.com/article/10.1007/s13748-015-0080-y,Feature selection for high-dimensional data,2016,Type,Progress in Artificial Intelligence,Core,"This paper offers a comprehensive approach to feature selection in the scope of classification problems, explaining the foundations, real application problems and the challenges of feature selection in the context of high-dimensional data. First, we focus on the basis of feature selection, providing a review of its history and basic concepts. Then, we address different topics in which feature selection plays a crucial role, such as microarray data, intrusion detection, or medical applications. Finally, we delve into the open challenges that researchers in the field have to deal with if they are interested to confront the advent of “Big Data” and, more specifically, the “Big Dimensionality”.",Artificial Intelligence,0,,,,"This paper offers a comprehensive approach to feature selection in the scope of classification problems, explaining the foundations, real application problems and the challenges of feature selection in the context of high-dimensional data. First, we focus on the basis of feature selection, providing a review of its history and basic concepts. Then, we address different topics in which feature selection plays a crucial role, such as microarray data, intrusion detection, or medical applications. Finally, we delve into the open challenges that researchers in the field have to deal with if they are interested to confront the advent of “Big Data” and, more specifically, the “Big Dimensionality”.",Low
Amir Bidaki,Automatic,Springer,http://link.springer.com/article/10.1007/s42979-023-01926-7,A Survey on Intrusion Detection and Prevention Systems,2023,Type,SN Computer Science,Core,"In the digital world, malicious activities that violate the confidentiality, integrity, or availability of data and devices are known as intrusions. An intrusion detection system (IDS) analyses the activities of a single system or a network to identify intrusions. It alerts the system administrators about the detected intrusions and makes them responsible for restoring the affected system(s). To automatically handle intrusions, an IDS is integrated with a response component, and the combined system is known as an intrusion detection and response system (IDRS). An IDRS forms a reactive pair that detects and responds to intrusions affecting the system(s). To prevent the occurrence of intrusions proactively, an intrusion prevention system (IPS) is deployed with an IDRS. Intrusion prevention and detection system (IPDS) forms a strong line of defense against malicious attempts that try to violate the privacy and security of the monitored device(s). This paper is an up-to-date survey of 113 research articles published in the area of IPSs, IDSs, and IDRSs in the past 7 years. It provides several insights into the literature, highlighting various future research areas. It describes the characteristics, merits, and demerits of different types of IPSs, IDSs, and IDRSs, that are a pre-requisite for developing efficient IPDSs. The foundations of the three systems and the description of their complementary functionalities as a combined system are also explained in this paper. To the best of our knowledge, there exist survey papers that focus on IPS, IDS, and IDRS separately, but not all three systems together. This paper explains the interconnected roles of IPS, IDS, and IDRS to develop an IPDS.",CRISPR-Cas systems,1 Citation,,,,"In the digital world, malicious activities that violate the confidentiality, integrity, or availability of data and devices are known as intrusions. An intrusion detection system (IDS) analyses the activities of a single system or a network to identify intrusions. It alerts the system administrators about the detected intrusions and makes them responsible for restoring the affected system(s). To automatically handle intrusions, an IDS is integrated with a response component, and the combined system is known as an intrusion detection and response system (IDRS). An IDRS forms a reactive pair that detects and responds to intrusions affecting the system(s). To prevent the occurrence of intrusions proactively, an intrusion prevention system (IPS) is deployed with an IDRS. Intrusion prevention and detection system (IPDS) forms a strong line of defense against malicious attempts that try to violate the privacy and security of the monitored device(s). This paper is an up-to-date survey of 113 research articles published in the area of IPSs, IDSs, and IDRSs in the past 7 years. It provides several insights into the literature, highlighting various future research areas. It describes the characteristics, merits, and demerits of different types of IPSs, IDSs, and IDRSs, that are a pre-requisite for developing efficient IPDSs. The foundations of the three systems and the description of their complementary functionalities as a combined system are also explained in this paper. To the best of our knowledge, there exist survey papers that focus on IPS, IDS, and IDRS separately, but not all three systems together. This paper explains the interconnected roles of IPS, IDS, and IDRS to develop an IPDS.",Low
Amir Bidaki,Automatic,Springer,http://link.springer.com/article/10.1007/s10044-017-0659-y,Reservoir-based network traffic stream summarization for anomaly detection,2018,Type,Pattern Analysis and Applications,Core,"Summarization is an important intermediate step for expediting knowledge discovery tasks such as anomaly detection. In the context of anomaly detection from data stream, the summary needs to represent both anomalous and normal data.
But streaming data has distinct characteristics, such as one-pass constraint, for which conducting data mining operations are difficult. Existing stream summarization techniques are unable to create summary which represent both normal and anomalous instances. To address this problem, in this paper, a number of hybrid summarization techniques are designed and developed using the concept of reservoir for anomaly detection from network traffic. Experimental results on thirteen benchmark data streams show that the summaries produced from stream using pairwise distance (PSSR) and template matching (TMSSR) techniques can retain more anomalies than existing stream summarization techniques, and anomaly detection technique can identify the anomalies with high true positive and low false positive rate.","Reservoir
Stream summary
Clustering
Anomaly detection",0,,,,"Summarization is an important intermediate step for expediting knowledge discovery tasks such as anomaly detection. In the context of anomaly detection from data stream, the summary needs to represent both anomalous and normal data.
But streaming data has distinct characteristics, such as one-pass constraint, for which conducting data mining operations are difficult. Existing stream summarization techniques are unable to create summary which represent both normal and anomalous instances. To address this problem, in this paper, a number of hybrid summarization techniques are designed and developed using the concept of reservoir for anomaly detection from network traffic. Experimental results on thirteen benchmark data streams show that the summaries produced from stream using pairwise distance (PSSR) and template matching (TMSSR) techniques can retain more anomalies than existing stream summarization techniques, and anomaly detection technique can identify the anomalies with high true positive and low false positive rate.",Low
Amir Bidaki,Automatic,Springer,http://link.springer.com/article/10.1007/s11042-019-7543-2,A review of improved extreme learning machine methods for data stream classification,2019,Type,Multimedia Tools and Applications,Core,"Classification is a hotspot in data stream mining and has gained increasing interest from various research fields. Compared with traditional data stream classification methods, Extreme Learning Machine (ELM) has attracted much attention because of its efficiency and simplicity, which inspired the development of many improved ELM algorithms that have been proposed in the past few years. This paper mainly reviews the current state of ELM used to classify data streams and its variants. First, we introduce the principles of ELM and the existing problems of data stream classification. Then we provide an overview of various improvements made to ELM, which further improves its stability, accuracy and generalization ability and present the practical applications of ELM used in data stream classification. Finally, the paper highlights the existing problems of ELM used for data stream mining and development prospects of ELM in the future.","Data streams
Classification
Improved extreme leaning machine
Concept drifts
Imbalanced data streams
Uncertain data streams",0,,,,"Classification is a hotspot in data stream mining and has gained increasing interest from various research fields. Compared with traditional data stream classification methods, Extreme Learning Machine (ELM) has attracted much attention because of its efficiency and simplicity, which inspired the development of many improved ELM algorithms that have been proposed in the past few years. This paper mainly reviews the current state of ELM used to classify data streams and its variants. First, we introduce the principles of ELM and the existing problems of data stream classification. Then we provide an overview of various improvements made to ELM, which further improves its stability, accuracy and generalization ability and present the practical applications of ELM used in data stream classification. Finally, the paper highlights the existing problems of ELM used for data stream mining and development prospects of ELM in the future.",Low
Amir Bidaki,Automatic,Springer,http://link.springer.com/article/10.1007/s11227-020-03196-z,MLEsIDSs: machine learning-based ensembles for intrusion detection systems—a review,2020,Type,The Journal of Supercomputing,Core,"Network security plays an essential role in secure communication and avoids financial loss and crippled services due to network intrusions. Intruders generally exploit the flaws of popular software to mount a variety of attacks against network computer systems. The damage caused in the network attacks may vary from a little disruption in service to on developing financial loss. Recently, intrusion detection systems (IDSs) comprising machine learning techniques have emerged for handling unauthorized usage and access to network resources. With the passage of time, a wide variety of machine learning techniques have been designed and integrated with IDSs. Still, most of the IDSs reported poor intrusion detection results using false positive rate and detection rate. For solving these issues, researchers focused on the development of ensemble classifiers involving the integration of predictions by multiple individual classifiers. The ensemble classifiers enable to compensate for the weakness of individual classifiers and use their combined knowledge to enhance its performance. This study presents motivation and comprehensive review of intrusion detection systems based on ensembles in machine learning as an extension of our previous work in the field. Particularly, different ensemble methods in the field are analysed, taking into consideration different types of ensembles, and various approaches for integrating the predictions of individual classifiers for an ensemble classifier. The representative studies are compared in chronological order for systematic and critical analysis, understanding the current challenges and status of research in the field. Finally, the study presents essential future research directions for the development of effective IDSs.","Artificial intelligence
Ensemble
Hybrid classifiers
Intrusion detection
Machine learning",48 Citations,,,,"Network security plays an essential role in secure communication and avoids financial loss and crippled services due to network intrusions. Intruders generally exploit the flaws of popular software to mount a variety of attacks against network computer systems. The damage caused in the network attacks may vary from a little disruption in service to on developing financial loss. Recently, intrusion detection systems (IDSs) comprising machine learning techniques have emerged for handling unauthorized usage and access to network resources. With the passage of time, a wide variety of machine learning techniques have been designed and integrated with IDSs. Still, most of the IDSs reported poor intrusion detection results using false positive rate and detection rate. For solving these issues, researchers focused on the development of ensemble classifiers involving the integration of predictions by multiple individual classifiers. The ensemble classifiers enable to compensate for the weakness of individual classifiers and use their combined knowledge to enhance its performance. This study presents motivation and comprehensive review of intrusion detection systems based on ensembles in machine learning as an extension of our previous work in the field. Particularly, different ensemble methods in the field are analysed, taking into consideration different types of ensembles, and various approaches for integrating the predictions of individual classifiers for an ensemble classifier. The representative studies are compared in chronological order for systematic and critical analysis, understanding the current challenges and status of research in the field. Finally, the study presents essential future research directions for the development of effective IDSs.",Low
Amir Bidaki,Automatic,Springer,http://link.springer.com/article/10.1007/s10115-021-01582-4,"Data stream classification with novel class detection: a review, comparison and challenges",2021,Type,Knowledge and Information Systems,Core,"Developing effective and efficient data stream classifiers is challenging for the machine learning community because of the dynamic nature of data streams. As a result, many data stream learning algorithms have been proposed during the past decades and achieve great success in various fields. This paper aims to explore a specific type of challenge in learning evolving data streams, called concept evolution (emergence of novel classes). Concept evolution indicates that the underlying patterns evolve over time, and new patterns (classes) may emerge at any time in streaming data. Therefore, data stream classifiers with emerging class detection have received increasing attention in recent years due to the practical values in many real-world applications. In this article, we provide a comprehensive overview of the existing works in this line of research. We discuss and analyze various aspects of the proposed algorithms for data stream classification with concept evolution detection and adaptation. Additionally, we discuss the potential application areas in which these techniques can be used. We also provide a detailed overview of evaluation measures and datasets used in these studies. Finally, we describe the current research challenges and future directions for data stream classification with novel class detection.","Novel class detection
Data stream classification
Concept drift
Clustering
Concept evolution",27 Citations,,,,"Developing effective and efficient data stream classifiers is challenging for the machine learning community because of the dynamic nature of data streams. As a result, many data stream learning algorithms have been proposed during the past decades and achieve great success in various fields. This paper aims to explore a specific type of challenge in learning evolving data streams, called concept evolution (emergence of novel classes). Concept evolution indicates that the underlying patterns evolve over time, and new patterns (classes) may emerge at any time in streaming data. Therefore, data stream classifiers with emerging class detection have received increasing attention in recent years due to the practical values in many real-world applications. In this article, we provide a comprehensive overview of the existing works in this line of research. We discuss and analyze various aspects of the proposed algorithms for data stream classification with concept evolution detection and adaptation. Additionally, we discuss the potential application areas in which these techniques can be used. We also provide a detailed overview of evaluation measures and datasets used in these studies. Finally, we describe the current research challenges and future directions for data stream classification with novel class detection.",Medium
Amir Bidaki,Automatic,Springer,http://link.springer.com/article/10.1007/s10462-024-10759-6,"A survey on imbalanced learning: latest research, applications and future directions",2024,Type,Artificial Intelligence Review,Core,"Imbalanced learning constitutes one of the most formidable challenges within data mining and machine learning. Despite continuous research advancement over the past decades, learning from data with an imbalanced class distribution remains a compelling research area. Imbalanced class distributions commonly constrain the practical utility of machine learning and even deep learning models in tangible applications. Numerous recent studies have made substantial progress in the field of imbalanced learning, deepening our understanding of its nature while concurrently unearthing new challenges. Given the field’s rapid evolution, this paper aims to encapsulate the recent breakthroughs in imbalanced learning by providing an in-depth review of extant strategies to confront this issue. Unlike most surveys that primarily address classification tasks in machine learning, we also delve into techniques addressing regression tasks and facets of deep long-tail learning. Furthermore, we explore real-world applications of imbalanced learning, devising a broad spectrum of research applications from management science to engineering, and lastly, discuss newly-emerging issues and challenges necessitating further exploration in the realm of imbalanced learning.",Artificial Intelligence,43 Citations,,,,"Imbalanced learning constitutes one of the most formidable challenges within data mining and machine learning. Despite continuous research advancement over the past decades, learning from data with an imbalanced class distribution remains a compelling research area. Imbalanced class distributions commonly constrain the practical utility of machine learning and even deep learning models in tangible applications. Numerous recent studies have made substantial progress in the field of imbalanced learning, deepening our understanding of its nature while concurrently unearthing new challenges. Given the field’s rapid evolution, this paper aims to encapsulate the recent breakthroughs in imbalanced learning by providing an in-depth review of extant strategies to confront this issue. Unlike most surveys that primarily address classification tasks in machine learning, we also delve into techniques addressing regression tasks and facets of deep long-tail learning. Furthermore, we explore real-world applications of imbalanced learning, devising a broad spectrum of research applications from management science to engineering, and lastly, discuss newly-emerging issues and challenges necessitating further exploration in the realm of imbalanced learning.",Low
Amir Bidaki,Automatic,Springer,http://link.springer.com/article/10.1007/s10619-021-07353-y,Research on network abnormal data flow mining based on improved cluster analysis,2022,Type,Distributed and Parallel Databases,Core,"Aiming at the problems of traditional methods that cannot adapt to the interference of noise or abnormal data, the data mining time is long, and the data mining accuracy is low, a network abnormal data stream mining method based on improved clustering analysis is proposed. By establishing a preprocessing model for abnormal network data flow, real-time data flow query is realized. Construct a network abnormal incremental data classification model to reduce the interference of noise data on data processing. The least square method is used to further filter the interference data in the abnormal incremental data of the network, and obtain the quantized data stream. Statistic network abnormal data frequent pattern data set, on this basis, adopt improved clustering method to complete the mining of network abnormal data stream. The experimental results show that the highest anti-noise coefficient of the proposed method is 0.7, and the data mining time is shorter, and the data mining accuracy is higher, which fully verifies the data stream mining performance of the method.","Improved cluster analysis
Data stream mining
Least squares method
Frequent patterns
Preprocessing model",0,,,,"Aiming at the problems of traditional methods that cannot adapt to the interference of noise or abnormal data, the data mining time is long, and the data mining accuracy is low, a network abnormal data stream mining method based on improved clustering analysis is proposed. By establishing a preprocessing model for abnormal network data flow, real-time data flow query is realized. Construct a network abnormal incremental data classification model to reduce the interference of noise data on data processing. The least square method is used to further filter the interference data in the abnormal incremental data of the network, and obtain the quantized data stream. Statistic network abnormal data frequent pattern data set, on this basis, adopt improved clustering method to complete the mining of network abnormal data stream. The experimental results show that the highest anti-noise coefficient of the proposed method is 0.7, and the data mining time is shorter, and the data mining accuracy is higher, which fully verifies the data stream mining performance of the method.",Medium
Amir Bidaki,Automatic,Springer,http://link.springer.com/article/10.1007/s12065-013-0101-3,A fast anomaly detection system using probabilistic artificial immune algorithm capable of learning new attacks,2014,Type,Evolutionary Intelligence,Core,"In this paper, we propose anomaly based intrusion detection algorithms in computer networks using artificial immune systems, capable of learning new attacks. Unique characteristics and observations specific to computer networks are considered in developing faster algorithms while achieving high performance. Although these characteristics play a key role in the proposed algorithms, we believe they have been neglected in the previous related works. We evaluate the proposed algorithms on a number of well-known intrusion detection datasets, as well as two new real datasets extracted from the data networks for intrusion detection. We analyze the detection performance and learning capabilities of the proposed algorithms, in addition to performance criteria such as false alarm rate, detection rate, and response time. The experimental results demonstrate that the proposed algorithms exhibit fast response time, low false alarm rate, and high detection rate. They can also learn new attack patterns, and identify them the next time they are introduced to the network.",Artificial Intelligence,0,,,,"In this paper, we propose anomaly based intrusion detection algorithms in computer networks using artificial immune systems, capable of learning new attacks. Unique characteristics and observations specific to computer networks are considered in developing faster algorithms while achieving high performance. Although these characteristics play a key role in the proposed algorithms, we believe they have been neglected in the previous related works. We evaluate the proposed algorithms on a number of well-known intrusion detection datasets, as well as two new real datasets extracted from the data networks for intrusion detection. We analyze the detection performance and learning capabilities of the proposed algorithms, in addition to performance criteria such as false alarm rate, detection rate, and response time. The experimental results demonstrate that the proposed algorithms exhibit fast response time, low false alarm rate, and high detection rate. They can also learn new attack patterns, and identify them the next time they are introduced to the network.",Medium
Amir Bidaki,Automatic,Springer,http://link.springer.com/article/10.1007/s11265-021-01728-1,SECURE-GEGELATI Always-On Intrusion Detection through GEGELATI Lightweight Tangled Program Graphs,2022,Type,Journal of Signal Processing Systems,Core,"The fast improvement of Machine-Learning (ML) methods gives rise to new attacks in Information System (IS). Simultaneously, ML also creates new opportunities for network intrusion detection. Early network intrusion detection is a valuable asset for IS security, as it fosters early deployment of countermeasures and reduces the impact of attacks on system availability. This paper proposes and studies an anomaly-based Network Intrusion Detection System (NIDS) based on Tangled Program Graph (TPG) ML and called Secure-Gegelati. Secure-GEGELATI learns how to detect intrusions from IS-produced traces and is optimised to fit the requirements of intrusion detection. The study evaluates the capacity of Secure-Gegelati to act as a continuously learning, real-time, and low energy NIDS when executed in an embedded network probe. We show that a TPG is capable of switching between training and inference phases, new training phases enriching the probe knowledge with limited degradation of previous intrusion detection capabilities. The Secure-GEGELATI software reaches \(8 \times\) the energy efficiency of an optimised Random Forests (RF)-based Intrusion Detection System (IDS) on the same platform. It is capable of processing 13.2 k connections/seconds with a peak power of less than \(3.3 Watts\) on an embedded platform, and is processing in real-time the CIC-IDS 2017 dataset while detecting 84% of intrusions and raising less than 0.2% of false alarms.","Tangled program graphs intelligence
Network intrusion detection
Cyber security
Network security
Real-time processing",3 Citations,,,,"The fast improvement of Machine-Learning (ML) methods gives rise to new attacks in Information System (IS). Simultaneously, ML also creates new opportunities for network intrusion detection. Early network intrusion detection is a valuable asset for IS security, as it fosters early deployment of countermeasures and reduces the impact of attacks on system availability. This paper proposes and studies an anomaly-based Network Intrusion Detection System (NIDS) based on Tangled Program Graph (TPG) ML and called Secure-Gegelati. Secure-GEGELATI learns how to detect intrusions from IS-produced traces and is optimised to fit the requirements of intrusion detection. The study evaluates the capacity of Secure-Gegelati to act as a continuously learning, real-time, and low energy NIDS when executed in an embedded network probe. We show that a TPG is capable of switching between training and inference phases, new training phases enriching the probe knowledge with limited degradation of previous intrusion detection capabilities. The Secure-GEGELATI software reaches \(8 \times\) the energy efficiency of an optimised Random Forests (RF)-based Intrusion Detection System (IDS) on the same platform. It is capable of processing 13.2 k connections/seconds with a peak power of less than \(3.3 Watts\) on an embedded platform, and is processing in real-time the CIC-IDS 2017 dataset while detecting 84% of intrusions and raising less than 0.2% of false alarms.",Medium
Amir Bidaki,Automatic,Springer,http://link.springer.com/article/10.1007/s11227-024-06345-w,A network intrusion detection system based on deep learning in the IoT,2024,Type,The Journal of Supercomputing,Core,"As industrial and everyday devices become increasingly interconnected, the data volume within the Internet of Things (IoT) has experienced a substantial surge. This surge in data presents a heightened risk of IoT environments being vulnerable to cyber attacks, which poses a significant threat to the seamless functioning of both industrial and daily activities. Therefore, the implementation of Network Intrusion Detection System (IDS) is vital for safeguarding the security of IoT network environments. This paper introduces a network intrusion detection model based on deep learning (DL). The model aims to enhance detection accuracy by extracting features from both the spatial and temporal dimensions of network traffic data. To tackle the challenge of low detection accuracy arising from data imbalance, in this study, a Conditional Tabular Generative Adversarial Network (CTGAN) is utilized to generate synthetic data for the minority class. The objective is to enhance the volume of minority class samples, address data imbalance, and subsequently enhance the accuracy of network intrusion detection. The classification performance of the proposed model is validated on UNSW-NB15, CIC-IDS2018, and CIC-IOT2023 datasets. The experimental findings demonstrate that the suggested model attains elevated levels of classification accuracy across all three datasets. The model presented in this article is particularly well suited to handle multi-class intrusion detection tasks. The model demonstrates superior performance compared to other models used for comparison.","Network intrusion detection
Deep learning
Data imbalance
Conditional tabular generative adversarial network",4 Citations,,,,"As industrial and everyday devices become increasingly interconnected, the data volume within the Internet of Things (IoT) has experienced a substantial surge. This surge in data presents a heightened risk of IoT environments being vulnerable to cyber attacks, which poses a significant threat to the seamless functioning of both industrial and daily activities. Therefore, the implementation of Network Intrusion Detection System (IDS) is vital for safeguarding the security of IoT network environments. This paper introduces a network intrusion detection model based on deep learning (DL). The model aims to enhance detection accuracy by extracting features from both the spatial and temporal dimensions of network traffic data. To tackle the challenge of low detection accuracy arising from data imbalance, in this study, a Conditional Tabular Generative Adversarial Network (CTGAN) is utilized to generate synthetic data for the minority class. The objective is to enhance the volume of minority class samples, address data imbalance, and subsequently enhance the accuracy of network intrusion detection. The classification performance of the proposed model is validated on UNSW-NB15, CIC-IDS2018, and CIC-IOT2023 datasets. The experimental findings demonstrate that the suggested model attains elevated levels of classification accuracy across all three datasets. The model presented in this article is particularly well suited to handle multi-class intrusion detection tasks. The model demonstrates superior performance compared to other models used for comparison.",Low
Amir Bidaki,Automatic,Springer,http://link.springer.com/article/10.1007/s10115-023-02002-5,A smart intelligent approach based on hybrid group search and pelican optimization algorithm for data stream clustering,2024,Type,Knowledge and Information Systems,Core,"Big data applications generate a huge range of evolving, real-time, and high-dimensional streaming data. In many applications, data stream clustering regarding efficiency and effectiveness becomes challenging. A major issue in data mining is clustering of data streams. The several clustering techniques were implemented for stream data, but they are mostly quite restricted approaches to cluster dynamics. Generally, the data stream is an arrival of data sequence and also several factors are added in the clustering, which is rather than the classical clustering. For every data point, the stream is mostly unbounded and also the data has been estimated atleast once. It leads to higher processing time and an additional requirement on memory. In addition, the clusters in each data and their statistical property vary over time, and streams can be noisy. To address these challenges, this research work aims to implement a novel data stream clustering which is developed with a hybrid meta-heuristic model. Initially, a data stream is collected, and the micro-clusters are formed by the K-Means Clustering (KMC) technique. Then, the formation of micro-clusters, merge and sorting of the data clusters, where the cluster optimization is performed by the Hybrid Group Search Pelican Optimization (HGSPO). The main objective of the clustering is performed to maximize the accuracy through the radius, distance and similarity measures and then, the thresholds of these metrics are optimized. In the training phase, a stream of clustering threshold is fixed for each cluster. When new data comes into this stream clustering model, the output of training data is measured with new data output that is decided to forward the data into the appropriate clusters based on the assigned threshold with minimum similarity. Through the performance analysis and the attained results, the clustering quality of the recommended system is ensured regarding standard performance metrics by estimating with various clustering and heuristic algorithms.","Data stream clustering
K-means clustering
Hybrid group search pelican optimization
Micro-cluster formation
Cluster organization
Cluster optimization",3 Citations,,,,"Big data applications generate a huge range of evolving, real-time, and high-dimensional streaming data. In many applications, data stream clustering regarding efficiency and effectiveness becomes challenging. A major issue in data mining is clustering of data streams. The several clustering techniques were implemented for stream data, but they are mostly quite restricted approaches to cluster dynamics. Generally, the data stream is an arrival of data sequence and also several factors are added in the clustering, which is rather than the classical clustering. For every data point, the stream is mostly unbounded and also the data has been estimated atleast once. It leads to higher processing time and an additional requirement on memory. In addition, the clusters in each data and their statistical property vary over time, and streams can be noisy. To address these challenges, this research work aims to implement a novel data stream clustering which is developed with a hybrid meta-heuristic model. Initially, a data stream is collected, and the micro-clusters are formed by the K-Means Clustering (KMC) technique. Then, the formation of micro-clusters, merge and sorting of the data clusters, where the cluster optimization is performed by the Hybrid Group Search Pelican Optimization (HGSPO). The main objective of the clustering is performed to maximize the accuracy through the radius, distance and similarity measures and then, the thresholds of these metrics are optimized. In the training phase, a stream of clustering threshold is fixed for each cluster. When new data comes into this stream clustering model, the output of training data is measured with new data output that is decided to forward the data into the appropriate clusters based on the assigned threshold with minimum similarity. Through the performance analysis and the attained results, the clustering quality of the recommended system is ensured regarding standard performance metrics by estimating with various clustering and heuristic algorithms.",Low
Amir Bidaki,Automatic,Springer,http://link.springer.com/article/10.1186/s40537-015-0013-4,Intrusion detection and Big Heterogeneous Data: a Survey,2015,Type,Journal of Big Data,Core,"Intrusion Detection has been heavily studied in both industry and academia, but cybersecurity analysts still desire much more alert accuracy and overall threat analysis in order to secure their systems within cyberspace. Improvements to Intrusion Detection could be achieved by embracing a more comprehensive approach in monitoring security events from many different heterogeneous sources. Correlating security events from heterogeneous sources can grant a more holistic view and greater situational awareness of cyber threats. One problem with this approach is that currently, even a single event source (e.g., network traffic) can experience Big Data challenges when considered alone. Attempts to use more heterogeneous data sources pose an even greater Big Data challenge. Big Data technologies for Intrusion Detection can help solve these Big Heterogeneous Data challenges. In this paper, we review the scope of works considering the problem of heterogeneous data and in particular Big Heterogeneous Data. We discuss the specific issues of Data Fusion, Heterogeneous Intrusion Detection Architectures, and Security Information and Event Management (SIEM) systems, as well as presenting areas where more research opportunities exist. Overall, both cyber threat analysis and cyber intelligence could be enhanced by correlating security events across many diverse heterogeneous sources.","Intrusion detection
Big data
Security
IDS
SIEM
Data fusion
Heterogeneous
Hadoop
Cloud
Feature selection
Situational awareness
Big Heterogeneous Data",166 Citations,,,,"Intrusion Detection has been heavily studied in both industry and academia, but cybersecurity analysts still desire much more alert accuracy and overall threat analysis in order to secure their systems within cyberspace. Improvements to Intrusion Detection could be achieved by embracing a more comprehensive approach in monitoring security events from many different heterogeneous sources. Correlating security events from heterogeneous sources can grant a more holistic view and greater situational awareness of cyber threats. One problem with this approach is that currently, even a single event source (e.g., network traffic) can experience Big Data challenges when considered alone. Attempts to use more heterogeneous data sources pose an even greater Big Data challenge. Big Data technologies for Intrusion Detection can help solve these Big Heterogeneous Data challenges. In this paper, we review the scope of works considering the problem of heterogeneous data and in particular Big Heterogeneous Data. We discuss the specific issues of Data Fusion, Heterogeneous Intrusion Detection Architectures, and Security Information and Event Management (SIEM) systems, as well as presenting areas where more research opportunities exist. Overall, both cyber threat analysis and cyber intelligence could be enhanced by correlating security events across many diverse heterogeneous sources.",Low
Amir Bidaki,Automatic,Springer,http://link.springer.com/article/10.1186/s13174-020-00127-2,"A survey on data analysis on large-Scale wireless networks: online stream processing, trends, and challenges",2020,Type,Journal of Internet Services and Applications,Core,"In this paper we focus on knowledge extraction from large-scale wireless networks through stream processing. We present the primary methods for sampling, data collection, and monitoring of wireless networks and we characterize knowledge extraction as a machine learning problem on big data stream processing. We show the main trends in big data stream processing frameworks. Additionally, we explore the data preprocessing, feature engineering, and the machine learning algorithms applied to the scenario of wireless network analytics. We address challenges and present research projects in wireless network monitoring and stream processing. Finally, future perspectives, such as deep learning and reinforcement learning in stream processing, are anticipated.","Stream processing
Big data
Wireless
Data mining",0,,,,"In this paper we focus on knowledge extraction from large-scale wireless networks through stream processing. We present the primary methods for sampling, data collection, and monitoring of wireless networks and we characterize knowledge extraction as a machine learning problem on big data stream processing. We show the main trends in big data stream processing frameworks. Additionally, we explore the data preprocessing, feature engineering, and the machine learning algorithms applied to the scenario of wireless network analytics. We address challenges and present research projects in wireless network monitoring and stream processing. Finally, future perspectives, such as deep learning and reinforcement learning in stream processing, are anticipated.",Medium
Amir Bidaki,Automatic,Springer,http://link.springer.com/article/10.1007/s44163-024-00120-9,Self-healing hybrid intrusion detection system: an ensemble machine learning approach,2024,Type,Discover Artificial Intelligence,Core,"The increasing complexity and adversity of cyber-attacks have prompted discussions in the cyber scenario for a prognosticate approach, rather than a reactionary one. In this paper, a signature-based intrusion detection system has been built based on C5 classifiers, to classify packets into normal and attack categories. Next, an anomaly-based intrusion detection was built based on the LSTM (Long-Short Term Memory) algorithm to detect anomalies. These anomalies are then fed into the signature generator to extract attributes. These attributes get uploaded into the C5 training set, aiding the ensemble model in continual learning with expanding signatures of unknown attacks. By generating signatures of unknown attacks, the self-healing attribute of the ensemble model contributes to the early detection of attacks. For the C5 classifier, the proposed model is evaluated on the UNSW-NB15 dataset, while for the LSTM model, it is evaluated on the ADFA-LD dataset. Compared to conventional models, the experimental results show better detection rates for both known and unknown attacks. The C5 classifier achieved a True Positive Rate of 97% while maintaining a false positive rate of 8%. Also, the LSTM model achieved a detection rate of 90% while retaining a 17% False Alarm Rate. As the proposed model learns, its performance in real network traffic also improves and it also eliminates human intervention when updating training data.",Artificial Intelligence,0,,,,"The increasing complexity and adversity of cyber-attacks have prompted discussions in the cyber scenario for a prognosticate approach, rather than a reactionary one. In this paper, a signature-based intrusion detection system has been built based on C5 classifiers, to classify packets into normal and attack categories. Next, an anomaly-based intrusion detection was built based on the LSTM (Long-Short Term Memory) algorithm to detect anomalies. These anomalies are then fed into the signature generator to extract attributes. These attributes get uploaded into the C5 training set, aiding the ensemble model in continual learning with expanding signatures of unknown attacks. By generating signatures of unknown attacks, the self-healing attribute of the ensemble model contributes to the early detection of attacks. For the C5 classifier, the proposed model is evaluated on the UNSW-NB15 dataset, while for the LSTM model, it is evaluated on the ADFA-LD dataset. Compared to conventional models, the experimental results show better detection rates for both known and unknown attacks. The C5 classifier achieved a True Positive Rate of 97% while maintaining a false positive rate of 8%. Also, the LSTM model achieved a detection rate of 90% while retaining a 17% False Alarm Rate. As the proposed model learns, its performance in real network traffic also improves and it also eliminates human intervention when updating training data.",High
Amir Bidaki,Automatic,Springer,http://link.springer.com/article/10.1007/s11227-024-06552-5,XIDINTFL-VAE: XGBoost-based intrusion detection of imbalance network traffic via class-wise focal loss variational autoencoder,2024,Type,The Journal of Supercomputing,Core,"Intrusion Detection Systems (IDS) face significant challenges in detecting minority class attacks within imbalanced network traffic, where traditional methods often struggle to maintain high accuracy without sacrificing key performance metrics like precision and recall. This study introduces the XIDINTFL-VAE framework, which leverages Class-Wise Focal Loss (CWFL) and Variational AutoEncoder (VAE) integrated with XGBoost to effectively address this imbalance. Our approach is designed to enhance the detection of minority class intrusions while maintaining robust overall performance. Current techniques, such as SMOTE and its variants, often fail to balance precision and recall adequately in highly imbalanced datasets, resulting in either high false positives or missed detections. The proposed method directly addresses this gap by generating synthetic data tailored to the most challenging cases within the minority class, thereby improving the classifier’s ability to detect these rare but critical instances. A comparative analysis of the proposed XIDINTFL-VAE method was conducted against various oversampling techniques, including SMOTE, Borderline-SMOTE, and Adaptive Synthetic Sampling (ADASYN), as well as traditional classifiers like Logistic Regression, K-Nearest Neighbors (KNN), Support Vector Machine (SVM), and Decision Tree. While existing techniques focus on balancing datasets or generating realistic data, the CWFL-VAE method takes a more targeted approach by generating synthetic data that specifically enhances the classifier’s ability to handle challenging minority class instances. Experimental evaluations using the NSL-KDD and CSE-CIC-IDS2018 datasets demonstrate that the XIDINTFL-VAE model outperforms traditional methods, achieving a precision of 99.67% and an F1 score of 94.74%, with a slight trade-off in recall at 89.41%. These results underscore the model’s capability to reduce false positives while maintaining high detection rates, which is crucial for real-world applications. The statistical significance of these improvements is confirmed by comparative analysis, establishing that our approach offers a meaningful advancement over existing methods.","Intrusion detection system
Imbalance network traffic
Focal loss
Variational autoencoder
Extreme gradient boosting",0,,,,"Intrusion Detection Systems (IDS) face significant challenges in detecting minority class attacks within imbalanced network traffic, where traditional methods often struggle to maintain high accuracy without sacrificing key performance metrics like precision and recall. This study introduces the XIDINTFL-VAE framework, which leverages Class-Wise Focal Loss (CWFL) and Variational AutoEncoder (VAE) integrated with XGBoost to effectively address this imbalance. Our approach is designed to enhance the detection of minority class intrusions while maintaining robust overall performance. Current techniques, such as SMOTE and its variants, often fail to balance precision and recall adequately in highly imbalanced datasets, resulting in either high false positives or missed detections. The proposed method directly addresses this gap by generating synthetic data tailored to the most challenging cases within the minority class, thereby improving the classifier’s ability to detect these rare but critical instances. A comparative analysis of the proposed XIDINTFL-VAE method was conducted against various oversampling techniques, including SMOTE, Borderline-SMOTE, and Adaptive Synthetic Sampling (ADASYN), as well as traditional classifiers like Logistic Regression, K-Nearest Neighbors (KNN), Support Vector Machine (SVM), and Decision Tree. While existing techniques focus on balancing datasets or generating realistic data, the CWFL-VAE method takes a more targeted approach by generating synthetic data that specifically enhances the classifier’s ability to handle challenging minority class instances. Experimental evaluations using the NSL-KDD and CSE-CIC-IDS2018 datasets demonstrate that the XIDINTFL-VAE model outperforms traditional methods, achieving a precision of 99.67% and an F1 score of 94.74%, with a slight trade-off in recall at 89.41%. These results underscore the model’s capability to reduce false positives while maintaining high detection rates, which is crucial for real-world applications. The statistical significance of these improvements is confirmed by comparative analysis, establishing that our approach offers a meaningful advancement over existing methods.",Low
Amir Bidaki,Automatic,Springer,http://link.springer.com/article/10.1007/s11227-022-05025-x,Network intrusion detection via tri-broad learning system based on spatial-temporal granularity,2023,Type,The Journal of Supercomputing,Core,"Network intrusion detection system plays a crucial role in protecting the integrity and availability of sensitive assets, where the detected traffic data contain a large amount of time, space, and statistical information. However, existing research lacks the utilization of spatial-temporal multi-granularity data features and the mutual support among different data features, thus making it difficult to specifically and accurately identify anomalies. Considering the distinctions among different granularities, we propose a framework called tri-broad learning system (TBLS), which can learn and integrate the three granular features. To explore the spatial-temporal connotation of the traffic information accurately, a feature dataset containing three granularities is constructed according to the characteristics of time, space, and data content. In this way, we use broad learning basic units to extract abstract features of different granularities and then express these features in different feature spaces to enhance them separately. We use a normal distribution initialization method in BLS to optimize the weights of feature nodes and enhancement nodes for better detection accuracy. The merits of our proposed model are exhibited on the UNSW-NB15, CIC-IDS-2017, CIC-DDoS-2019, and mixed traffic datasets. Experimental results show that TBLS outperforms the typical BLS in terms of various evaluation metrics and time consumption. Compared with other machine learning methods, TBLS achieves better performance metrics.","Network intrusion detection
Spatial-temporal multi-granularity
Traffic information
Broad learning system",0,,,,"Network intrusion detection system plays a crucial role in protecting the integrity and availability of sensitive assets, where the detected traffic data contain a large amount of time, space, and statistical information. However, existing research lacks the utilization of spatial-temporal multi-granularity data features and the mutual support among different data features, thus making it difficult to specifically and accurately identify anomalies. Considering the distinctions among different granularities, we propose a framework called tri-broad learning system (TBLS), which can learn and integrate the three granular features. To explore the spatial-temporal connotation of the traffic information accurately, a feature dataset containing three granularities is constructed according to the characteristics of time, space, and data content. In this way, we use broad learning basic units to extract abstract features of different granularities and then express these features in different feature spaces to enhance them separately. We use a normal distribution initialization method in BLS to optimize the weights of feature nodes and enhancement nodes for better detection accuracy. The merits of our proposed model are exhibited on the UNSW-NB15, CIC-IDS-2017, CIC-DDoS-2019, and mixed traffic datasets. Experimental results show that TBLS outperforms the typical BLS in terms of various evaluation metrics and time consumption. Compared with other machine learning methods, TBLS achieves better performance metrics.",Low
Amir Bidaki,Automatic,Springer,http://link.springer.com/article/10.1007/s40860-024-00226-y,A survey on the contribution of ML and DL to the detection and prevention of botnet attacks,2024,Type,Journal of Reliable Intelligent Environments,Core,"Machine Learning (ML) and Deep Learning (DL) are transforming the detection and prevention of botnets, significant threats in cybersecurity. In this survey, we highlight the shift from traditional detection methods to advanced ML and DL techniques. We demonstrate their effectiveness through case studies involving classification algorithms, clustering techniques, and neural networks. We also explore innovative strategies like federated learning and meta-learning models that enhance proactive defenses, including predictive analytics, real-time systems, and automated responses. Our paper discusses challenges such as data privacy, model overfitting, and the need for adaptability to sophisticated botnet structures. We emphasize the importance of ongoing research and collaboration across disciplines to keep pace with fast-evolving cyber threats, offering insights for developing intelligent cybersecurity defenses.",Artificial Intelligence,2 Citations,,,,"Machine Learning (ML) and Deep Learning (DL) are transforming the detection and prevention of botnets, significant threats in cybersecurity. In this survey, we highlight the shift from traditional detection methods to advanced ML and DL techniques. We demonstrate their effectiveness through case studies involving classification algorithms, clustering techniques, and neural networks. We also explore innovative strategies like federated learning and meta-learning models that enhance proactive defenses, including predictive analytics, real-time systems, and automated responses. Our paper discusses challenges such as data privacy, model overfitting, and the need for adaptability to sophisticated botnet structures. We emphasize the importance of ongoing research and collaboration across disciplines to keep pace with fast-evolving cyber threats, offering insights for developing intelligent cybersecurity defenses.",Medium
Amir Bidaki,Automatic,Springer,http://link.springer.com/article/10.1007/s00521-019-04396-2,NSNAD: negative selection-based network anomaly detection approach with relevant feature subset,2020,Type,Neural Computing and Applications,Core,"Intrusion detection systems are one of the security tools widely deployed in network architectures in order to monitor, detect and eventually respond to any suspicious activity in the network. However, the constantly growing complexity of networks and the virulence of new attacks require more adaptive approaches for optimal responses. In this work, we propose a semi-supervised approach for network anomaly detection inspired from the biological negative selection process. Based on a reduced dataset with a filter/ranking feature selection technique, our algorithm, namely negative selection for network anomaly detection (NSNAD), generates a set of detectors and uses them to classify events as anomaly. Otherwise, they are matched against an Artificial Human Leukocyte Antigen in order to be classified as normal. The accuracy and the computational time of NSNAD are tested under three intrusion detection datasets: NSL-KDD, Kyoto2006+ and UNSW-NB15. We compare the performance of NSNAD against a fully supervised algorithm (Naïve Bayes), an unsupervised clustering algorithm (K-means) and a semi-supervised algorithm (One-class SVM) with respect to multiple accuracy metrics. We also compare the time incurred by each algorithm in training and classification stages.",Artificial Intelligence,24 Citations,,,,"Intrusion detection systems are one of the security tools widely deployed in network architectures in order to monitor, detect and eventually respond to any suspicious activity in the network. However, the constantly growing complexity of networks and the virulence of new attacks require more adaptive approaches for optimal responses. In this work, we propose a semi-supervised approach for network anomaly detection inspired from the biological negative selection process. Based on a reduced dataset with a filter/ranking feature selection technique, our algorithm, namely negative selection for network anomaly detection (NSNAD), generates a set of detectors and uses them to classify events as anomaly. Otherwise, they are matched against an Artificial Human Leukocyte Antigen in order to be classified as normal. The accuracy and the computational time of NSNAD are tested under three intrusion detection datasets: NSL-KDD, Kyoto2006+ and UNSW-NB15. We compare the performance of NSNAD against a fully supervised algorithm (Naïve Bayes), an unsupervised clustering algorithm (K-means) and a semi-supervised algorithm (One-class SVM) with respect to multiple accuracy metrics. We also compare the time incurred by each algorithm in training and classification stages.",Low
Amir Bidaki,Automatic,Springer,http://link.springer.com/article/10.1007/s11036-024-02435-4,Intrusion Detection with Federated Learning and Conditional Generative Adversarial Network in Satellite-Terrestrial Integrated Networks,2024,Type,Mobile Networks and Applications,Core,"Network intrusion detection is a challenging network security research topic, especially when data privacy has become an increasing concern in satellite-terrestrial integrated networks. Federated learning was introduced as an effective distributed learning scheme. However, existing studies have primarily focused on terrestrial networks. In this study, we propose a federated learning framework based on a conditional generative adversarial network (CGAN) model for intrusion detection in satellite-terrestrial integrated networks. We further propose an efficient federated learning scheme called federated learning with dynamic weight and momentum (FedDWM) for aggregating local model parameters from terrestrial clients to satellite fed servers. Numerical experiments with the CIC-IDS2017 and CSE-CIC-IDS2018 datasets demonstrate the effectiveness of the proposed approach over baselines for imbalanced intrusion detection.","Intrusion detection
Federated learning
Conditional generative adversarial network
Satellite-terrestrial integrated network",0,,,,"Network intrusion detection is a challenging network security research topic, especially when data privacy has become an increasing concern in satellite-terrestrial integrated networks. Federated learning was introduced as an effective distributed learning scheme. However, existing studies have primarily focused on terrestrial networks. In this study, we propose a federated learning framework based on a conditional generative adversarial network (CGAN) model for intrusion detection in satellite-terrestrial integrated networks. We further propose an efficient federated learning scheme called federated learning with dynamic weight and momentum (FedDWM) for aggregating local model parameters from terrestrial clients to satellite fed servers. Numerical experiments with the CIC-IDS2017 and CSE-CIC-IDS2018 datasets demonstrate the effectiveness of the proposed approach over baselines for imbalanced intrusion detection.",Medium
Amir Bidaki,Automatic,Springer,http://link.springer.com/article/10.1186/s42400-023-00171-y,FMSA: a meta-learning framework-based fast model stealing attack technique against intelligent network intrusion detection systems,2023,Type,Cybersecurity,Core,"Intrusion detection systems are increasingly using machine learning. While machine learning has shown excellent performance in identifying malicious traffic, it may increase the risk of privacy leakage. This paper focuses on implementing a model stealing attack on intrusion detection systems. Existing model stealing attacks are hard to implement in practical network environments, as they either need private data of the victim dataset or frequent access to the victim model. In this paper, we propose a novel solution called Fast Model Stealing Attack (FMSA) to address the problem in the field of model stealing attacks. We also highlight the risks of using ML-NIDS in network security. First, meta-learning frameworks are introduced into the model stealing algorithm to clone the victim model in a black-box state. Then, the number of accesses to the target model is used as an optimization term, resulting in minimal queries to achieve model stealing. Finally, adversarial training is used to simulate the data distribution of the target model and achieve the recovery of privacy data. Through experiments on multiple public datasets, compared to existing state-of-the-art algorithms, FMSA reduces the number of accesses to the target model and improves the accuracy of the clone model on the test dataset to 88.9% and the similarity with the target model to 90.1%. We can demonstrate the successful execution of model stealing attacks on the ML-NIDS system even with protective measures in place to limit the number of anomalous queries.","AI security
Model stealing attack
Network intrusion detection
Meta learning",0,,,,"Intrusion detection systems are increasingly using machine learning. While machine learning has shown excellent performance in identifying malicious traffic, it may increase the risk of privacy leakage. This paper focuses on implementing a model stealing attack on intrusion detection systems. Existing model stealing attacks are hard to implement in practical network environments, as they either need private data of the victim dataset or frequent access to the victim model. In this paper, we propose a novel solution called Fast Model Stealing Attack (FMSA) to address the problem in the field of model stealing attacks. We also highlight the risks of using ML-NIDS in network security. First, meta-learning frameworks are introduced into the model stealing algorithm to clone the victim model in a black-box state. Then, the number of accesses to the target model is used as an optimization term, resulting in minimal queries to achieve model stealing. Finally, adversarial training is used to simulate the data distribution of the target model and achieve the recovery of privacy data. Through experiments on multiple public datasets, compared to existing state-of-the-art algorithms, FMSA reduces the number of accesses to the target model and improves the accuracy of the clone model on the test dataset to 88.9% and the similarity with the target model to 90.1%. We can demonstrate the successful execution of model stealing attacks on the ML-NIDS system even with protective measures in place to limit the number of anomalous queries.",Low
Amir Bidaki,Automatic,Springer,http://link.springer.com/article/10.1007/s10922-018-9478-8,Dynamic Link Anomaly Analysis for Network Security Management,2019,Type,Journal of Network and Systems Management,Core,"Network management is challenging due to ever increasing complexity and dynamics of network interactions. While many changes in networks are normal, some changes are not. One of the daily tasks of network administrators is to identify and analyze these abnormal changes that are hard to find by traditional security mechanisms (IDS, firewall, anti-virus, etc.). This research conducts dynamic network analysis (DNA) and presents practical methodologies of data stream mining based dynamic link anomaly analysis (DLAA) using novel sliding time window structures and network analytics metrics. DLAA employs spatiotemporal link analysis to detect anomalies from dynamic network graphs. We formally define the network link anomaly types and use key link-structure similarity metrics and time-weighted functions to model the dynamics of topological changes. The methodology is generic in that it does not require additional information from nodes or links but only the topology itself. The DLAA framework consists of three algorithmic components including sliding time window, link scoring and link anomaly detection algorithms. Through experimental study on publicly available dataset, we demonstrate that the proposed DLAA framework has the capability to construct effective knowledge structures for measuring the security levels of large scale dynamic networks, and to provide insight for generalized DNA in network security domain.","Dynamic network analysis
Link anomaly
Network security management
Graph mining",0,,,,"Network management is challenging due to ever increasing complexity and dynamics of network interactions. While many changes in networks are normal, some changes are not. One of the daily tasks of network administrators is to identify and analyze these abnormal changes that are hard to find by traditional security mechanisms (IDS, firewall, anti-virus, etc.). This research conducts dynamic network analysis (DNA) and presents practical methodologies of data stream mining based dynamic link anomaly analysis (DLAA) using novel sliding time window structures and network analytics metrics. DLAA employs spatiotemporal link analysis to detect anomalies from dynamic network graphs. We formally define the network link anomaly types and use key link-structure similarity metrics and time-weighted functions to model the dynamics of topological changes. The methodology is generic in that it does not require additional information from nodes or links but only the topology itself. The DLAA framework consists of three algorithmic components including sliding time window, link scoring and link anomaly detection algorithms. Through experimental study on publicly available dataset, we demonstrate that the proposed DLAA framework has the capability to construct effective knowledge structures for measuring the security levels of large scale dynamic networks, and to provide insight for generalized DNA in network security domain.",Medium
Amir Bidaki,Automatic,Springer,http://link.springer.com/article/10.1007/s11042-023-17300-x,Deep learning-based network intrusion detection in smart healthcare enterprise systems,2024,Type,Multimedia Tools and Applications,Core,"Network-based intrusion detection (N-IDS) is an essential system inside an organization in a smart healthcare enterprise system to prevent the system and its networks from network attacks. A survey of the literature shows that in recent days deep learning approaches are employed successfully for N-IDS using network connections. However, finding the right features from a network connection is a daunting task. This work proposes a multidimensional attention-based deep learning approach for N-IDS that extracts the optimal features for intrusion detection using network payload. The proposed approach includes an embedding that transforms every word in the payload into a 100-dimensional feature vector representation and embedding follows deep learning layers such as a convolutional neural network (CNN) and long short-term memory (LSTM) with attention to extracting optimal features for attack classification. Next, the features of CNN and LSTM layers are concatenated and passed into fully connected layers for intrusion detection. The proposed approach showed 99% accuracy on the KISTI enterprise network payload dataset. In addition, the proposed approach showed 98% accuracy and 99% accuracy on network-based datasets such as KDDCup-99, CICIDS-2017, and WSN-DS and UNSW-NB15 respectively. The good experimental results on various network-based datasets suggest that the proposed N-IDS in smart healthcare enterprise systems is robust and generalizable to detect attacks from different network environments. The proposed approach performed better in all the experiments than the other deep learning-based methods. The model showed a 5% accuracy performance improvement compared to the existing study using the KISTI dataset. In addition, the proposed model has shown similar performances on the other intrusion datasets. The proposed approach serves as a network monitoring tool for efficient and accurate detection of attacks inside an organization on a healthcare enterprise network system.","Healthcare
IoT
Cybersecurity
Cyberattacks
Intrusion detection
Deep learning",0,,,,"Network-based intrusion detection (N-IDS) is an essential system inside an organization in a smart healthcare enterprise system to prevent the system and its networks from network attacks. A survey of the literature shows that in recent days deep learning approaches are employed successfully for N-IDS using network connections. However, finding the right features from a network connection is a daunting task. This work proposes a multidimensional attention-based deep learning approach for N-IDS that extracts the optimal features for intrusion detection using network payload. The proposed approach includes an embedding that transforms every word in the payload into a 100-dimensional feature vector representation and embedding follows deep learning layers such as a convolutional neural network (CNN) and long short-term memory (LSTM) with attention to extracting optimal features for attack classification. Next, the features of CNN and LSTM layers are concatenated and passed into fully connected layers for intrusion detection. The proposed approach showed 99% accuracy on the KISTI enterprise network payload dataset. In addition, the proposed approach showed 98% accuracy and 99% accuracy on network-based datasets such as KDDCup-99, CICIDS-2017, and WSN-DS and UNSW-NB15 respectively. The good experimental results on various network-based datasets suggest that the proposed N-IDS in smart healthcare enterprise systems is robust and generalizable to detect attacks from different network environments. The proposed approach performed better in all the experiments than the other deep learning-based methods. The model showed a 5% accuracy performance improvement compared to the existing study using the KISTI dataset. In addition, the proposed model has shown similar performances on the other intrusion datasets. The proposed approach serves as a network monitoring tool for efficient and accurate detection of attacks inside an organization on a healthcare enterprise network system.",Low
Amir Bidaki,Automatic,Springer,http://link.springer.com/article/10.1007/s10700-020-09332-x,Towards fuzzy anomaly detection-based security: a comprehensive review,2021,Type,Fuzzy Optimization and Decision Making,Core,"In the data security context, anomaly detection is a branch of intrusion detection that can detect emerging intrusions and security attacks. A number of anomaly detection systems (ADSs) have been proposed in the literature that using various algorithms and techniques try to detect the intrusions and anomalies. This paper focuses on the ADS schemes which have applied fuzzy logic in combination with other machine learning and data mining techniques to deal with the inherent uncertainty in the intrusion detection process. For this purpose, it first presents the key knowledge about intrusion detection systems and then classifies the fuzzy ADS approaches regarding their utilized fuzzy algorithm. Afterward, it summarizes their major contributions and illuminates their advantages and limitations. Finally, concluding issues and directions for future researches in the fuzzy ADS context are highlighted.",Artificial Intelligence,0,,,,"In the data security context, anomaly detection is a branch of intrusion detection that can detect emerging intrusions and security attacks. A number of anomaly detection systems (ADSs) have been proposed in the literature that using various algorithms and techniques try to detect the intrusions and anomalies. This paper focuses on the ADS schemes which have applied fuzzy logic in combination with other machine learning and data mining techniques to deal with the inherent uncertainty in the intrusion detection process. For this purpose, it first presents the key knowledge about intrusion detection systems and then classifies the fuzzy ADS approaches regarding their utilized fuzzy algorithm. Afterward, it summarizes their major contributions and illuminates their advantages and limitations. Finally, concluding issues and directions for future researches in the fuzzy ADS context are highlighted.",Low
Amir Bidaki,Automatic,Springer,http://link.springer.com/article/10.1007/s12083-023-01478-w,"IoT malware: An attribute-based taxonomy, detection mechanisms and challenges",2023,Type,Peer-to-Peer Networking and Applications,Core,"During the past decade, the Internet of Things (IoT) has paved the way for the ongoing digitization of society in unique ways. Its penetration into enterprise and day-to-day lives improved the supply chain in numerous ways. Unfortunately, the profuse diversity of IoT devices has become an attractive target for malware authors who take advantage of its vulnerabilities. Accordingly, enhancing the security of IoT devices has become the primary objective of industrialists and researchers. However, most present studies lack a deep understanding of IoT malware and its various aspects. As understanding IoT malware is the preliminary base of research, in this work, we present an IoT malware taxonomy with 100 attributes based on the IoT malware categories, attack types, attack surfaces, malware distribution architecture, victim devices, victim device architecture, IoT malware characteristics, access mechanisms, programming languages, and protocols. In addition, we have mapped these categories into 77 IoT Malwares identified between 2008 and 2022. Furthermore, To provide insight into the challenges in IoT malware research for future researchers, our study also reviews the existing IoT malware detection works.","Internet of Things
Malware
Taxonomy
Challenges of malware detection methods",27 Citations,,,,"During the past decade, the Internet of Things (IoT) has paved the way for the ongoing digitization of society in unique ways. Its penetration into enterprise and day-to-day lives improved the supply chain in numerous ways. Unfortunately, the profuse diversity of IoT devices has become an attractive target for malware authors who take advantage of its vulnerabilities. Accordingly, enhancing the security of IoT devices has become the primary objective of industrialists and researchers. However, most present studies lack a deep understanding of IoT malware and its various aspects. As understanding IoT malware is the preliminary base of research, in this work, we present an IoT malware taxonomy with 100 attributes based on the IoT malware categories, attack types, attack surfaces, malware distribution architecture, victim devices, victim device architecture, IoT malware characteristics, access mechanisms, programming languages, and protocols. In addition, we have mapped these categories into 77 IoT Malwares identified between 2008 and 2022. Furthermore, To provide insight into the challenges in IoT malware research for future researchers, our study also reviews the existing IoT malware detection works.",Low
Amir Bidaki,Automatic,Springer,http://link.springer.com/article/10.1007/s42979-024-03376-1,Stacking Enabled Ensemble Learning Based Intrusion Detection Scheme (SELIDS) for IoV,2024,Type,SN Computer Science,Core,"A revolutionary approach for enhancing driving efficiency and safety in intelligent transportation systems (ITS) is deploying autonomous vehicles. Vehicle-to-everything (V2X) technology facilitates interactions between vehicles and infrastructure. However, the Internet of Vehicles (IoV) is susceptible to many cyberattacks, encompassing impersonation, denial of service (DoS), and fuzzy assaults. This paper proposes an intelligent network intrusion detection system (NIDS) using machine learning algorithms. The usage of the ML approach in a Stacking-enabled Ensemble Learning-based Intrusion Detection Scheme (SELIDS) for IoV is proposed. We additionally examine each technique’s shortcomings and how they affect the NIDS efficiency. Deploying the proposed NIDS on the benchmark dataset demonstrates the capacity of the system to recognize different kinds of assaults. Finally, we explore the potential for NIDS to collaborate with additional security technologies in the future. Empirical results prove the efficacy of the proposed mechanism.","Intelligent transportation systems (ITS)
Internet of vehicles (IoV)
Network intrusion detection system (NIDS)
Machine learning (ML)
Attacks",0,,,,"A revolutionary approach for enhancing driving efficiency and safety in intelligent transportation systems (ITS) is deploying autonomous vehicles. Vehicle-to-everything (V2X) technology facilitates interactions between vehicles and infrastructure. However, the Internet of Vehicles (IoV) is susceptible to many cyberattacks, encompassing impersonation, denial of service (DoS), and fuzzy assaults. This paper proposes an intelligent network intrusion detection system (NIDS) using machine learning algorithms. The usage of the ML approach in a Stacking-enabled Ensemble Learning-based Intrusion Detection Scheme (SELIDS) for IoV is proposed. We additionally examine each technique’s shortcomings and how they affect the NIDS efficiency. Deploying the proposed NIDS on the benchmark dataset demonstrates the capacity of the system to recognize different kinds of assaults. Finally, we explore the potential for NIDS to collaborate with additional security technologies in the future. Empirical results prove the efficacy of the proposed mechanism.",Low
Amir Bidaki,Automatic,Springer,http://link.springer.com/article/10.1007/s11227-024-06070-4,Enhancing network intrusion detection by lifelong active online learning,2024,Type,The Journal of Supercomputing,Core,"Machine learning has been widely used to build intrusion detection models in detecting unknown attack traffic. How to train a model properly in order to attain the desired intrusion detection is an important topic. In contrast to offline learning, online learning proves more practical as it can update models simultaneously in the detecting process to comply with real network traffic. Active learning is an effective way to realize online learning. Among existing active learning mechanisms proposed to perform intrusion detection, most fail to meet the real online environment or to run persistently. This paper presents a new active online learning mechanism to secure better intrusion detection performance. The new mechanism advances related works in bringing the lifelong learning practice to fit in the online environment. It uses the efficient random forest (RF) as the detection model to train samples and adds a new tree to train a new batch of data when updating the model at each online stage, to pursue lifelong learning. By training a new batch of data only, it can keep the previously trained weights from being updated so as to preserve the past knowledge. Our mechanism is experimentally proved to yield better overall results than existing mechanisms: It produces superior training efficiency and detection performance—with the least training time, best training data quality and much reduced training data quantity.","Network intrusion detection
Machine learning
Active learning
Lifelong learning
Online learning
Performance evaluation",0,,,,"Machine learning has been widely used to build intrusion detection models in detecting unknown attack traffic. How to train a model properly in order to attain the desired intrusion detection is an important topic. In contrast to offline learning, online learning proves more practical as it can update models simultaneously in the detecting process to comply with real network traffic. Active learning is an effective way to realize online learning. Among existing active learning mechanisms proposed to perform intrusion detection, most fail to meet the real online environment or to run persistently. This paper presents a new active online learning mechanism to secure better intrusion detection performance. The new mechanism advances related works in bringing the lifelong learning practice to fit in the online environment. It uses the efficient random forest (RF) as the detection model to train samples and adds a new tree to train a new batch of data when updating the model at each online stage, to pursue lifelong learning. By training a new batch of data only, it can keep the previously trained weights from being updated so as to preserve the past knowledge. Our mechanism is experimentally proved to yield better overall results than existing mechanisms: It produces superior training efficiency and detection performance—with the least training time, best training data quality and much reduced training data quantity.",Medium
Amir Bidaki,Automatic,Springer,http://link.springer.com/article/10.1007/s00521-024-09863-z,Network security AIOps for online stream data monitoring,2024,Type,Neural Computing and Applications,Core,"In cybersecurity, live production data for predictive analysis pose a significant challenge due to the inherently secure nature of the domain. Although there are publicly available, synthesized, and artificially generated datasets, authentic scenarios are rarely encountered. For anomaly-based detection, the dynamic definition of thresholds has gained importance and attention in detecting abnormalities and preventing malicious activities. Unlike conventional threshold-based methods, deep learning data modeling provides a more nuanced perspective on network monitoring. This enables security systems to continually refine and adapt to the evolving situation in streaming data online, which is also our goal. Furthermore, our work in this paper contributes significantly to AIOps research, particularly through the deployment of our intelligent module that cooperates within a monitoring system in production. Our work addresses a crucial gap in the security research landscape toward more practical and effective secure strategies.",Artificial Intelligence,0,,,,"In cybersecurity, live production data for predictive analysis pose a significant challenge due to the inherently secure nature of the domain. Although there are publicly available, synthesized, and artificially generated datasets, authentic scenarios are rarely encountered. For anomaly-based detection, the dynamic definition of thresholds has gained importance and attention in detecting abnormalities and preventing malicious activities. Unlike conventional threshold-based methods, deep learning data modeling provides a more nuanced perspective on network monitoring. This enables security systems to continually refine and adapt to the evolving situation in streaming data online, which is also our goal. Furthermore, our work in this paper contributes significantly to AIOps research, particularly through the deployment of our intelligent module that cooperates within a monitoring system in production. Our work addresses a crucial gap in the security research landscape toward more practical and effective secure strategies.",High
Amir Bidaki,Automatic,Springer,http://link.springer.com/article/10.1007/s10115-007-0072-8,The importance of generalizability for anomaly detection,2008,Type,Knowledge and Information Systems,Core,"In security-related areas there is concern over novel “zero-day” attacks that penetrate system defenses and wreak havoc. The best methods for countering these threats are recognizing “nonself” as in an Artificial Immune System or recognizing “self” through clustering. For either case, the concern remains that something that appears similar to self could be missed. Given this situation, one could incorrectly assume that a preference for a tighter fit to self over generalizability is important for false positive reduction in this type of learning problem. This article confirms that in anomaly detection as in other forms of classification a tight fit, although important, does not supersede model generality. This is shown using three systems each with a different geometric bias in the decision space. The first two use spherical and ellipsoid clusters with a k-means algorithm modified to work on the one-class/blind classification problem. The third is based on wrapping the self points with a multidimensional convex hull (polytope) algorithm capable of learning disjunctive concepts via a thresholding constant. All three of these algorithms are tested using the Voting dataset from the UCI Machine Learning Repository, the MIT Lincoln Labs intrusion detection dataset, and the lossy-compressed steganalysis domain.","Clustering
Anomaly detection
Convex polytope
Ellipsoid",0,,,,"In security-related areas there is concern over novel “zero-day” attacks that penetrate system defenses and wreak havoc. The best methods for countering these threats are recognizing “nonself” as in an Artificial Immune System or recognizing “self” through clustering. For either case, the concern remains that something that appears similar to self could be missed. Given this situation, one could incorrectly assume that a preference for a tighter fit to self over generalizability is important for false positive reduction in this type of learning problem. This article confirms that in anomaly detection as in other forms of classification a tight fit, although important, does not supersede model generality. This is shown using three systems each with a different geometric bias in the decision space. The first two use spherical and ellipsoid clusters with a k-means algorithm modified to work on the one-class/blind classification problem. The third is based on wrapping the self points with a multidimensional convex hull (polytope) algorithm capable of learning disjunctive concepts via a thresholding constant. All three of these algorithms are tested using the Voting dataset from the UCI Machine Learning Repository, the MIT Lincoln Labs intrusion detection dataset, and the lossy-compressed steganalysis domain.",Low
Amir Bidaki,Automatic,Springer,http://link.springer.com/article/10.1007/s12650-019-00580-7,An interactive visual analytics approach for network anomaly detection through smart labeling,2019,Type,Journal of Visualization,Core,"Network anomaly detection is an important means for safeguarding network security. On account of the difficulties encountered in traditional automatic detection methods such as lack of labeled data, expensive retraining costs for new data and non-explanation, we propose a novel smart labeling method, which combines active learning and visual interaction, to detect network anomalies through the iterative labeling process of the users. The algorithms and the visual interfaces are tightly integrated. The network behavior patterns are first learned by using the self-organizing incremental neural network. Then, the model uses a Fuzzy c-means-based algorithm to do classification on the basis of user feedback. After that, the visual interfaces are updated to present the improved results of the model, which can help users to choose meaningful candidates, judge anomalies and understand the model results. The experiments show that compared to labeling without our visualizations, our method can achieve a high accuracy rate of anomaly detection with fewer labeled samples.","Anomaly detection
Interactive labeling
Visual analysis",16 Citations,,,,"Network anomaly detection is an important means for safeguarding network security. On account of the difficulties encountered in traditional automatic detection methods such as lack of labeled data, expensive retraining costs for new data and non-explanation, we propose a novel smart labeling method, which combines active learning and visual interaction, to detect network anomalies through the iterative labeling process of the users. The algorithms and the visual interfaces are tightly integrated. The network behavior patterns are first learned by using the self-organizing incremental neural network. Then, the model uses a Fuzzy c-means-based algorithm to do classification on the basis of user feedback. After that, the visual interfaces are updated to present the improved results of the model, which can help users to choose meaningful candidates, judge anomalies and understand the model results. The experiments show that compared to labeling without our visualizations, our method can achieve a high accuracy rate of anomaly detection with fewer labeled samples.",Medium
Amir Bidaki,Automatic,Springer,http://link.springer.com/article/10.1007/s10586-023-04234-0,Streaming traffic classification: a hybrid deep learning and big data approach,2024,Type,Cluster Computing,Core,"Massive amounts of real-time streaming network data are generated quickly because of the exponential growth of applications. Analyzing patterns in generated flow traffic streaming offers benefits in reducing traffic congestion, enhancing network management, and improving the quality of service management. Processing massive volumes of generated traffic poses more challenges when data traffic encryption is raised. Classifying encrypted network traffic in real-time with deep learning networks has received attention because of their excellent performance. The substantial volume of incoming packets, characterized by high speed and wide variety, puts real-time traffic classification within the domain of big data problems. Classifying traffic with high speed and accuracy is a significant challenge in the era of big data. The real-time nature of traffic intensifies deep learning networks, necessitating a considerable number of parameters, layers, and resources for optimal network training. Until now, various datasets have been employed to evaluate the effectiveness of previous methods for classifying encrypted traffic. The primary objective has been to enhance accuracy, precision, and F1-measure. Presently, encrypted traffic classification performance depends on pre-existing datasets. The learning and testing phases are done offline, and more research is needed to investigate the feasibility of these methods in real-world scenarios. This paper examines the possibility of a tradeoff between evaluating the model’s effectiveness, execution time, and utilization of processing resources when processing stream-based input data for traffic classification. We aim to explore the feasibility of establishing a tradeoff between these factors and determining optimal parameter settings. This paper used the ISCX VPN-Non VPN 2016 public dataset to evaluate the proposed method. All packets from the dataset were streamed continuously through Apache Kafka to the classification framework. Numerous experiments have been designed to demonstrate the efficacy of the proposed method. The experimental results show that the proposed method outperforms the baseline methods by 11% in the F1-measure when the number of workers is two and by 25% when the number of workers is equal to 32.","Deep learning
Encrypted traffic
Streaming network
Traffic classification",0,,,,"Massive amounts of real-time streaming network data are generated quickly because of the exponential growth of applications. Analyzing patterns in generated flow traffic streaming offers benefits in reducing traffic congestion, enhancing network management, and improving the quality of service management. Processing massive volumes of generated traffic poses more challenges when data traffic encryption is raised. Classifying encrypted network traffic in real-time with deep learning networks has received attention because of their excellent performance. The substantial volume of incoming packets, characterized by high speed and wide variety, puts real-time traffic classification within the domain of big data problems. Classifying traffic with high speed and accuracy is a significant challenge in the era of big data. The real-time nature of traffic intensifies deep learning networks, necessitating a considerable number of parameters, layers, and resources for optimal network training. Until now, various datasets have been employed to evaluate the effectiveness of previous methods for classifying encrypted traffic. The primary objective has been to enhance accuracy, precision, and F1-measure. Presently, encrypted traffic classification performance depends on pre-existing datasets. The learning and testing phases are done offline, and more research is needed to investigate the feasibility of these methods in real-world scenarios. This paper examines the possibility of a tradeoff between evaluating the model’s effectiveness, execution time, and utilization of processing resources when processing stream-based input data for traffic classification. We aim to explore the feasibility of establishing a tradeoff between these factors and determining optimal parameter settings. This paper used the ISCX VPN-Non VPN 2016 public dataset to evaluate the proposed method. All packets from the dataset were streamed continuously through Apache Kafka to the classification framework. Numerous experiments have been designed to demonstrate the efficacy of the proposed method. The experimental results show that the proposed method outperforms the baseline methods by 11% in the F1-measure when the number of workers is two and by 25% when the number of workers is equal to 32.",Medium
Amir Bidaki,Automatic,Springer,http://link.springer.com/article/10.1007/s10922-023-09742-3,Mitigating DoS Attack in MANETs Considering Node Reputation with AI,2023,Type,Journal of Network and Systems Management,Core,"Mobile Adhoc Network (MANET) is a decentralized and dynamically adoptable network. It is infrastructure-less and hence can be used where a fixed configuration is not possible or required. MANETs have various real-life applications and hence have gained the attention of the research community. Security is an integral part of any computer network system and MANETs are no different. This paper focuses on solving DoS attacks in MANET and shows that a general classification model might fail to identify this kind of attack as these models fail to differentiate between network errors and a real DoS attack. A reputation-based node classification scheme is proposed to improve the identification of real DoS attacks versus any other cause that might not be an attack. Results showed that our proposed reputation-based approach when integrated with any classifier increases its accuracy by around 3.25%. Further, the combined model can block real DoS attacks and allow any other cause which is not an attack.",Artificial Intelligence,2 Citations,,,,"Mobile Adhoc Network (MANET) is a decentralized and dynamically adoptable network. It is infrastructure-less and hence can be used where a fixed configuration is not possible or required. MANETs have various real-life applications and hence have gained the attention of the research community. Security is an integral part of any computer network system and MANETs are no different. This paper focuses on solving DoS attacks in MANET and shows that a general classification model might fail to identify this kind of attack as these models fail to differentiate between network errors and a real DoS attack. A reputation-based node classification scheme is proposed to improve the identification of real DoS attacks versus any other cause that might not be an attack. Results showed that our proposed reputation-based approach when integrated with any classifier increases its accuracy by around 3.25%. Further, the combined model can block real DoS attacks and allow any other cause which is not an attack.",Low
Amir Bidaki,Automatic,Springer,http://link.springer.com/article/10.1007/s10479-024-06444-0,A fundamental overview of ensemble deep learning models and applications: systematic literature and state of the art,2024,Type,Annals of Operations Research,Core,"The increasing popularity of deep learning (DL) leads to new applications and possibilities, the fast advancement of techniques, and the development of new domains by combining several algorithms. Deep learning and ensemble deep learning (EDL) are widely used strategies that consistently provide exceptional performance across several tasks. EDL is a novel advancement that combines these two approaches to provide a robust framework with significant performance improvements and enhanced generalization capabilities compared to the individual techniques. This paper presents the concept of ensemble deep learning and provides a comprehensive analysis of the utilization of EDL techniques in various application domains like image recognition, sentimental analysis and text classification, cancerous tumor classification, speech, healthcare, fake news and fraud detection forecasting, and other applications. We offer an analysis of various ensemble strategies, including bagging, boosting, stacking, negative correlation-based deep ensemble models, explicit/implicit ensembles, homogeneous/heterogeneous ensembles, and decision fusion techniques-based deep ensemble models. The main objective of this study is to comprehensively evaluate existing works in the area of EDL and highlight the future directions that may be explored further to develop it as a tool for several application domain-related tasks. This first study primarily focuses on ensemble deep learning for smart healthcare and other related domain-specific applications. This study aims to serve as a valuable resource for researchers in academia and industry working with medical data, supporting advanced novel applications of ensemble deep learning models to solve challenges in existing medical decision-processing systems.",Artificial Intelligence,1 Citation,,,,"The increasing popularity of deep learning (DL) leads to new applications and possibilities, the fast advancement of techniques, and the development of new domains by combining several algorithms. Deep learning and ensemble deep learning (EDL) are widely used strategies that consistently provide exceptional performance across several tasks. EDL is a novel advancement that combines these two approaches to provide a robust framework with significant performance improvements and enhanced generalization capabilities compared to the individual techniques. This paper presents the concept of ensemble deep learning and provides a comprehensive analysis of the utilization of EDL techniques in various application domains like image recognition, sentimental analysis and text classification, cancerous tumor classification, speech, healthcare, fake news and fraud detection forecasting, and other applications. We offer an analysis of various ensemble strategies, including bagging, boosting, stacking, negative correlation-based deep ensemble models, explicit/implicit ensembles, homogeneous/heterogeneous ensembles, and decision fusion techniques-based deep ensemble models. The main objective of this study is to comprehensively evaluate existing works in the area of EDL and highlight the future directions that may be explored further to develop it as a tool for several application domain-related tasks. This first study primarily focuses on ensemble deep learning for smart healthcare and other related domain-specific applications. This study aims to serve as a valuable resource for researchers in academia and industry working with medical data, supporting advanced novel applications of ensemble deep learning models to solve challenges in existing medical decision-processing systems.",Low
Amir Bidaki,Automatic,Springer,http://link.springer.com/article/10.1007/s11431-019-9532-5,An artificial immune and incremental learning inspired novel framework for performance pattern identification of complex electromechanical systems,2020,Type,Science China Technological Sciences,Core,"Performance pattern identification is the key basis for fault detection and condition prediction, which plays a major role in ensuring safety and reliability in complex electromechanical systems (CESs). However, there are a few problems related to the automatic and adaptive updating of an identification model. Aiming to solve the problem of identification model updating, a novel framework for performance pattern identification of the CESs based on the artificial immune systems and incremental learning is proposed in this paper to classify real-time monitoring data into different performance patterns. First, an unsupervised clustering technique is used to construct an initial identification model. Second, the artificial immune and outlier detection algorithms are applied to identify abnormal data and determine the type of immune response. Third, incremental learning is employed to trace the dynamic changes of patterns, and operations such as pattern insertion, pattern removal, and pattern revision are designed to realize automatic and adaptive updates of an identification model. The effectiveness of the proposed framework is demonstrated through experiments with the benchmark and actual pattern identification applications. As an unsupervised and self-adapting approach, the proposed framework inherits the preponderances of the conventional methods but overcomes some of their drawbacks because the retraining process is not required in perceiving the pattern changes. Therefore, this method can be flexibly and efficiently used for performance pattern identification of the CESs. Moreover, the proposed method provides a foundation for fault detection and condition prediction, and can be used in other engineering applications.","performance pattern identification
complex electromechanical systems
artificial immune
incremental learning
data classification",0,,,,"Performance pattern identification is the key basis for fault detection and condition prediction, which plays a major role in ensuring safety and reliability in complex electromechanical systems (CESs). However, there are a few problems related to the automatic and adaptive updating of an identification model. Aiming to solve the problem of identification model updating, a novel framework for performance pattern identification of the CESs based on the artificial immune systems and incremental learning is proposed in this paper to classify real-time monitoring data into different performance patterns. First, an unsupervised clustering technique is used to construct an initial identification model. Second, the artificial immune and outlier detection algorithms are applied to identify abnormal data and determine the type of immune response. Third, incremental learning is employed to trace the dynamic changes of patterns, and operations such as pattern insertion, pattern removal, and pattern revision are designed to realize automatic and adaptive updates of an identification model. The effectiveness of the proposed framework is demonstrated through experiments with the benchmark and actual pattern identification applications. As an unsupervised and self-adapting approach, the proposed framework inherits the preponderances of the conventional methods but overcomes some of their drawbacks because the retraining process is not required in perceiving the pattern changes. Therefore, this method can be flexibly and efficiently used for performance pattern identification of the CESs. Moreover, the proposed method provides a foundation for fault detection and condition prediction, and can be used in other engineering applications.",Medium
Amir Bidaki,Automatic,Springer,http://link.springer.com/article/10.1007/s10586-023-04179-4,Multi-stage intrusion detection system aided by grey wolf optimization algorithm,2024,Type,Cluster Computing,Core,"A Network Intrusion Detection System (NIDS) is frequently used for monitoring and detecting malicious activities in network traffic. A typical NIDS has four stages: a data source, data pre-processing, a decision-making technique, and a defense reaction. We have utilized both anomaly and signature based techniques to build a framework which is resilient to identifying both known and unknown attack. The incoming data packet is fed into the Stacked Autoencoder to identify whether it is a benign or malicious. If found to be malicious we extract the most relevant features from the network packet using grey wolf optimization algorithm. Then these attribute are provided to RandomForest classifier to determine if this malign attack is present in our knowledge base. If it is present we progress to identify the attack type using LightGBM classifier. If not, we term it as zero-day attack. To evaluate the usability of the proposed framework we have assessed it using two publicly available datasets namely UNSW-NB15 and CIC-IDS-2017 dataset. We have obtained an accuracy of 90.94% and 99.67% on the datasets respectively.","Computer networks
Intrusion detection system
Stacked autoencoder
Decison trees",1 Citation,,,,"A Network Intrusion Detection System (NIDS) is frequently used for monitoring and detecting malicious activities in network traffic. A typical NIDS has four stages: a data source, data pre-processing, a decision-making technique, and a defense reaction. We have utilized both anomaly and signature based techniques to build a framework which is resilient to identifying both known and unknown attack. The incoming data packet is fed into the Stacked Autoencoder to identify whether it is a benign or malicious. If found to be malicious we extract the most relevant features from the network packet using grey wolf optimization algorithm. Then these attribute are provided to RandomForest classifier to determine if this malign attack is present in our knowledge base. If it is present we progress to identify the attack type using LightGBM classifier. If not, we term it as zero-day attack. To evaluate the usability of the proposed framework we have assessed it using two publicly available datasets namely UNSW-NB15 and CIC-IDS-2017 dataset. We have obtained an accuracy of 90.94% and 99.67% on the datasets respectively.",Low
Amir Bidaki,Automatic,Springer,http://link.springer.com/article/10.1007/s42979-024-02983-2,Enhancing Drone Security Through Multi-Sensor Anomaly Detection and Machine Learning,2024,Type,SN Computer Science,Core,"Unmanned Aerial Vehicles (UAVs), commonly referred to as drones, have determined numerous applications across industries, ranging from aerial surveillance to package shipping. As drones are used in vital operations, ensuring cyberattacks and anomalies protect them from attackers is now a big challenge. This research study presents a complete approach to enhancing drone safety by integrating multi-sensor anomaly detection and superior machine learning techniques. The proposed methodology capitalizes on the rich sensor suite embedded in present-day drones, encompassing GPS receivers, accelerometers, gyroscopes, cameras, communication modules, and more. Leveraging an array of sensors in drones, our technique detects abnormal drone behavior indicative of unauthorized access, GPS spoofing, communication jamming, and malicious activities. By extracting features from sensor records, we develop a robust anomaly detection framework using the “uav attack dataset” able to identify deviations from normal flight patterns, communication signals, and environmental interactions. Central to our methodology is the utilization of machine learning algorithms. These algorithms are skilled on labeled datasets containing numerous flight eventualities, each normal and hostile, together with the ones discovered inside the “uav attack dataset”. The obtained results are eventually evaluated using rigorous performance metrics to quantify their effectiveness in distinguishing genuine anomalies from benign variations. The findings of our study underscore the capacity of multi-sensor anomaly detection for drones. By harnessing the power of machine learning and sensor fusion, we exhibit the ability to hit upon attacks at an early level, mitigating capability harm and permitting rapid responses. This study contributes now not only to the field of drone safety but also to the broader panorama of self-sustaining systems protection, highlighting the importance of adaptive and proactive protection mechanisms. Results show that an accuracy of 99% with AUC of 100$ was achieved when all the sensors were used.","Drone Security
Multi-Sensor Anomaly Detection
Machine Learning for Drones
Anomaly Detection
Sensor Fusion",0,,,,"Unmanned Aerial Vehicles (UAVs), commonly referred to as drones, have determined numerous applications across industries, ranging from aerial surveillance to package shipping. As drones are used in vital operations, ensuring cyberattacks and anomalies protect them from attackers is now a big challenge. This research study presents a complete approach to enhancing drone safety by integrating multi-sensor anomaly detection and superior machine learning techniques. The proposed methodology capitalizes on the rich sensor suite embedded in present-day drones, encompassing GPS receivers, accelerometers, gyroscopes, cameras, communication modules, and more. Leveraging an array of sensors in drones, our technique detects abnormal drone behavior indicative of unauthorized access, GPS spoofing, communication jamming, and malicious activities. By extracting features from sensor records, we develop a robust anomaly detection framework using the “uav attack dataset” able to identify deviations from normal flight patterns, communication signals, and environmental interactions. Central to our methodology is the utilization of machine learning algorithms. These algorithms are skilled on labeled datasets containing numerous flight eventualities, each normal and hostile, together with the ones discovered inside the “uav attack dataset”. The obtained results are eventually evaluated using rigorous performance metrics to quantify their effectiveness in distinguishing genuine anomalies from benign variations. The findings of our study underscore the capacity of multi-sensor anomaly detection for drones. By harnessing the power of machine learning and sensor fusion, we exhibit the ability to hit upon attacks at an early level, mitigating capability harm and permitting rapid responses. This study contributes now not only to the field of drone safety but also to the broader panorama of self-sustaining systems protection, highlighting the importance of adaptive and proactive protection mechanisms. Results show that an accuracy of 99% with AUC of 100$ was achieved when all the sensors were used.",Low
Amir Bidaki,Automatic,Springer,http://link.springer.com/article/10.1007/s42979-024-03597-4,Real-Time Intrusion Detection in IIoT Stream Data Using Window-Based Weighted Ensemble Techniques,2025,Type,SN Computer Science,Core,"The Industrial Internet of Things (IIoT) is a fast-expanding field of technology that radically transforms the industrial environment into an automated one. Network stream data offers a constant stream of real-time data from numerous sensors and devices, which is essential in IIoT systems. Assailants can more readily access network stream data when there is network automation, making network data collection more susceptible. For effective data analytics, we have to identify the counteract cyber threats. To gain relevant insights from this data, Intrusion Detection Systems (IDS) are required. To address this issue, the Automated Intrusion Detection Framework (AIDF)” is developed for network drift adaption in IIoT systems. This framework has a Window-based Weighted Ensemble (WWE) model with optimized feature selection using Whale Optimization. The effectiveness of the suggested framework for real-time network intrusion detection is evaluated using a dataset from both the real world and static. The proposed framework outperforms well compared to the state-of-the-art methods. This is applicable in several sectors, including banking, healthcare, and transportation, which may use the suggested framework to improve their cyber security posture and protect themselves from online threats.","Automated intrusion detection systems
Drift
Drift-based Dynamic Feature Selection
Intrusion detection system
Industrial IoT
Whale optimization
Window-based weighted ensemble",0,,,,"The Industrial Internet of Things (IIoT) is a fast-expanding field of technology that radically transforms the industrial environment into an automated one. Network stream data offers a constant stream of real-time data from numerous sensors and devices, which is essential in IIoT systems. Assailants can more readily access network stream data when there is network automation, making network data collection more susceptible. For effective data analytics, we have to identify the counteract cyber threats. To gain relevant insights from this data, Intrusion Detection Systems (IDS) are required. To address this issue, the Automated Intrusion Detection Framework (AIDF)” is developed for network drift adaption in IIoT systems. This framework has a Window-based Weighted Ensemble (WWE) model with optimized feature selection using Whale Optimization. The effectiveness of the suggested framework for real-time network intrusion detection is evaluated using a dataset from both the real world and static. The proposed framework outperforms well compared to the state-of-the-art methods. This is applicable in several sectors, including banking, healthcare, and transportation, which may use the suggested framework to improve their cyber security posture and protect themselves from online threats.",Medium
Amir Bidaki,Automatic,Springer,http://link.springer.com/article/10.1007/s10586-021-03249-9,Distributed anomaly detection using concept drift detection based hybrid ensemble techniques in streamed network data,2021,Type,Cluster Computing,Core,"Ever since the internet became part of the everyday lives of humans providing network security has been considered of utmost importance. Over the years lot of time and energy has been devoted by people in the research community and industry to provide better, improved and secure mechanisms to ensure secure communications on the internet. Amongst the many fields of study, the most prominent and ever evolving one has been the study of network traffic for attack detection and mitigation. The advent of new technologies has led to an increase in the pace of network based attacks and therefore novel modified approaches are needed to be able to cope with these latest trends. Distributed machine learning with the development of new tools and frameworks like RDD structure in Apache Spark provides an immense scope of growth in this direction. Moreover, the dynamic nature of present day network traffic called concept drift has also necessitated studying solutions from a different angle. We, therefore, in this paper have worked on distributed machine learning based ensemble techniques to detect the presence of concept drift in network traffic and detect network based attacks. The work has been done in three parts. Firstly, two classifiers, namely, Random Forest and Logistic Regression have been used as level ‘0′ learners and Support Vector Machine has been used as level ‘1′ learner. Secondly, to handle the process of concept drift we have used a sliding window based K-means clustering. And thirdly ensemble based techniques for detection of attacks in the traffic. The experiments have been performed on three datasets, namely, the NSL-KDD dataset, the CIDDS-2017 dataset and generated Testbed dataset. These tests have been conducted on different machines by varying the number of executor cores to study time latency in a distributed environment. An accuracy of 93% on NSL-KDD, 98% on CIDDS-2017 and 97% on Testbed datasets for SVM based blending model have been achieved.","Distributed
Concept drift
Ensemble
Streaming",0,,,,"Ever since the internet became part of the everyday lives of humans providing network security has been considered of utmost importance. Over the years lot of time and energy has been devoted by people in the research community and industry to provide better, improved and secure mechanisms to ensure secure communications on the internet. Amongst the many fields of study, the most prominent and ever evolving one has been the study of network traffic for attack detection and mitigation. The advent of new technologies has led to an increase in the pace of network based attacks and therefore novel modified approaches are needed to be able to cope with these latest trends. Distributed machine learning with the development of new tools and frameworks like RDD structure in Apache Spark provides an immense scope of growth in this direction. Moreover, the dynamic nature of present day network traffic called concept drift has also necessitated studying solutions from a different angle. We, therefore, in this paper have worked on distributed machine learning based ensemble techniques to detect the presence of concept drift in network traffic and detect network based attacks. The work has been done in three parts. Firstly, two classifiers, namely, Random Forest and Logistic Regression have been used as level ‘0′ learners and Support Vector Machine has been used as level ‘1′ learner. Secondly, to handle the process of concept drift we have used a sliding window based K-means clustering. And thirdly ensemble based techniques for detection of attacks in the traffic. The experiments have been performed on three datasets, namely, the NSL-KDD dataset, the CIDDS-2017 dataset and generated Testbed dataset. These tests have been conducted on different machines by varying the number of executor cores to study time latency in a distributed environment. An accuracy of 93% on NSL-KDD, 98% on CIDDS-2017 and 97% on Testbed datasets for SVM based blending model have been achieved.",Medium
Amir Bidaki,Automatic,Springer,http://link.springer.com/article/10.1186/s13174-018-0087-2,"A comprehensive survey on machine learning for networking: evolution, applications and research opportunities",2018,Type,Journal of Internet Services and Applications,Core,"Machine Learning (ML) has been enjoying an unprecedented surge in applications that solve problems and enable automation in diverse domains. Primarily, this is due to the explosion in the availability of data, significant improvements in ML techniques, and advancement in computing capabilities. Undoubtedly, ML has been applied to various mundane and complex problems arising in network operation and management. There are various surveys on ML for specific areas in networking or for specific network technologies. This survey is original, since it jointly presents the application of diverse ML techniques in various key areas of networking across different network technologies. In this way, readers will benefit from a comprehensive discussion on the different learning paradigms and ML techniques applied to fundamental problems in networking, including traffic prediction, routing and classification, congestion control, resource and fault management, QoS and QoE management, and network security. Furthermore, this survey delineates the limitations, give insights, research challenges and future opportunities to advance ML in networking. Therefore, this is a timely contribution of the implications of ML for networking, that is pushing the barriers of autonomic network operation and management.","Machine learning
Traffic prediction
Traffic classification
Traffic routing
Congestion control
Resource management
Fault management
QoS and QoE management
Network security",0,,,,"Machine Learning (ML) has been enjoying an unprecedented surge in applications that solve problems and enable automation in diverse domains. Primarily, this is due to the explosion in the availability of data, significant improvements in ML techniques, and advancement in computing capabilities. Undoubtedly, ML has been applied to various mundane and complex problems arising in network operation and management. There are various surveys on ML for specific areas in networking or for specific network technologies. This survey is original, since it jointly presents the application of diverse ML techniques in various key areas of networking across different network technologies. In this way, readers will benefit from a comprehensive discussion on the different learning paradigms and ML techniques applied to fundamental problems in networking, including traffic prediction, routing and classification, congestion control, resource and fault management, QoS and QoE management, and network security. Furthermore, this survey delineates the limitations, give insights, research challenges and future opportunities to advance ML in networking. Therefore, this is a timely contribution of the implications of ML for networking, that is pushing the barriers of autonomic network operation and management.",Low
Amir Bidaki,Automatic,Springer,http://link.springer.com/article/10.1186/s40537-015-0036-x,Data stream clustering by divide and conquer approach based on vector model,2016,Type,Journal of Big Data,Core,"Recently, many researchers have focused on data stream processing as an efficient method for extracting knowledge from big data. Data stream clustering is an unsupervised approach that is employed for huge data. The continuous effort on data stream clustering method has one common goal which is to achieve an accurate clustering algorithm. However, there are some issues that are overlooked by the previous works in proposing data stream clustering solutions; (1) clustering dataset including big segments of repetitive data, (2) monitoring clustering structure for ordinal data streams and (3) determining important parameters such as k number of exact clusters in stream of data. In this paper, DCSTREAM method is proposed with regard to the mentioned issues to cluster big datasets using the vector model and k-Means divide and conquer approach. Experimental results show that DCSTREAM can achieve superior quality and performance as compare to STREAM and ConStream methods for abrupt and gradual real world datasets. Results show that the usage of batch processing in DCSTREAM and ConStream is time consuming compared to STREAM but it avoids further analysis for detecting outliers and novel micro-clusters.","Data mining
Data stream clustering
Vector space model
Divide-and-conquer",41 Citations,,,,"Recently, many researchers have focused on data stream processing as an efficient method for extracting knowledge from big data. Data stream clustering is an unsupervised approach that is employed for huge data. The continuous effort on data stream clustering method has one common goal which is to achieve an accurate clustering algorithm. However, there are some issues that are overlooked by the previous works in proposing data stream clustering solutions; (1) clustering dataset including big segments of repetitive data, (2) monitoring clustering structure for ordinal data streams and (3) determining important parameters such as k number of exact clusters in stream of data. In this paper, DCSTREAM method is proposed with regard to the mentioned issues to cluster big datasets using the vector model and k-Means divide and conquer approach. Experimental results show that DCSTREAM can achieve superior quality and performance as compare to STREAM and ConStream methods for abrupt and gradual real world datasets. Results show that the usage of batch processing in DCSTREAM and ConStream is time consuming compared to STREAM but it avoids further analysis for detecting outliers and novel micro-clusters.",Low
Amir Bidaki,Automatic,Springer,http://link.springer.com/article/10.1186/s13635-023-00141-4,Network traffic classification model based on attention mechanism and spatiotemporal features,2023,Type,EURASIP Journal on Information Security,Core,"Traffic classification is widely used in network security and network management. Early studies have mainly focused on mapping network traffic to different unencrypted applications, but little research has been done on network traffic classification of encrypted applications, especially the underlying traffic of encrypted applications. To address the above issues, this paper proposes a network encryption traffic classification model that combines attention mechanisms and spatiotemporal features. The model firstly uses the long short-term memory (LSTM) method to analyze continuous network flows and find the temporal correlation features between these network flows. Secondly, the convolutional neural network (CNN) method is used to extract the high-order spatial features of the network flow, and then, the squeeze and excitation (SE) module is used to weight and redistribute the high-order spatial features to obtain the key spatial features of the network flow. Finally, through the above three stages of training and learning, fast classification of network flows is achieved. The main advantages of this model are as follows: (1) the mapping relationship between network flow and label is automatically constructed by the model without manual intervention and decision by network features, (2) it has strong generalization ability and can quickly adapt to different network traffic datasets, and (3) it can handle encrypted applications and their underlying traffic with high accuracy. The experimental results show that the model can be applied to classify network traffic of encrypted and unencrypted applications at the same time, especially the classification accuracy of the underlying traffic of encrypted applications is improved. In most cases, the accuracy generally exceeds 90%.","Traffic classification
CNN
LSTM
Attention mechanism",19 Citations,,,,"Traffic classification is widely used in network security and network management. Early studies have mainly focused on mapping network traffic to different unencrypted applications, but little research has been done on network traffic classification of encrypted applications, especially the underlying traffic of encrypted applications. To address the above issues, this paper proposes a network encryption traffic classification model that combines attention mechanisms and spatiotemporal features. The model firstly uses the long short-term memory (LSTM) method to analyze continuous network flows and find the temporal correlation features between these network flows. Secondly, the convolutional neural network (CNN) method is used to extract the high-order spatial features of the network flow, and then, the squeeze and excitation (SE) module is used to weight and redistribute the high-order spatial features to obtain the key spatial features of the network flow. Finally, through the above three stages of training and learning, fast classification of network flows is achieved. The main advantages of this model are as follows: (1) the mapping relationship between network flow and label is automatically constructed by the model without manual intervention and decision by network features, (2) it has strong generalization ability and can quickly adapt to different network traffic datasets, and (3) it can handle encrypted applications and their underlying traffic with high accuracy. The experimental results show that the model can be applied to classify network traffic of encrypted and unencrypted applications at the same time, especially the classification accuracy of the underlying traffic of encrypted applications is improved. In most cases, the accuracy generally exceeds 90%.",Low
Amir Bidaki,Automatic,Springer,http://link.springer.com/article/10.1007/s10115-010-0366-0,A segment-based framework for modeling and mining data streams,2012,Type,Knowledge and Information Systems,Core,"Data Streams have become ubiquitous in recent years because of advances in hardware technology which have enabled automated recording of large amounts of data. The primary constraint in the effective mining of streams is the large volume of data which must be processed in real time. In many cases, it is desirable to store a summary of the data stream segments in order to perform data mining tasks. Since density estimation provides a comprehensive overview of the probabilistic data distribution of a stream segment, it is a natural choice for this purpose. A direct use of density distributions can however turn out to be an inefficient storage and processing mechanism in practice. In this paper, we introduce the concept of cluster histograms, which provides an efficient way to estimate and summarize the most important data distribution profiles over different stream segments. These profiles can be constructed in a supervised or unsupervised way depending upon the nature of the underlying application. The profiles can also be used for change detection, anomaly detection, segmental nearest neighbor search, or supervised stream segment classification. Furthermore, these techniques can also be used for modeling other kinds of data such as text and categorical data. The flexibility of the tasks which can be performed from the cluster histogram framework follows from its generality in storing the historical density profile of the data stream. As a result, this method provides a holistic framework for density-based mining of data streams. We discuss and test the application of the cluster histogram framework to a variety of interesting data mining applications.","Clustering
Stream mining",0,,,,"Data Streams have become ubiquitous in recent years because of advances in hardware technology which have enabled automated recording of large amounts of data. The primary constraint in the effective mining of streams is the large volume of data which must be processed in real time. In many cases, it is desirable to store a summary of the data stream segments in order to perform data mining tasks. Since density estimation provides a comprehensive overview of the probabilistic data distribution of a stream segment, it is a natural choice for this purpose. A direct use of density distributions can however turn out to be an inefficient storage and processing mechanism in practice. In this paper, we introduce the concept of cluster histograms, which provides an efficient way to estimate and summarize the most important data distribution profiles over different stream segments. These profiles can be constructed in a supervised or unsupervised way depending upon the nature of the underlying application. The profiles can also be used for change detection, anomaly detection, segmental nearest neighbor search, or supervised stream segment classification. Furthermore, these techniques can also be used for modeling other kinds of data such as text and categorical data. The flexibility of the tasks which can be performed from the cluster histogram framework follows from its generality in storing the historical density profile of the data stream. As a result, this method provides a holistic framework for density-based mining of data streams. We discuss and test the application of the cluster histogram framework to a variety of interesting data mining applications.",Low
Amir Bidaki,Automatic,Springer,http://link.springer.com/article/10.1023/B%3AJONS.0000015698.32353.61,Detecting Network Attacks in the Internet via Statistical Network Traffic Normality Prediction,2004,Type,Journal of Network and Systems Management,Core,"The information technology advances that provide new capabilities to the network users and providers, also provide powerful new tools for network intruders that intend to launch attacks on critical information resources. In this paper we present a novel network attack diagnostic methodology, based on the characterization of the dynamic statistical properties of normal network traffic. The ability to detect network anomalies and attacks as unacceptable when significant deviations from the expected behavior occurs. Specifically, to provide an accurate identification of the normal network traffic behavior, we first develop an anomaly-tolerant nonstationary traffic prediction technique that is capable of removing both single pulse and continuous anomalies. Furthermore, we introduce and design dynamic thresholds, where we define adaptive anomaly violation conditions as a combined function of both magnitude and duration of the traffic deviations. Finally numerical results are presented that demonstrate the operational effectiveness and efficiency of the proposed approach under the presence of different attacks, such as mail-bombing attacks and UDP flooding attacks.","Intrusion Detection
Anomaly-Tolerance
Dynamic Threshold
Network Anomalies",0,,,,"The information technology advances that provide new capabilities to the network users and providers, also provide powerful new tools for network intruders that intend to launch attacks on critical information resources. In this paper we present a novel network attack diagnostic methodology, based on the characterization of the dynamic statistical properties of normal network traffic. The ability to detect network anomalies and attacks as unacceptable when significant deviations from the expected behavior occurs. Specifically, to provide an accurate identification of the normal network traffic behavior, we first develop an anomaly-tolerant nonstationary traffic prediction technique that is capable of removing both single pulse and continuous anomalies. Furthermore, we introduce and design dynamic thresholds, where we define adaptive anomaly violation conditions as a combined function of both magnitude and duration of the traffic deviations. Finally numerical results are presented that demonstrate the operational effectiveness and efficiency of the proposed approach under the presence of different attacks, such as mail-bombing attacks and UDP flooding attacks.",Low
Amir Bidaki,Automatic,Springer,http://link.springer.com/article/10.1186/s40537-024-01048-8,A systematic review of AI-enhanced techniques in credit card fraud detection,2025,Type,Journal of Big Data,Core,"The rapid increase of fraud attacks on banking systems, financial institutions, and even credit card holders demonstrate the high demand for enhanced fraud detection (FD) systems for these attacks. This paper provides a systematic review of enhanced techniques using Artificial Intelligence (AI), machine learning (ML), deep learning (DL), and meta-heuristic optimization (MHO) algorithms for credit card fraud detection (CCFD). Carefully selected recent research papers have been investigated to examine the effectiveness of these AI-integrated approaches in recognizing a wide range of fraud attacks. These AI techniques were evaluated and compared to discover the advantages and disadvantages of each one, leading to the exploration of existing limitations of ML or DL-enhanced models. Discovering the limitation is crucial for future work and research to increase the effectiveness and robustness of various AI models. The key finding from this study demonstrates the need for continuous development of AI models that could be alert to the latest fraudulent activities.",Artificial Intelligence,0,,,,"The rapid increase of fraud attacks on banking systems, financial institutions, and even credit card holders demonstrate the high demand for enhanced fraud detection (FD) systems for these attacks. This paper provides a systematic review of enhanced techniques using Artificial Intelligence (AI), machine learning (ML), deep learning (DL), and meta-heuristic optimization (MHO) algorithms for credit card fraud detection (CCFD). Carefully selected recent research papers have been investigated to examine the effectiveness of these AI-integrated approaches in recognizing a wide range of fraud attacks. These AI techniques were evaluated and compared to discover the advantages and disadvantages of each one, leading to the exploration of existing limitations of ML or DL-enhanced models. Discovering the limitation is crucial for future work and research to increase the effectiveness and robustness of various AI models. The key finding from this study demonstrates the need for continuous development of AI models that could be alert to the latest fraudulent activities.",Medium
Amir Bidaki,Automatic,Springer,http://link.springer.com/article/10.1007/s10586-024-04483-7,Robust intrusion detection for network communication on the Internet of Things: a hybrid machine learning approach,2024,Type,Cluster Computing,Core,"The importance and growth of the Internet of Things (IoT) in computer networks and applications have been increasing. Additionally, many of these applications generate large volumes of data, which are critical and require protection against attacks. Various techniques have been proposed to identify and counteract these threats. In this paper, we offer a hybrid machine learning approach (using the k-nearest neighbors and random forests as supervised classifiers) to enhance the accuracy of intrusion detection systems and minimize the risk of potential attacks. Also, we employ backward elimination and linear discriminant analysis algorithms for feature reduction and to lower computational costs. Following the training phase, when discrepancies arose between the decisions of the classifiers, the ultimate determination was supported by ISO/IEC 27001 regulations. The performance of the proposed model was assessed within a Python programming framework, utilizing the CICIDS 2017, NSL-KDD, and TON-IoT datasets. The outcomes illustrated that the proposed approach attained a noteworthy accuracy of 99.96% in the multi-class classification of CICIDS 2017, 99.37% in the binary classification of the NSL-KDD dataset, and 99.96% in the multi-class classification of TON-IoT dataset. Furthermore, the attack success rate for each dataset stands at 0.05%, 0.24%, and 0% respectively, demonstrating a significant reduction compared to other methods.","Machine learning
Intrusion detection
IoT
Network communication
Supervised learning",0,,,,"The importance and growth of the Internet of Things (IoT) in computer networks and applications have been increasing. Additionally, many of these applications generate large volumes of data, which are critical and require protection against attacks. Various techniques have been proposed to identify and counteract these threats. In this paper, we offer a hybrid machine learning approach (using the k-nearest neighbors and random forests as supervised classifiers) to enhance the accuracy of intrusion detection systems and minimize the risk of potential attacks. Also, we employ backward elimination and linear discriminant analysis algorithms for feature reduction and to lower computational costs. Following the training phase, when discrepancies arose between the decisions of the classifiers, the ultimate determination was supported by ISO/IEC 27001 regulations. The performance of the proposed model was assessed within a Python programming framework, utilizing the CICIDS 2017, NSL-KDD, and TON-IoT datasets. The outcomes illustrated that the proposed approach attained a noteworthy accuracy of 99.96% in the multi-class classification of CICIDS 2017, 99.37% in the binary classification of the NSL-KDD dataset, and 99.96% in the multi-class classification of TON-IoT dataset. Furthermore, the attack success rate for each dataset stands at 0.05%, 0.24%, and 0% respectively, demonstrating a significant reduction compared to other methods.",Low
Amir Bidaki,Automatic,Springer,http://link.springer.com/article/10.1038/s41598-024-54438-6,Ensemble averaging deep neural network for botnet detection in heterogeneous Internet of Things devices,2024,Type,Scientific Reports,Core,"The botnet attack is one of the coordinated attack types that can infect Internet of Things (IoT) devices and cause them to malfunction. Botnets can steal sensitive information from IoT devices and control them to launch another attack, such as a Distributed Denial-of-Service (DDoS) attack or email spam. This attack is commonly detected using a network-based Intrusion Detection System (NIDS) that monitors the network device’s activity. However, IoT network is dynamic and IoT devices have many types with different configurations and vendors in IoT environments. Therefore, this research proposes an Intrusion Detection System (IDS) by ensemble-ing traffic from heterogeneous IoT devices. This research proposes Deep Neural Network (DNN) to create a training model from each heterogeneous IoT device. After that, each training model from each heterogeneous IoT device is used to predict the traffic. The prediction results from each training model are averaged using the ensemble averaging method to determine the final result. This research used the N-BaIoT dataset to validate the proposed IDS model. Based on experimental results, ensemble averaging DNN can detect botnet attacks in heterogeneous IoT devices with an average accuracy of 97.21, precision of 91.41, recall of 87.31, and F1-score 88.48.",Artificial Intelligence,6 Citations,,,,"The botnet attack is one of the coordinated attack types that can infect Internet of Things (IoT) devices and cause them to malfunction. Botnets can steal sensitive information from IoT devices and control them to launch another attack, such as a Distributed Denial-of-Service (DDoS) attack or email spam. This attack is commonly detected using a network-based Intrusion Detection System (NIDS) that monitors the network device’s activity. However, IoT network is dynamic and IoT devices have many types with different configurations and vendors in IoT environments. Therefore, this research proposes an Intrusion Detection System (IDS) by ensemble-ing traffic from heterogeneous IoT devices. This research proposes Deep Neural Network (DNN) to create a training model from each heterogeneous IoT device. After that, each training model from each heterogeneous IoT device is used to predict the traffic. The prediction results from each training model are averaged using the ensemble averaging method to determine the final result. This research used the N-BaIoT dataset to validate the proposed IDS model. Based on experimental results, ensemble averaging DNN can detect botnet attacks in heterogeneous IoT devices with an average accuracy of 97.21, precision of 91.41, recall of 87.31, and F1-score 88.48.",Low
Amir Bidaki,Automatic,Springer,http://link.springer.com/article/10.1007/s10922-024-09801-3,SA-O2DCA: Seasonal Adapted Online Outlier Detection and Classification Approach for WSN,2024,Type,Journal of Network and Systems Management,Core,"Wireless Sensor Networks (WSNs) play a critical role in the Internet of Things by collecting information for real-world applications such as healthcare, agriculture, and smart cities. These networks consist of low-resource sensors that produce streaming data requiring online processing. However, since data outliers can occur, it’s important to identify and classify them as errors or events using outlier detection and classification techniques. In this paper, we propose a new and enhanced approach for online outlier detection and classification in WSNs. Our approach is titled SA-O2DCA for Seasonal Adapted Online Outlier Detection and Classification Approach. SA-O2DCA, combines the benefits of the K-means algorithm for clustering, the Iforest algorithm for outlier detection and the Newton interpolation to classify the outliers. We evaluate our approach against other works in literature using multivariate datasets. The simulation results, which encompass the assessment of our proposed approach using a combination of synthetic and real-life multivariate datasets, reveal that SA-O2DCA is stable with fewer training models number and outperforms other works on various metrics, including Detection Rate, False Alarm Rate, and Accuracy Rate. Furthermore, our enhanced approach is suitable for working with seasonal real-life applications as it can dynamically change the Training Model at the end of each season period.","Outlier classification
Online
WSN
Training model (TM)
Detection rate (DR)
False Alarm Rate (FAR)",0,,,,"Wireless Sensor Networks (WSNs) play a critical role in the Internet of Things by collecting information for real-world applications such as healthcare, agriculture, and smart cities. These networks consist of low-resource sensors that produce streaming data requiring online processing. However, since data outliers can occur, it’s important to identify and classify them as errors or events using outlier detection and classification techniques. In this paper, we propose a new and enhanced approach for online outlier detection and classification in WSNs. Our approach is titled SA-O2DCA for Seasonal Adapted Online Outlier Detection and Classification Approach. SA-O2DCA, combines the benefits of the K-means algorithm for clustering, the Iforest algorithm for outlier detection and the Newton interpolation to classify the outliers. We evaluate our approach against other works in literature using multivariate datasets. The simulation results, which encompass the assessment of our proposed approach using a combination of synthetic and real-life multivariate datasets, reveal that SA-O2DCA is stable with fewer training models number and outperforms other works on various metrics, including Detection Rate, False Alarm Rate, and Accuracy Rate. Furthermore, our enhanced approach is suitable for working with seasonal real-life applications as it can dynamically change the Training Model at the end of each season period.",Medium
Amir Bidaki,Automatic,Springer,http://link.springer.com/article/10.1007/s10586-024-04444-0,SFC-NIDS: a sustainable and explainable flow filtering based concept drift-driven security approach for network introspection,2024,Type,Cluster Computing,Core,"The evolving behavior of the attacks may affect the decision boundaries of the trained machine learning models. The issue has not been well investigated, especially with hypervisor-based security solutions where virtual machine (VM)’s network artifacts are introspected and analyzed. In this paper, we proposed a sustainable and explainable flow-filtering-based concept drift-driven network intrusion detection approach, called ‘SFC-NIDS’ which introspects network activities by analyzing VM traffic profile. The VM traffic is captured and pre-processed at the hypervisor to extract important network artifacts. The redundant and trivial network flows have been filtered using the proposed gradient descent-based flow filtering mechanism and validated using explainability. SFC-NIDS employs auto-encoders to reconstruct the traffic features to capture additional patterns. Afterward, the 1D-convolution neural network has been employed to learn and detect malicious attack flows. The model’s sustainability is ensured by integrating the drift detection mechanism with the decision model to retrain it with evolving attack patterns. The approach has been validated with virtual network traffic artifacts collected at the hypervisor and provides 98.9% accuracy, 99.03%, and F1-Score. In addition, the approach has also been validated using the KDD99 dataset, showcasing an accuracy of 99.97% and an F1-Score of 99.98%.","Concept drift
Hypervisor
Intrusion detection system
Network attack detection
Deep neural network
Network introspection
Gradient descent",1 Citation,,,,"The evolving behavior of the attacks may affect the decision boundaries of the trained machine learning models. The issue has not been well investigated, especially with hypervisor-based security solutions where virtual machine (VM)’s network artifacts are introspected and analyzed. In this paper, we proposed a sustainable and explainable flow-filtering-based concept drift-driven network intrusion detection approach, called ‘SFC-NIDS’ which introspects network activities by analyzing VM traffic profile. The VM traffic is captured and pre-processed at the hypervisor to extract important network artifacts. The redundant and trivial network flows have been filtered using the proposed gradient descent-based flow filtering mechanism and validated using explainability. SFC-NIDS employs auto-encoders to reconstruct the traffic features to capture additional patterns. Afterward, the 1D-convolution neural network has been employed to learn and detect malicious attack flows. The model’s sustainability is ensured by integrating the drift detection mechanism with the decision model to retrain it with evolving attack patterns. The approach has been validated with virtual network traffic artifacts collected at the hypervisor and provides 98.9% accuracy, 99.03%, and F1-Score. In addition, the approach has also been validated using the KDD99 dataset, showcasing an accuracy of 99.97% and an F1-Score of 99.98%.",Medium
Amir Bidaki,Automatic,Springer,http://link.springer.com/article/10.1007/s13369-024-08929-3,CDA-PDDWE: Concept Drift-Aware Performance-Based Diversified Dynamic Weighted Ensemble for Non-stationary Environments,2024,Type,Arabian Journal for Science and Engineering,Core,"Over the past decades, technological advancements have included the production of a huge number of data streams. Data streams comprise large amounts of partially sequenced, infinite data. Changes in the statistical properties of the input data distribution, such as mean, variance and standard deviation or changes in their relationships with the target label, are referred to as concept drift. Concept drift and class imbalance poses challenges in maintaining accurate classification, adapting to evolving data patterns and effectively classifying minority classes. Addressing this problem requires techniques that handle both class imbalance and concept drift. If such problems are left unaddressed, they will hinder the learning model’s performance. The proposed model in the paper uses adaptive synthetic sampling (ADASYN) to deal with class imbalances. The ADASYN method generates synthetic data by using a weighted distribution based on the severity of the minority class and hard to learn minority class samples. To adapt to concept drift, the performance-based diversified dynamic weighted ensemble is used subsequently. In addition, the cumulative sum statistical test is used to detect drift and, one of the ensemble’s base learners, the self-organizing neural network model, which automatically creates a new layer when drift occurs and provide solution to catastrophic forgetting, performance-based pruning and ensemble evolution if more drift occurs. The proposed model’s efficacy is assessed by utilizing a variety of state-of-the-art ensemble methods and seven datasets in a prequential test-then-train approach with single-pass learning. The results of the experiments show that the proposed model outperforms state-of-the-art ensemble methods.","Non-stationary data streams
Concept drift
Data distribution
Class imbalance
Ensemble method
Neural network",0,,,,"Over the past decades, technological advancements have included the production of a huge number of data streams. Data streams comprise large amounts of partially sequenced, infinite data. Changes in the statistical properties of the input data distribution, such as mean, variance and standard deviation or changes in their relationships with the target label, are referred to as concept drift. Concept drift and class imbalance poses challenges in maintaining accurate classification, adapting to evolving data patterns and effectively classifying minority classes. Addressing this problem requires techniques that handle both class imbalance and concept drift. If such problems are left unaddressed, they will hinder the learning model’s performance. The proposed model in the paper uses adaptive synthetic sampling (ADASYN) to deal with class imbalances. The ADASYN method generates synthetic data by using a weighted distribution based on the severity of the minority class and hard to learn minority class samples. To adapt to concept drift, the performance-based diversified dynamic weighted ensemble is used subsequently. In addition, the cumulative sum statistical test is used to detect drift and, one of the ensemble’s base learners, the self-organizing neural network model, which automatically creates a new layer when drift occurs and provide solution to catastrophic forgetting, performance-based pruning and ensemble evolution if more drift occurs. The proposed model’s efficacy is assessed by utilizing a variety of state-of-the-art ensemble methods and seven datasets in a prequential test-then-train approach with single-pass learning. The results of the experiments show that the proposed model outperforms state-of-the-art ensemble methods.",Medium
Amir Bidaki,Automatic,Springer,http://link.springer.com/article/10.1186/s40537-021-00555-2,Programming big data analysis: principles and solutions,2022,Type,Journal of Big Data,Core,"In the age of the Internet of Things and social media platforms, huge amounts of digital data are generated by and collected from many sources, including sensors, mobile devices, wearable trackers and security cameras. This data, commonly referred to as Big Data, is challenging current storage, processing, and analysis capabilities. New models, languages, systems and algorithms continue to be developed to effectively collect, store, analyze and learn from Big Data. Most of the recent surveys provide a global analysis of the tools that are used in the main phases of Big Data management (generation, acquisition, storage, querying and visualization of data). Differently, this work analyzes and reviews parallel and distributed paradigms, languages and systems used today to analyze and learn from Big Data on scalable computers. In particular, we provide an in-depth analysis of the properties of the main parallel programming paradigms (MapReduce, workflow, BSP, message passing, and SQL-like) and, through programming examples, we describe the most used systems for Big Data analysis (e.g., Hadoop, Spark, and Storm). Furthermore, we discuss and compare the different systems by highlighting the main features of each of them, their diffusion (community of developers and users) and the main advantages and disadvantages of using them to implement Big Data analysis applications. The final goal of this work is to help designers and developers in identifying and selecting the best/appropriate programming solution based on their skills, hardware availability, application domains and purposes, and also considering the support provided by the developer community.","Parallel Programming models
Programming systems
Big Data analysis
MapReduce
Workflow
Message Passing
Bulk Synchronous Parallel
SQL-like",37 Citations,,,,"In the age of the Internet of Things and social media platforms, huge amounts of digital data are generated by and collected from many sources, including sensors, mobile devices, wearable trackers and security cameras. This data, commonly referred to as Big Data, is challenging current storage, processing, and analysis capabilities. New models, languages, systems and algorithms continue to be developed to effectively collect, store, analyze and learn from Big Data. Most of the recent surveys provide a global analysis of the tools that are used in the main phases of Big Data management (generation, acquisition, storage, querying and visualization of data). Differently, this work analyzes and reviews parallel and distributed paradigms, languages and systems used today to analyze and learn from Big Data on scalable computers. In particular, we provide an in-depth analysis of the properties of the main parallel programming paradigms (MapReduce, workflow, BSP, message passing, and SQL-like) and, through programming examples, we describe the most used systems for Big Data analysis (e.g., Hadoop, Spark, and Storm). Furthermore, we discuss and compare the different systems by highlighting the main features of each of them, their diffusion (community of developers and users) and the main advantages and disadvantages of using them to implement Big Data analysis applications. The final goal of this work is to help designers and developers in identifying and selecting the best/appropriate programming solution based on their skills, hardware availability, application domains and purposes, and also considering the support provided by the developer community.",Low
Amir Bidaki,Automatic,Springer,http://link.springer.com/article/10.1007/s00521-021-06332-9,"Bio-inspired computation for big data fusion, storage, processing, learning and visualization: state of the art and future directions",2021,Type,Neural Computing and Applications,Core,"This overview gravitates on research achievements that have recently emerged from the confluence between Big Data technologies and bio-inspired computation. A manifold of reasons can be identified for the profitable synergy between these two paradigms, all rooted on the adaptability, intelligence and robustness that biologically inspired principles can provide to technologies aimed to manage, retrieve, fuse and process Big Data efficiently. We delve into this research field by first analyzing in depth the existing literature, with a focus on advances reported in the last few years. This prior literature analysis is complemented by an identification of the new trends and open challenges in Big Data that remain unsolved to date, and that can be effectively addressed by bio-inspired algorithms. As a second contribution, this work elaborates on how bio-inspired algorithms need to be adapted for their use in a Big Data context, in which data fusion becomes crucial as a previous step to allow processing and mining several and potentially heterogeneous data sources. This analysis allows exploring and comparing the scope and efficiency of existing approaches across different problems and domains, with the purpose of identifying new potential applications and research niches. Finally, this survey highlights open issues that remain unsolved to date in this research avenue, alongside a prescription of recommendations for future research.",Artificial Intelligence,0,,,,"This overview gravitates on research achievements that have recently emerged from the confluence between Big Data technologies and bio-inspired computation. A manifold of reasons can be identified for the profitable synergy between these two paradigms, all rooted on the adaptability, intelligence and robustness that biologically inspired principles can provide to technologies aimed to manage, retrieve, fuse and process Big Data efficiently. We delve into this research field by first analyzing in depth the existing literature, with a focus on advances reported in the last few years. This prior literature analysis is complemented by an identification of the new trends and open challenges in Big Data that remain unsolved to date, and that can be effectively addressed by bio-inspired algorithms. As a second contribution, this work elaborates on how bio-inspired algorithms need to be adapted for their use in a Big Data context, in which data fusion becomes crucial as a previous step to allow processing and mining several and potentially heterogeneous data sources. This analysis allows exploring and comparing the scope and efficiency of existing approaches across different problems and domains, with the purpose of identifying new potential applications and research niches. Finally, this survey highlights open issues that remain unsolved to date in this research avenue, alongside a prescription of recommendations for future research.",Low
Amir Bidaki,Automatic,Springer,http://link.springer.com/article/10.1007/s40860-024-00238-8,Enhancing trustworthiness in ML-based network intrusion detection with uncertainty quantification,2024,Type,Journal of Reliable Intelligent Environments,Core,"A crucial role in the security of modern networks is played by Intrusion Detection Systems (IDSs), security devices designed to identify and mitigate attacks to networks structure. Data-driven approaches based on Machine Learning (ML) have gained more and more popularity for executing the classification tasks required by signature-based IDSs. However, typical ML models adopted for this purpose do not properly take into account the uncertainty associated with their prediction. This poses significant challenges, as they tend to produce misleadingly high classification scores for both misclassified inputs and inputs belonging to unknown classes (e.g. novel attacks), limiting the trustworthiness of existing ML-based solutions. In this paper, we argue that ML-based IDSs should always provide accurate uncertainty quantification to avoid overconfident predictions. In fact, an uncertainty-aware classification would be beneficial to enhance closed-set classification performance, would make it possible to carry out Active Learning, and would help recognize inputs of unknown classes as truly unknowns, unlocking open-set classification capabilities and Out-of-Distribution (OoD) detection. To verify it, we compare various ML-based methods for uncertainty quantification and open-set classification, either specifically designed for or tailored to the domain of network intrusion detection. Moreover, we develop a custom model based on Bayesian Neural Networks that stands out for its OoD detection capabilities and robustness, with a lower variance in the results over different scenarios, compared to other baselines, thus showing how proper uncertainty quantification can be exploited to significantly enhance the trustworthiness of ML-based IDSs.",Artificial Intelligence,0,,,,"A crucial role in the security of modern networks is played by Intrusion Detection Systems (IDSs), security devices designed to identify and mitigate attacks to networks structure. Data-driven approaches based on Machine Learning (ML) have gained more and more popularity for executing the classification tasks required by signature-based IDSs. However, typical ML models adopted for this purpose do not properly take into account the uncertainty associated with their prediction. This poses significant challenges, as they tend to produce misleadingly high classification scores for both misclassified inputs and inputs belonging to unknown classes (e.g. novel attacks), limiting the trustworthiness of existing ML-based solutions. In this paper, we argue that ML-based IDSs should always provide accurate uncertainty quantification to avoid overconfident predictions. In fact, an uncertainty-aware classification would be beneficial to enhance closed-set classification performance, would make it possible to carry out Active Learning, and would help recognize inputs of unknown classes as truly unknowns, unlocking open-set classification capabilities and Out-of-Distribution (OoD) detection. To verify it, we compare various ML-based methods for uncertainty quantification and open-set classification, either specifically designed for or tailored to the domain of network intrusion detection. Moreover, we develop a custom model based on Bayesian Neural Networks that stands out for its OoD detection capabilities and robustness, with a lower variance in the results over different scenarios, compared to other baselines, thus showing how proper uncertainty quantification can be exploited to significantly enhance the trustworthiness of ML-based IDSs.",Low
Amir Bidaki,Automatic,Springer,http://link.springer.com/article/10.1007/s10586-020-03129-8,RETRACTED ARTICLE: Intrusion detection and performance simulation based on improved sequential pattern mining algorithm,2020,Type,Cluster Computing,Core,"Traditional network intrusion detection algorithm is based on pattern matching, which has made great progress in network intrusion detection system, but the efficiency of this algorithm for data packet matching is quite low. With the rapid increase of Internet scale and capacity, the general information security problem appears, and it brought hidden danger for an open network security. In this paper, the author analyse the intrusion detection and performance simulation based on improved sequential pattern mining algorithm. We integrate the data mining algorithms to implement the IDS, and the simulation result reflects the effectiveness of the methodology. The simulation shows that when minimum support is very small, PrefixSpan running time running a lot less time than other algorithm, and the difference between the two is obvious. Due to the mining algorithm of the relative independence of intrusion detection system, algorithm does not depend on the specific data and specific system, so the intrusion detection system based on data mining to data source requirement is very low.","Pattern mining algorithm
Data clustering
Intrusion detection
External network attacks",0,,,,"Traditional network intrusion detection algorithm is based on pattern matching, which has made great progress in network intrusion detection system, but the efficiency of this algorithm for data packet matching is quite low. With the rapid increase of Internet scale and capacity, the general information security problem appears, and it brought hidden danger for an open network security. In this paper, the author analyse the intrusion detection and performance simulation based on improved sequential pattern mining algorithm. We integrate the data mining algorithms to implement the IDS, and the simulation result reflects the effectiveness of the methodology. The simulation shows that when minimum support is very small, PrefixSpan running time running a lot less time than other algorithm, and the difference between the two is obvious. Due to the mining algorithm of the relative independence of intrusion detection system, algorithm does not depend on the specific data and specific system, so the intrusion detection system based on data mining to data source requirement is very low.",Low
Amir Bidaki,Automatic,Springer,http://link.springer.com/article/10.1007/s10462-012-9372-9,Evaluation of artificial intelligent techniques to secure information in enterprises,2014,Type,Artificial Intelligence Review,Core,"Information security paradigm is under a constant threat in enterprises particularly. The extension of World Wide Web and rapid expansion in size and types of documents involved in enterprises has generated many challenges. Extensive research has been conducted to determine the effective solutions to detect and respond but still the space is felt for improvement. Factors that hinder the development of an accurate detection and response techniques have shown links to the amount of data processing involved, number of protocols and application running across and variation in users’ requirements and responses. This paper is aimed at discussing the current issue in artificial intelligent (A.I.) techniques that could help in developing a better threat detection algorithm to secure information in enterprises. It is also investigated that the current information security techniques in enterprises have shown an inclination towards A.I. Conventional techniques for detection and response mostly requires human efforts to extract characteristics of malicious intent, investigate and analyze abnormal behaviors and later encode the derived results into the detection algorithm. Instead, A.I. can provide a direct solution to these requirements with a minimal human input. We have made an effort in this paper to discuss the current issues in information security and describe the benefits of artificially trained techniques in security process. We have also carried out survey of current A.I. techniques for IDS. Limitations of the techniques are discussed to identify the factors to be taken into account for efficient performance. Lastly, we have provided a possible research direction in this domain.",Artificial Intelligence,0,,,,"Information security paradigm is under a constant threat in enterprises particularly. The extension of World Wide Web and rapid expansion in size and types of documents involved in enterprises has generated many challenges. Extensive research has been conducted to determine the effective solutions to detect and respond but still the space is felt for improvement. Factors that hinder the development of an accurate detection and response techniques have shown links to the amount of data processing involved, number of protocols and application running across and variation in users’ requirements and responses. This paper is aimed at discussing the current issue in artificial intelligent (A.I.) techniques that could help in developing a better threat detection algorithm to secure information in enterprises. It is also investigated that the current information security techniques in enterprises have shown an inclination towards A.I. Conventional techniques for detection and response mostly requires human efforts to extract characteristics of malicious intent, investigate and analyze abnormal behaviors and later encode the derived results into the detection algorithm. Instead, A.I. can provide a direct solution to these requirements with a minimal human input. We have made an effort in this paper to discuss the current issues in information security and describe the benefits of artificially trained techniques in security process. We have also carried out survey of current A.I. techniques for IDS. Limitations of the techniques are discussed to identify the factors to be taken into account for efficient performance. Lastly, we have provided a possible research direction in this domain.",Low
Amir Bidaki,Automatic,Springer,http://link.springer.com/article/10.1007/s13042-024-02332-y,5G-SIID: an intelligent hybrid DDoS intrusion detector for 5G IoT networks,2024,Type,International Journal of Machine Learning and Cybernetics,Core,"The constrained resources of Internet of Things (IoT) devices make them susceptible to Distributed Denial-of-Service (DDoS) attacks that disrupt service availability by overwhelming systems. Thus, effective intrusion detection is critical to ensuring uninterrupted IoT activities. This research presents a scalable system that combines machine and deep learning models with optimized data processing to secure IoT devices against DDoS attacks. A real-world 5G-IoT network simulation dataset was used to evaluate performance. Robust feature selection identified the 10 most informative features from the high-dimensional data. These features were used to train eight classifiers, namely: k-Nearest Neighbors (KNN), Naive Bayes (NB), Decision Tree (DT), Random Forest (RF), Multi-Layer Perceptron (MLP), Convolutional Neural Network (CNN), Long-Short-Term Memory (LSTM) and hybrid CNN-LSTM models for DDoS attack detection. Experiments demonstrated 99.99% and 99.98% accuracy for multiclass and binary classification using the proposed hybrid CNN-LSTM model. Crucially, time- and space-complexity analysis validates real-world feasibility. Unlike prior works, this system optimally balances accuracy, efficiency, and adaptability through a precisely engineered model architecture, outperforming existing models. In general, this accurate, efficient, and adaptable system addresses critical IoT security challenges, improving cyber resilience in smart cities and autonomous vehicles.",,0,,,,"The constrained resources of Internet of Things (IoT) devices make them susceptible to Distributed Denial-of-Service (DDoS) attacks that disrupt service availability by overwhelming systems. Thus, effective intrusion detection is critical to ensuring uninterrupted IoT activities. This research presents a scalable system that combines machine and deep learning models with optimized data processing to secure IoT devices against DDoS attacks. A real-world 5G-IoT network simulation dataset was used to evaluate performance. Robust feature selection identified the 10 most informative features from the high-dimensional data. These features were used to train eight classifiers, namely: k-Nearest Neighbors (KNN), Naive Bayes (NB), Decision Tree (DT), Random Forest (RF), Multi-Layer Perceptron (MLP), Convolutional Neural Network (CNN), Long-Short-Term Memory (LSTM) and hybrid CNN-LSTM models for DDoS attack detection. Experiments demonstrated 99.99% and 99.98% accuracy for multiclass and binary classification using the proposed hybrid CNN-LSTM model. Crucially, time- and space-complexity analysis validates real-world feasibility. Unlike prior works, this system optimally balances accuracy, efficiency, and adaptability through a precisely engineered model architecture, outperforming existing models. In general, this accurate, efficient, and adaptable system addresses critical IoT security challenges, improving cyber resilience in smart cities and autonomous vehicles.",Low
Amir Bidaki,Automatic,Springer,http://link.springer.com/article/10.1007/s10462-023-10466-8,"Deep learning modelling techniques: current progress, applications, advantages, and challenges",2023,Type,Artificial Intelligence Review,Core,"Deep learning (DL) is revolutionizing evidence-based decision-making techniques that can be applied across various sectors. Specifically, it possesses the ability to utilize two or more levels of non-linear feature transformation of the given data via representation learning in order to overcome limitations posed by large datasets. As a multidisciplinary field that is still in its nascent phase, articles that survey DL architectures encompassing the full scope of the field are rather limited. Thus, this paper comprehensively reviews the state-of-art DL modelling techniques and provides insights into their advantages and challenges. It was found that many of the models exhibit a highly domain-specific efficiency and could be trained by two or more methods. However, training DL models can be very time-consuming, expensive, and requires huge samples for better accuracy. Since DL is also susceptible to deception and misclassification and tends to get stuck on local minima, improved optimization of parameters is required to create more robust models. Regardless, DL has already been leading to groundbreaking results in the healthcare, education, security, commercial, industrial, as well as government sectors. Some models, like the convolutional neural network (CNN), generative adversarial networks (GAN), recurrent neural network (RNN), recursive neural networks, and autoencoders, are frequently used, while the potential of other models remains widely unexplored. Pertinently, hybrid conventional DL architectures have the capacity to overcome the challenges experienced by conventional models. Considering that capsule architectures may dominate future DL models, this work aimed to compile information for stakeholders involved in the development and use of DL models in the contemporary world.",,0,,,,"Deep learning (DL) is revolutionizing evidence-based decision-making techniques that can be applied across various sectors. Specifically, it possesses the ability to utilize two or more levels of non-linear feature transformation of the given data via representation learning in order to overcome limitations posed by large datasets. As a multidisciplinary field that is still in its nascent phase, articles that survey DL architectures encompassing the full scope of the field are rather limited. Thus, this paper comprehensively reviews the state-of-art DL modelling techniques and provides insights into their advantages and challenges. It was found that many of the models exhibit a highly domain-specific efficiency and could be trained by two or more methods. However, training DL models can be very time-consuming, expensive, and requires huge samples for better accuracy. Since DL is also susceptible to deception and misclassification and tends to get stuck on local minima, improved optimization of parameters is required to create more robust models. Regardless, DL has already been leading to groundbreaking results in the healthcare, education, security, commercial, industrial, as well as government sectors. Some models, like the convolutional neural network (CNN), generative adversarial networks (GAN), recurrent neural network (RNN), recursive neural networks, and autoencoders, are frequently used, while the potential of other models remains widely unexplored. Pertinently, hybrid conventional DL architectures have the capacity to overcome the challenges experienced by conventional models. Considering that capsule architectures may dominate future DL models, this work aimed to compile information for stakeholders involved in the development and use of DL models in the contemporary world.",Low
Amir Bidaki,Automatic,Springer,http://link.springer.com/article/10.1007/s13177-024-00450-z,Generative AI and Online Learning Based Road Rage and Aggressive Driving Detection,2024,Type,International Journal of Intelligent Transportation Systems Research,Core,"Usage-Based Insurance (UBI) revolutionizes traditional coverage by utilizing technology to tailor insurance premiums to individual driving behaviour. Road rage and Aggressive driving behaviour Detection (RAD) is pivotal for enhancing the precision of risk assessment and premium customization. This paper introduces a novel approach to UBI based on RAD using Global Positioning Systems (GPS) signals and heart rate monitoring through smartphones and wearable technology. To mitigate the skewed nature of real-world driving data, it employs the Conditional Tabular Generative Adversarial Network (CTGAN), while an online learning paradigm ensures the model remains dynamic and scalable, adapting to new data over time. The proposed system called the Incremental Learning-based UBI System with Temporal Analysis and Recognition (ILUBISTAR) introduces a transformative approach to UBI pricing by leveraging RAD. This research employs a Bi-directional Long Short-Term Memory (BiLSTM) network for processing sequential driving data acquired real-time to classify driving behaviour. This model is trained and evaluated with data acquired from a cohort, demonstrating an accuracy of 99.9% in identifying drivers with consistent bad driving behaviours. It also features desirable performance metrics in the detection of good, unhealthy, always bad and road rage & aggressive drivers. This research demonstrates ILUBISTAR’s effectiveness in identifying various driving behaviour classification, underscoring its innovative contribution to UBI pricing. This research not only showcases ILUBISTAR’s superior performance in identifying diverse driving patterns but also highlights its potential to revolutionize UBI by promoting safer driving and more equitable insurance premiums.",,0,,,,"Usage-Based Insurance (UBI) revolutionizes traditional coverage by utilizing technology to tailor insurance premiums to individual driving behaviour. Road rage and Aggressive driving behaviour Detection (RAD) is pivotal for enhancing the precision of risk assessment and premium customization. This paper introduces a novel approach to UBI based on RAD using Global Positioning Systems (GPS) signals and heart rate monitoring through smartphones and wearable technology. To mitigate the skewed nature of real-world driving data, it employs the Conditional Tabular Generative Adversarial Network (CTGAN), while an online learning paradigm ensures the model remains dynamic and scalable, adapting to new data over time. The proposed system called the Incremental Learning-based UBI System with Temporal Analysis and Recognition (ILUBISTAR) introduces a transformative approach to UBI pricing by leveraging RAD. This research employs a Bi-directional Long Short-Term Memory (BiLSTM) network for processing sequential driving data acquired real-time to classify driving behaviour. This model is trained and evaluated with data acquired from a cohort, demonstrating an accuracy of 99.9% in identifying drivers with consistent bad driving behaviours. It also features desirable performance metrics in the detection of good, unhealthy, always bad and road rage & aggressive drivers. This research demonstrates ILUBISTAR’s effectiveness in identifying various driving behaviour classification, underscoring its innovative contribution to UBI pricing. This research not only showcases ILUBISTAR’s superior performance in identifying diverse driving patterns but also highlights its potential to revolutionize UBI by promoting safer driving and more equitable insurance premiums.",Medium
Amir Bidaki,Automatic,Springer,http://link.springer.com/article/10.1007/s10489-021-03036-4,Lie group continual meta learning algorithm,2022,Type,Applied Intelligence,Core,"Humans can use acquired experience to learn new skills quickly and without forgetting the knowledge they already have. However, the neural network cannot do continual learning like humans, because it is easy to fall into the stability-plasticity dilemma and lead to catastrophic forgetting. Since meta-learning with the already acquired knowledge as a priori can directly optimize the final goal, this paper proposes LGCMLA (Lie Group Continual Meta Learning Algorithm) based on meta-learning, this algorithm is an improvement of CMLA (Continual Meta Learning Algorithm) proposed by Jiang et al. On the one hand, LGCMLA enhances the continuity between tasks by changing the inner-loop update rule (from using random initialization parameters for each task to using the updated parameters of the previous task for the subsequent task). On the other hand, it uses orthogonal groups to limit the parameter space and adopts the natural Riemannian gradient descent to accelerate the convergence speed. It not only corrects the shortcomings of poor convergence and stability of CMLA, but also further improves the generalization performance of the model and solves the stability-plasticity dilemma more effectively. Experiments on miniImageNet, tieredImageNet and Fewshot-CIFAR100 (Canadian Institute For Advanced Research) datasets prove the effectiveness of LGCMLA. Especially compared to MAML (Model-Agnostic Meta-Learning) with standard four-layer convolution, the accuracy of 1 shot and 5 shot is improved by 16.4% and 17.99% respectively under the setting of 5-way on miniImageNet.",,0,,,,"Humans can use acquired experience to learn new skills quickly and without forgetting the knowledge they already have. However, the neural network cannot do continual learning like humans, because it is easy to fall into the stability-plasticity dilemma and lead to catastrophic forgetting. Since meta-learning with the already acquired knowledge as a priori can directly optimize the final goal, this paper proposes LGCMLA (Lie Group Continual Meta Learning Algorithm) based on meta-learning, this algorithm is an improvement of CMLA (Continual Meta Learning Algorithm) proposed by Jiang et al. On the one hand, LGCMLA enhances the continuity between tasks by changing the inner-loop update rule (from using random initialization parameters for each task to using the updated parameters of the previous task for the subsequent task). On the other hand, it uses orthogonal groups to limit the parameter space and adopts the natural Riemannian gradient descent to accelerate the convergence speed. It not only corrects the shortcomings of poor convergence and stability of CMLA, but also further improves the generalization performance of the model and solves the stability-plasticity dilemma more effectively. Experiments on miniImageNet, tieredImageNet and Fewshot-CIFAR100 (Canadian Institute For Advanced Research) datasets prove the effectiveness of LGCMLA. Especially compared to MAML (Model-Agnostic Meta-Learning) with standard four-layer convolution, the accuracy of 1 shot and 5 shot is improved by 16.4% and 17.99% respectively under the setting of 5-way on miniImageNet.",Medium
Amir Bidaki,Automatic,Springer,http://link.springer.com/article/10.1007/s11416-020-00349-9,Analytical modelling of cyber-physical systems: applying kinetic gas theory to anomaly detection in networks,2020,Type,Journal of Computer Virology and Hacking Techniques,Core,"In connection with anomaly detection in cyber-physical systems, we suggest in this paper a new way of modelling large systems consisting of a huge number of sensors, actuators and controllers. We base the approach on analytical methods usually used in kinetic gas theory, where one tries to describe the overall behavior of a gas without looking at each molecule separately. We model the system as a multi-agent network and derive predictions on the behavior of the network as a whole. These predictions can then be used to monitor the operation of the system. If the deviation between the predictions and the measured attributes of the operational cyber-physical system is sufficiently large, the monitoring system can raise an alarm. This way of modelling the normal behavior of a cyber-physical system has the advantage over machine learning methods mainly used for this purpose, that it is not based on the effective operation of the system during a training phase, but rather on the specification of the system and its intended use. It will detect anomalies in the system’s operation independent of their source—may it be an attack, a malfunction or a faulty implementation.","Cyber-physical system
Anomaly detection
Security
Analytical modelling
Kinetic theory",0,,,,"In connection with anomaly detection in cyber-physical systems, we suggest in this paper a new way of modelling large systems consisting of a huge number of sensors, actuators and controllers. We base the approach on analytical methods usually used in kinetic gas theory, where one tries to describe the overall behavior of a gas without looking at each molecule separately. We model the system as a multi-agent network and derive predictions on the behavior of the network as a whole. These predictions can then be used to monitor the operation of the system. If the deviation between the predictions and the measured attributes of the operational cyber-physical system is sufficiently large, the monitoring system can raise an alarm. This way of modelling the normal behavior of a cyber-physical system has the advantage over machine learning methods mainly used for this purpose, that it is not based on the effective operation of the system during a training phase, but rather on the specification of the system and its intended use. It will detect anomalies in the system’s operation independent of their source—may it be an attack, a malfunction or a faulty implementation.",Low
Amir Bidaki,Automatic,Springer,http://link.springer.com/article/10.1007/s10044-011-0255-5,Evaluation of an adaptive genetic-based signature extraction system for network intrusion detection,2013,Type,Pattern Analysis and Applications,Core,"Machine learning techniques are frequently applied to intrusion detection problems in various ways such as to classify normal and intrusive activities or to mine interesting intrusion patterns. Self-learning rule-based systems can relieve domain experts from the difficult task of hand crafting signatures, in addition to providing intrusion classification capabilities. To this end, a genetic-based signature learning system has been developed that can adaptively and dynamically learn signatures of both normal and intrusive activities from the network traffic. In this paper, we extend the evaluation of our systems to real time network traffic which is captured from a university departmental server. A methodology is developed to build fully labelled intrusion detection data set by mixing real background traffic with attacks simulated in a controlled environment. Tools are developed to pre-process the raw network data into feature vector format suitable for a supervised learning classifier system and other related machine learning systems. The signature extraction system is then applied to this data set and the results are discussed. We show that even simple feature sets can help detecting payload-based attacks.","Intrusion detection
Intrusion detection evaluation
Supervised learning
Genetic-based learning
Signature extraction
Incremental learning
Online learning",0,,,,"Machine learning techniques are frequently applied to intrusion detection problems in various ways such as to classify normal and intrusive activities or to mine interesting intrusion patterns. Self-learning rule-based systems can relieve domain experts from the difficult task of hand crafting signatures, in addition to providing intrusion classification capabilities. To this end, a genetic-based signature learning system has been developed that can adaptively and dynamically learn signatures of both normal and intrusive activities from the network traffic. In this paper, we extend the evaluation of our systems to real time network traffic which is captured from a university departmental server. A methodology is developed to build fully labelled intrusion detection data set by mixing real background traffic with attacks simulated in a controlled environment. Tools are developed to pre-process the raw network data into feature vector format suitable for a supervised learning classifier system and other related machine learning systems. The signature extraction system is then applied to this data set and the results are discussed. We show that even simple feature sets can help detecting payload-based attacks.",Medium
Amir Bidaki,Automatic,Springer,http://link.springer.com/article/10.1007/s41870-021-00720-z,Prolific detect and prevent invasion architecture with curvaceous K-pro acted neural nets for alpine the security in networking,2022,Type,International Journal of Information Technology,Core,"Internet plays an essential role in day today’s life and today’s generation rely on the internet for social, personal, and professional activities. More sensitive material users need a detection system to protect their data from intrusion. Most commercial intrusion detection prevention systems cannot detect potential attacks and ""zero days"" which results in low detection rates, network-specific fine-tuning, and high false positives requiring significant optimization. The novel prolific detect and prevent invasion architecture has been proposed that provides high security by detecting the anomalies and to prevent the data from intruders. Curvaceous K-pro acted neural nets is implemented which comprises new persistent merest optimization algorithm to tackle the major intricacy as orthogonal slating that arises while training to classify our data for instance analysis and also isolated state token generation algorithm for preventing the data from intruder via allocation of trust authority. Therefore, prolific detect and prevent invasion architecture is proposed to detect and prevent intruders in the emerging field of networking.",Artificial Intelligence,0,,,,"Internet plays an essential role in day today’s life and today’s generation rely on the internet for social, personal, and professional activities. More sensitive material users need a detection system to protect their data from intrusion. Most commercial intrusion detection prevention systems cannot detect potential attacks and ""zero days"" which results in low detection rates, network-specific fine-tuning, and high false positives requiring significant optimization. The novel prolific detect and prevent invasion architecture has been proposed that provides high security by detecting the anomalies and to prevent the data from intruders. Curvaceous K-pro acted neural nets is implemented which comprises new persistent merest optimization algorithm to tackle the major intricacy as orthogonal slating that arises while training to classify our data for instance analysis and also isolated state token generation algorithm for preventing the data from intruder via allocation of trust authority. Therefore, prolific detect and prevent invasion architecture is proposed to detect and prevent intruders in the emerging field of networking.",Low
Amir Bidaki,Automatic,Springer,http://link.springer.com/article/10.1007/s11277-021-08479-z,Adversarial Attack by Inducing Drift in Streaming Data,2022,Type,Wireless Personal Communications,Core,"Due to the innovation in technology, various techniques and tools are used to generate, collect, and store the data arriving at high speed and in huge volume. Stream data is the well-ordered sequence of items that arrive in a timely manner. The various sources of stream data are mobile devices, sensor nodes, network monitoring, traffic management, weblogs, etc. This paper aims at mining data stream with and without concept drift using the supervised machine learning technique in massive online analysis (MOA) frame work. The existing challenges in handling the stream data are improving the efficiency of the learners, effective memory management, reducing time taken and the way of handling the drifted data. The intrusion datasets are streamed using MOA framework and classification is performed using Naive Bayes, hoeffding tree, and ensemble algorithms, with and without drifted data. It is found that ensemble algorithm outperforms the other similar algorithms.","Classification
Concept drift
Naïve Bayes
Hoeffding tree
Ensemble classifiers
Massive online analysis",0,,,,"Due to the innovation in technology, various techniques and tools are used to generate, collect, and store the data arriving at high speed and in huge volume. Stream data is the well-ordered sequence of items that arrive in a timely manner. The various sources of stream data are mobile devices, sensor nodes, network monitoring, traffic management, weblogs, etc. This paper aims at mining data stream with and without concept drift using the supervised machine learning technique in massive online analysis (MOA) frame work. The existing challenges in handling the stream data are improving the efficiency of the learners, effective memory management, reducing time taken and the way of handling the drifted data. The intrusion datasets are streamed using MOA framework and classification is performed using Naive Bayes, hoeffding tree, and ensemble algorithms, with and without drifted data. It is found that ensemble algorithm outperforms the other similar algorithms.",Medium
Amir Bidaki,Automatic,Springer,http://link.springer.com/article/10.1007/s10766-016-0456-z,Hadoop Based Parallel Binary Bat Algorithm for Network Intrusion Detection,2017,Type,International Journal of Parallel Programming,Core,"In Internet applications, due to the growth of big data with more features, intrusion detection has become a difficult process in terms of computational complexity, storage efficiency and getting optimized solutions of classification through existing sequential computing environment. Using a parallel computing model and a nature inspired feature selection technique, a Hadoop Based Parallel Binary Bat Algorithm method is proposed for efficient feature selection and classification in order to obtain optimized detection rate. The MapReduce programming model of Hadoop improves computational complexity, the Parallel Binary Bat algorithm optimizes the prominent features selection and parallel Naïve Bayes provide cost-effective classification. The experimental results show that the proposed methodologies perform competently better than sequential computing approaches on massive data and the computational complexity is significantly reduced for feature selection as well as classification in big data applications.","Hadoop
Parallel Binary Bat
MapReduce
Feature selection
Classification",0,,,,"In Internet applications, due to the growth of big data with more features, intrusion detection has become a difficult process in terms of computational complexity, storage efficiency and getting optimized solutions of classification through existing sequential computing environment. Using a parallel computing model and a nature inspired feature selection technique, a Hadoop Based Parallel Binary Bat Algorithm method is proposed for efficient feature selection and classification in order to obtain optimized detection rate. The MapReduce programming model of Hadoop improves computational complexity, the Parallel Binary Bat algorithm optimizes the prominent features selection and parallel Naïve Bayes provide cost-effective classification. The experimental results show that the proposed methodologies perform competently better than sequential computing approaches on massive data and the computational complexity is significantly reduced for feature selection as well as classification in big data applications.",Low
Amir Bidaki,Automatic,Springer,http://link.springer.com/article/10.1186/s13638-019-1591-1,Web intrusion detection system combined with feature analysis and SVM optimization,2020,Type,EURASIP Journal on Wireless Communications and Networking,Core,"The current network traffic is large, and the network attacks have multiple types. Therefore, anomaly detection model combined with machine learning is developing rapidly. Frequent occurrences of Web Application Firewall (WAF) bypass attacks and the redundancy of the data characteristics in Hypertext Transfer Protocol (HTTP) protocol make it difficult to extract data characteristics. In this paper, an integrated web intrusion detection system combined with feature analysis and support vector machine (SVM) optimization is proposed. By using expert’s knowledge, the characteristics of the common Web attacks are analyzed. The related data characteristics are selected by the analysis of the HTTP protocol. In the classification learning, the mature and robust support vector machine algorithm is utilized and the grid search method is used for the parameter optimization. Consequently, a better detection capability on Web attacks can be obtained. By using the HTTP DATASET CSIC 2010 data set, experiments have been carried out to compare the detection capability of different kernel functions. The results show that the proposed system performs good in the detection capability and can detect the WAF bypass attacks effectively.","Hidden Markov model
Protocol analysis
Support vector machine
Grid search",0,,,,"The current network traffic is large, and the network attacks have multiple types. Therefore, anomaly detection model combined with machine learning is developing rapidly. Frequent occurrences of Web Application Firewall (WAF) bypass attacks and the redundancy of the data characteristics in Hypertext Transfer Protocol (HTTP) protocol make it difficult to extract data characteristics. In this paper, an integrated web intrusion detection system combined with feature analysis and support vector machine (SVM) optimization is proposed. By using expert’s knowledge, the characteristics of the common Web attacks are analyzed. The related data characteristics are selected by the analysis of the HTTP protocol. In the classification learning, the mature and robust support vector machine algorithm is utilized and the grid search method is used for the parameter optimization. Consequently, a better detection capability on Web attacks can be obtained. By using the HTTP DATASET CSIC 2010 data set, experiments have been carried out to compare the detection capability of different kernel functions. The results show that the proposed system performs good in the detection capability and can detect the WAF bypass attacks effectively.",Low
Amir Bidaki,Automatic,Springer,http://link.springer.com/article/10.1007/s11227-023-05715-0,NEAE: NeuroEvolution AutoEncoder for anomaly detection in internet traffic data,2024,Type,The Journal of Supercomputing,Core,"Abnormal behaviors degrade overall system efficiency and may lead to system suspension. Extensive academic research has focused on anomaly detection across various sectors, including industrial, information security, and artificial intelligence. In this study, we propose a NeuroEvolution AutoEncoder (NEAE) for classifying and predicting abnormalities in internet-based applications. The NEAE is genetically programmed to process dataset features that encompass abnormal behaviors in internet activities. Our approach targets multivariate anomalies using parallel dimension processing, aiming for synchronized intelligence. Genetic programming is central to our technique, enhancing optimization. We evaluate our proposed technique using a substantial internet-based dataset, specifically network traffic. Simulation results demonstrate the effectiveness of the NEAE in detecting abnormalities based on performance metrics. Given its relevance and integration with diverse industries such as the Internet of Things, wireless communication, network traffic, and the web, we choose an Internet-based application dataset to validate the NEAE’s performance.","Evolutionary
NeuroEvolution
Abnormality
Network",1 Citation,,,,"Abnormal behaviors degrade overall system efficiency and may lead to system suspension. Extensive academic research has focused on anomaly detection across various sectors, including industrial, information security, and artificial intelligence. In this study, we propose a NeuroEvolution AutoEncoder (NEAE) for classifying and predicting abnormalities in internet-based applications. The NEAE is genetically programmed to process dataset features that encompass abnormal behaviors in internet activities. Our approach targets multivariate anomalies using parallel dimension processing, aiming for synchronized intelligence. Genetic programming is central to our technique, enhancing optimization. We evaluate our proposed technique using a substantial internet-based dataset, specifically network traffic. Simulation results demonstrate the effectiveness of the NEAE in detecting abnormalities based on performance metrics. Given its relevance and integration with diverse industries such as the Internet of Things, wireless communication, network traffic, and the web, we choose an Internet-based application dataset to validate the NEAE’s performance.",Low
Amir Bidaki,Automatic,Springer,http://link.springer.com/article/10.1007/s11227-024-06018-8,Semi-supervised attack detection in industrial control systems with deviation networks and feature selection,2024,Type,The Journal of Supercomputing,Core,"With the rapid development of Industry 4.0, the importance of cyber security for industrial control systems has become increasingly prominent. The complexity and diversity of industrial control systems result in data with high dimensionality and strong correlation, posing significant challenges in obtaining labeled data. However, current intrusion detection methods often demand large amounts of labeled data for effective training. To address this limitation, this paper proposes a semi-supervised anomaly detection framework, called SFSD, which leverages feature selection and deviation networks to detect anomalies in industrial control systems. Specifically, we introduce a feature selection algorithm (IG-PCA) that utilizes information gain and principal component analysis to reduce the dimensionality of features in industrial control data by eliminating redundant features. Then, we propose a semi-supervised learning method based on an improved deviation network, which utilizes an anomaly scoring network to learn end-to-end anomaly scores for the training data, thus assigning anomaly scores to each training data. Finally, using a limited amount of anomaly-labeled data, we design a specific deviation loss function to optimize the anomaly scoring network, enabling a significant score bias between positive and negative samples. Experimental results demonstrate that the proposed SFSD outperforms existing semi-supervised anomaly detection frameworks by improving the accuracy and detection rate by an average of 1–2%. Moreover, SFSD requires less training time compared to existing frameworks, resulting in a training time reduction of approximately 10% or more.","Industrial control systems
Intrusion detection
Feature selection
Semi-supervised learning
PCA",0,,,,"With the rapid development of Industry 4.0, the importance of cyber security for industrial control systems has become increasingly prominent. The complexity and diversity of industrial control systems result in data with high dimensionality and strong correlation, posing significant challenges in obtaining labeled data. However, current intrusion detection methods often demand large amounts of labeled data for effective training. To address this limitation, this paper proposes a semi-supervised anomaly detection framework, called SFSD, which leverages feature selection and deviation networks to detect anomalies in industrial control systems. Specifically, we introduce a feature selection algorithm (IG-PCA) that utilizes information gain and principal component analysis to reduce the dimensionality of features in industrial control data by eliminating redundant features. Then, we propose a semi-supervised learning method based on an improved deviation network, which utilizes an anomaly scoring network to learn end-to-end anomaly scores for the training data, thus assigning anomaly scores to each training data. Finally, using a limited amount of anomaly-labeled data, we design a specific deviation loss function to optimize the anomaly scoring network, enabling a significant score bias between positive and negative samples. Experimental results demonstrate that the proposed SFSD outperforms existing semi-supervised anomaly detection frameworks by improving the accuracy and detection rate by an average of 1–2%. Moreover, SFSD requires less training time compared to existing frameworks, resulting in a training time reduction of approximately 10% or more.",Low
Amir Bidaki,Automatic,Springer,http://link.springer.com/article/10.1186/s40537-014-0007-7,Deep learning applications and challenges in big data analytics,2015,Type,Journal of Big Data,Core,"Big Data Analytics and Deep Learning are two high-focus of data science. Big Data has become important as many organizations both public and private have been collecting massive amounts of domain-specific information, which can contain useful information about problems such as national intelligence, cyber security, fraud detection, marketing, and medical informatics. Companies such as Google and Microsoft are analyzing large volumes of data for business analysis and decisions, impacting existing and future technology. Deep Learning algorithms extract high-level, complex abstractions as data representations through a hierarchical learning process. Complex abstractions are learnt at a given level based on relatively simpler abstractions formulated in the preceding level in the hierarchy. A key benefit of Deep Learning is the analysis and learning of massive amounts of unsupervised data, making it a valuable tool for Big Data Analytics where raw data is largely unlabeled and un-categorized. In the present study, we explore how Deep Learning can be utilized for addressing some important problems in Big Data Analytics, including extracting complex patterns from massive volumes of data, semantic indexing, data tagging, fast information retrieval, and simplifying discriminative tasks. We also investigate some aspects of Deep Learning research that need further exploration to incorporate specific challenges introduced by Big Data Analytics, including streaming data, high-dimensional data, scalability of models, and distributed computing. We conclude by presenting insights into relevant future works by posing some questions, including defining data sampling criteria, domain adaptation modeling, defining criteria for obtaining useful data abstractions, improving semantic indexing, semi-supervised learning, and active learning.",Artificial Intelligence,0,,,,"Big Data Analytics and Deep Learning are two high-focus of data science. Big Data has become important as many organizations both public and private have been collecting massive amounts of domain-specific information, which can contain useful information about problems such as national intelligence, cyber security, fraud detection, marketing, and medical informatics. Companies such as Google and Microsoft are analyzing large volumes of data for business analysis and decisions, impacting existing and future technology. Deep Learning algorithms extract high-level, complex abstractions as data representations through a hierarchical learning process. Complex abstractions are learnt at a given level based on relatively simpler abstractions formulated in the preceding level in the hierarchy. A key benefit of Deep Learning is the analysis and learning of massive amounts of unsupervised data, making it a valuable tool for Big Data Analytics where raw data is largely unlabeled and un-categorized. In the present study, we explore how Deep Learning can be utilized for addressing some important problems in Big Data Analytics, including extracting complex patterns from massive volumes of data, semantic indexing, data tagging, fast information retrieval, and simplifying discriminative tasks. We also investigate some aspects of Deep Learning research that need further exploration to incorporate specific challenges introduced by Big Data Analytics, including streaming data, high-dimensional data, scalability of models, and distributed computing. We conclude by presenting insights into relevant future works by posing some questions, including defining data sampling criteria, domain adaptation modeling, defining criteria for obtaining useful data abstractions, improving semantic indexing, semi-supervised learning, and active learning.",Low
Amir Bidaki,Automatic,Springer,http://link.springer.com/article/10.1007/s10462-024-10995-w,Revisiting streaming anomaly detection: benchmark and evaluation,2024,Type,Artificial Intelligence Review,Core,"Anomaly detection in streaming data is an important task for many real-world applications, such as network security, fraud detection, and system monitoring. However, streaming data often exhibit concept drift, which means that the data distribution changes over time. This poses a significant challenge for many anomaly detection algorithms, as they need to adapt to the evolving data to maintain high detection accuracy. Existing streaming anomaly detection algorithms lack a unified evaluation framework that validly assesses their performance and robustness under different types of concept drifts and anomalies. In this paper, we conduct a systematic technical review of the state-of-the-art methods for anomaly detection in streaming data. We propose a new data generator, called SCAR (Streaming data generator with Customizable Anomalies and concept dRifts), that can synthesize streaming data based on synthetic and real-world datasets from different domains. Furthermore, we adapt four static anomaly detection models to the streaming setting using a generic reconstruction strategy as baselines, and then compare them systematically with 9 existing streaming anomaly detection algorithms on 76 synthesized datasets that have various types of anomalies and concept drifts. The challenges and future research directions for anomaly detection in streaming data are also presented.",Artificial Intelligence,0,,,,"Anomaly detection in streaming data is an important task for many real-world applications, such as network security, fraud detection, and system monitoring. However, streaming data often exhibit concept drift, which means that the data distribution changes over time. This poses a significant challenge for many anomaly detection algorithms, as they need to adapt to the evolving data to maintain high detection accuracy. Existing streaming anomaly detection algorithms lack a unified evaluation framework that validly assesses their performance and robustness under different types of concept drifts and anomalies. In this paper, we conduct a systematic technical review of the state-of-the-art methods for anomaly detection in streaming data. We propose a new data generator, called SCAR (Streaming data generator with Customizable Anomalies and concept dRifts), that can synthesize streaming data based on synthetic and real-world datasets from different domains. Furthermore, we adapt four static anomaly detection models to the streaming setting using a generic reconstruction strategy as baselines, and then compare them systematically with 9 existing streaming anomaly detection algorithms on 76 synthesized datasets that have various types of anomalies and concept drifts. The challenges and future research directions for anomaly detection in streaming data are also presented.",Medium
Amir Bidaki,Automatic,Springer,http://link.springer.com/article/10.1007/s12650-021-00789-5,HyIDSVis: hybrid intrusion detection visualization analysis based on rare category and association rules,2022,Type,Journal of Visualization,Core,"Cyber security issues are always worthy of attention. Intrusion detection system (IDS) is one of approaches used to protect computer system and identify potential attack. However, existing methods are limited by high-dimensional computational complexity in rare or unknown attack detection task. To improve the ability of detecting anomaly intrusions, a hybrid intrusion detection framework is proposed in this paper. The proposed RuleRCD algorithm first uses fuzzy association rules based on Apriori and K-Means algorithm for normal pattern and major attack detection. The other data are then fed to an active learning-based rare category detection algorithm to identify its attack pattern. This paper also introduces an interactive visualization system, which integrates experts’ decision into intrusion detection workflow. The method improves the effectiveness and interpretability of detection process. KDD-99 dataset is used to evaluate the proposed framework. The result shows that the approach outperforms some methods, especially in terms of the rare attack.","Visualization system
Cyber intrusion detection
Rare category detection
Association rules",0,,,,"Cyber security issues are always worthy of attention. Intrusion detection system (IDS) is one of approaches used to protect computer system and identify potential attack. However, existing methods are limited by high-dimensional computational complexity in rare or unknown attack detection task. To improve the ability of detecting anomaly intrusions, a hybrid intrusion detection framework is proposed in this paper. The proposed RuleRCD algorithm first uses fuzzy association rules based on Apriori and K-Means algorithm for normal pattern and major attack detection. The other data are then fed to an active learning-based rare category detection algorithm to identify its attack pattern. This paper also introduces an interactive visualization system, which integrates experts’ decision into intrusion detection workflow. The method improves the effectiveness and interpretability of detection process. KDD-99 dataset is used to evaluate the proposed framework. The result shows that the approach outperforms some methods, especially in terms of the rare attack.",Low
Amir Bidaki,Automatic,Springer,http://link.springer.com/article/10.1186/1687-417X-2014-3,ADLU: a novel anomaly detection and location-attribution algorithm for UWB wireless sensor networks,2014,Type,EURASIP Journal on Information Security,Core,"Wireless sensor networks (WSNs) are gaining more and more interest in the research community due to their unique characteristics. Besides energy consumption considerations, security has emerged as an equally important aspect in their network design. This is because WSNs are vulnerable to various types of attacks and to node compromises, and as such, they require security mechanisms to defend against them. An intrusion detection system (IDS) is one such solution to the problem. While several signature-based and anomaly-based detection algorithms have been proposed to date for WSNs, none of them is specifically designed for the ultra-wideband (UWB) radio technology. UWB is a key solution for wireless connectivity among inexpensive devices characterized by ultra-low power consumption and high precision ranging. Based on these principles, in this paper, we propose a novel anomaly-based detection and location-attribution algorithm for cluster-based UWB WSNs. The proposed algorithm, abbreviated as ADLU, has dedicated procedures for secure cluster formation, periodic re-clustering, and efficient cluster member monitoring. The performance of ADLU in identifying and localizing intrusions using a rule-based anomaly detection scheme is studied via simulations.","Wireless sensor networks
UWB radio technology
Security in UWB WSNs
Anomaly-based detection
Attack attribution
Ranging attacks",8 Citations,,,,"Wireless sensor networks (WSNs) are gaining more and more interest in the research community due to their unique characteristics. Besides energy consumption considerations, security has emerged as an equally important aspect in their network design. This is because WSNs are vulnerable to various types of attacks and to node compromises, and as such, they require security mechanisms to defend against them. An intrusion detection system (IDS) is one such solution to the problem. While several signature-based and anomaly-based detection algorithms have been proposed to date for WSNs, none of them is specifically designed for the ultra-wideband (UWB) radio technology. UWB is a key solution for wireless connectivity among inexpensive devices characterized by ultra-low power consumption and high precision ranging. Based on these principles, in this paper, we propose a novel anomaly-based detection and location-attribution algorithm for cluster-based UWB WSNs. The proposed algorithm, abbreviated as ADLU, has dedicated procedures for secure cluster formation, periodic re-clustering, and efficient cluster member monitoring. The performance of ADLU in identifying and localizing intrusions using a rule-based anomaly detection scheme is studied via simulations.",Low
Amir Bidaki,Automatic,Springer,http://link.springer.com/article/10.1007/s12243-024-01049-x,CSNet 2022 special issue—decentralized and data-driven security in networking,2024,Type,Annals of Telecommunications,Core,"Correspondence to
Diogo Menezes Ferrazani Mattos.",,0,,,,"Correspondence to
Diogo Menezes Ferrazani Mattos.",Low
Amir Bidaki,Automatic,Springer,http://link.springer.com/article/10.1007/s00521-020-05627-7,Hidden Markov models on a self-organizing map for anomaly detection in 802.11 wireless networks,2021,Type,Neural Computing and Applications,Core,"The present work introduces a hybrid integration of the self-organizing map and the hidden Markov model (HMM) for anomaly detection in 802.11 wireless networks. The self-organizing hidden Markov model map (SOHMMM) deals with the spatial connections of HMMs, along with the inherent temporal dependencies of data sequences. In essence, an HMM is associated with each neuron of the SOHMMM lattice. In this paper, the SOHMMM algorithm is employed for anomaly detection in 802.11 wireless access point usage data. Furthermore, we extend the SOHMMM online gradient descent unsupervised learning algorithm for multivariate Gaussian emissions. The experimental analysis uses two types of data: synthetic data to investigate the accuracy and convergence of the SOHMMM algorithm and wireless simulation data to verify the significance and efficiency of the algorithm in anomaly detection. The sensitivity and specificity of the SOHMMM algorithm in anomaly detection are compared to two other approaches, namely HMM initialized with universal background model (HMM-UBM) and SOHMMM with zero neighborhood (Z-SOHMMM). The results from the wireless simulation experiments show that SOHMMM outperformed the aforementioned approaches in all the presented anomalous scenarios.",Artificial Intelligence,0,,,,"The present work introduces a hybrid integration of the self-organizing map and the hidden Markov model (HMM) for anomaly detection in 802.11 wireless networks. The self-organizing hidden Markov model map (SOHMMM) deals with the spatial connections of HMMs, along with the inherent temporal dependencies of data sequences. In essence, an HMM is associated with each neuron of the SOHMMM lattice. In this paper, the SOHMMM algorithm is employed for anomaly detection in 802.11 wireless access point usage data. Furthermore, we extend the SOHMMM online gradient descent unsupervised learning algorithm for multivariate Gaussian emissions. The experimental analysis uses two types of data: synthetic data to investigate the accuracy and convergence of the SOHMMM algorithm and wireless simulation data to verify the significance and efficiency of the algorithm in anomaly detection. The sensitivity and specificity of the SOHMMM algorithm in anomaly detection are compared to two other approaches, namely HMM initialized with universal background model (HMM-UBM) and SOHMMM with zero neighborhood (Z-SOHMMM). The results from the wireless simulation experiments show that SOHMMM outperformed the aforementioned approaches in all the presented anomalous scenarios.",Medium
Amir Bidaki,Automatic,Springer,http://link.springer.com/article/10.1007/s10844-023-00819-8,Learning autoencoder ensembles for detecting malware hidden communications in IoT ecosystems,2024,Type,Journal of Intelligent Information Systems,Core,"Modern IoT ecosystems are the preferred target of threat actors wanting to incorporate resource-constrained devices within a botnet or leak sensitive information. A major research effort is then devoted to create countermeasures for mitigating attacks, for instance, hardware-level verification mechanisms or effective network intrusion detection frameworks. Unfortunately, advanced malware is often endowed with the ability of cloaking communications within network traffic, e.g., to orchestrate compromised IoT nodes or exfiltrate data without being noticed. Therefore, this paper showcases how different autoencoder-based architectures can spot the presence of malicious communications hidden in conversations, especially in the TTL of IPv4 traffic. To conduct tests, this work considers IoT traffic traces gathered in a real setting and the presence of an attacker deploying two hiding schemes (i.e., naive and “elusive” approaches). Collected results showcase the effectiveness of our method as well as the feasibility of deploying autoencoders in production-quality IoT settings.",Artificial Intelligence,0,,,,"Modern IoT ecosystems are the preferred target of threat actors wanting to incorporate resource-constrained devices within a botnet or leak sensitive information. A major research effort is then devoted to create countermeasures for mitigating attacks, for instance, hardware-level verification mechanisms or effective network intrusion detection frameworks. Unfortunately, advanced malware is often endowed with the ability of cloaking communications within network traffic, e.g., to orchestrate compromised IoT nodes or exfiltrate data without being noticed. Therefore, this paper showcases how different autoencoder-based architectures can spot the presence of malicious communications hidden in conversations, especially in the TTL of IPv4 traffic. To conduct tests, this work considers IoT traffic traces gathered in a real setting and the presence of an attacker deploying two hiding schemes (i.e., naive and “elusive” approaches). Collected results showcase the effectiveness of our method as well as the feasibility of deploying autoencoders in production-quality IoT settings.",Low
Amir Bidaki,Automatic,Springer,http://link.springer.com/article/10.1007/s00500-016-2258-z,Streaming data anomaly detection method based on hyper-grid structure and online ensemble learning,2017,Type,Soft Computing,Core,"This paper proposes a novel online streaming data anomaly detection method. By using the new method, the improved \(L_{1}\) detection neighbor region optimizes the initial hyper-grid-based anomaly detection method by decreasing the quantity of neighbor detection region, and online ensemble learning adapts to the distribution evolving characteristic of streaming data and overcomes the difficulty of obtaining the optimal hyper-grid structure. To validate the proposed method, the paper uses a real-world dataset and two simulated datasets and finds out that the experimental results are near to the optimal results.",Artificial Intelligence,0,,,,"This paper proposes a novel online streaming data anomaly detection method. By using the new method, the improved \(L_{1}\) detection neighbor region optimizes the initial hyper-grid-based anomaly detection method by decreasing the quantity of neighbor detection region, and online ensemble learning adapts to the distribution evolving characteristic of streaming data and overcomes the difficulty of obtaining the optimal hyper-grid structure. To validate the proposed method, the paper uses a real-world dataset and two simulated datasets and finds out that the experimental results are near to the optimal results.",Medium
Amir Bidaki,Automatic,Springer,http://link.springer.com/article/10.1007/s10723-020-09505-3,An Anomaly Data Mining Method for Mass Sensor Networks Using Improved PSO Algorithm Based on Spark Parallel Framework,2020,Type,Journal of Grid Computing,Core,"Accurate detection and capture of anomaly data in complex network data stream is an important part of ensuring network security. Traditional methods cannot adapt to the high dynamic changes of abnormal data characteristics in complex network. Thus, the detection accuracy is reduced. In this paper, a k-means parallel clustering algorithm is proposed. It is optimized by particle swarm optimization with dynamic adaptive inertia weight (dsPSOK-means). And it is used to mine the anomaly data for mass sensor networks. The inertia weight is dynamically adjusted through the fitness function, so that the dsPSO algorithm has the adaptive characteristics. Then, the output of the dsPSO algorithm is taken as the input of the k-means algorithm. Thus, the intelligence and self-adaptability of the k-means algorithm in selecting the initial center point is improved. Finally, with the help of Spark platform, the parallelization of dsPSOK-means clustering algorithm in the clustering environment is designed and implemented. It is shown by the experimental results that the traffic among nodes in the execution process can be effectively reduced by the dsPSOK-means algorithm. And the accuracy of abnormal data mining in complex network data flow is 5% higher than that of the comparison algorithm on average.","Spark parallel framework
Anomaly data mining
Fitness function
Improved PSO algorithm
Sensor network
Mass data",0,,,,"Accurate detection and capture of anomaly data in complex network data stream is an important part of ensuring network security. Traditional methods cannot adapt to the high dynamic changes of abnormal data characteristics in complex network. Thus, the detection accuracy is reduced. In this paper, a k-means parallel clustering algorithm is proposed. It is optimized by particle swarm optimization with dynamic adaptive inertia weight (dsPSOK-means). And it is used to mine the anomaly data for mass sensor networks. The inertia weight is dynamically adjusted through the fitness function, so that the dsPSO algorithm has the adaptive characteristics. Then, the output of the dsPSO algorithm is taken as the input of the k-means algorithm. Thus, the intelligence and self-adaptability of the k-means algorithm in selecting the initial center point is improved. Finally, with the help of Spark platform, the parallelization of dsPSOK-means clustering algorithm in the clustering environment is designed and implemented. It is shown by the experimental results that the traffic among nodes in the execution process can be effectively reduced by the dsPSOK-means algorithm. And the accuracy of abnormal data mining in complex network data flow is 5% higher than that of the comparison algorithm on average.",Low
Amir Bidaki,Automatic,Springer,http://link.springer.com/article/10.1007/s00170-024-13874-4,Improving operations through a lean AI paradigm: a view to an AI-aided lean manufacturing via versatile convolutional neural network,2024,Type,The International Journal of Advanced Manufacturing Technology,Core,"Integrating Lean Manufacturing tools with artificial intelligence (AI) is emerging as a revolutionary approach to optimize production processes, reduce waste, and enhance efficiency. Traditional Lean practices focus on waste reduction and process improvement, often relying on human expertise for problem identification and resolution. AI algorithms, on the other hand, excel in pattern recognition, data analysis, and decision-making. Lean tools and AI can offer more precise, data-driven solutions for common manufacturing challenges when integrated. AI algorithms can automate and refine Lean techniques like value stream mapping, Kanban, and 5S by providing real-time, actionable insights drawn from big data. This fusion of Lean and AI aids in predictive maintenance, quality control, and optimization, enhancing the efficiency and responsiveness of the manufacturing process. Moreover, AI’s capability for machine learning allows the system to adapt and improve autonomously over time, further aligning with Lean’s continuous improvement ethos. Seven case studies were conducted to show how this alignment might aid Lean Manufacturing. However, successful implementation necessitates overcoming data quality and algorithmic bias challenges. Despite these hurdles, integrating Lean tools and AI can redefine best practices in manufacturing, setting new standards for operational excellence.",Artificial Intelligence,7 Citations,,,,"Integrating Lean Manufacturing tools with artificial intelligence (AI) is emerging as a revolutionary approach to optimize production processes, reduce waste, and enhance efficiency. Traditional Lean practices focus on waste reduction and process improvement, often relying on human expertise for problem identification and resolution. AI algorithms, on the other hand, excel in pattern recognition, data analysis, and decision-making. Lean tools and AI can offer more precise, data-driven solutions for common manufacturing challenges when integrated. AI algorithms can automate and refine Lean techniques like value stream mapping, Kanban, and 5S by providing real-time, actionable insights drawn from big data. This fusion of Lean and AI aids in predictive maintenance, quality control, and optimization, enhancing the efficiency and responsiveness of the manufacturing process. Moreover, AI’s capability for machine learning allows the system to adapt and improve autonomously over time, further aligning with Lean’s continuous improvement ethos. Seven case studies were conducted to show how this alignment might aid Lean Manufacturing. However, successful implementation necessitates overcoming data quality and algorithmic bias challenges. Despite these hurdles, integrating Lean tools and AI can redefine best practices in manufacturing, setting new standards for operational excellence.",Medium
Amir Bidaki,Automatic,Springer,http://link.springer.com/article/10.1007/s40860-019-00080-3,Systematic literature review and metadata analysis of ransomware attacks and detection mechanisms,2019,Type,Journal of Reliable Intelligent Environments,Core,"Ransomware is advanced and upgraded malicious software which comes in the forms of Crypto or Locker, with the intention to attack and take control of basic infrastructures and computer systems. The vast majority of these threats are aimed at directly or indirectly making money from the victims by asking for a ransom in exchange for decryption keys. This systematic literature analysed the anatomy of ransomware, including its trends and mode of attacks to find the possible solutions by querying various academic literature. In contrast to previous reviews, sources of ransomware dataset are revealed in this review paper to ease the challenges of researchers in getting access to ransomware datasets. In addition, a taxonomy of ransomware current trends is presented in the paper. We discussed the articles in detail, the evolution and trend in ransomware researches. Most of the techniques deployed could not completely prevent ransomware attacks because of its obfuscation techniques, but rather recommend proper and regular backup of important files. This review can serve as a benchmark for researchers in proposing a novel ransomware detection methodology and starting point for novice researchers.",Artificial Intelligence,42 Citations,,,,"Ransomware is advanced and upgraded malicious software which comes in the forms of Crypto or Locker, with the intention to attack and take control of basic infrastructures and computer systems. The vast majority of these threats are aimed at directly or indirectly making money from the victims by asking for a ransom in exchange for decryption keys. This systematic literature analysed the anatomy of ransomware, including its trends and mode of attacks to find the possible solutions by querying various academic literature. In contrast to previous reviews, sources of ransomware dataset are revealed in this review paper to ease the challenges of researchers in getting access to ransomware datasets. In addition, a taxonomy of ransomware current trends is presented in the paper. We discussed the articles in detail, the evolution and trend in ransomware researches. Most of the techniques deployed could not completely prevent ransomware attacks because of its obfuscation techniques, but rather recommend proper and regular backup of important files. This review can serve as a benchmark for researchers in proposing a novel ransomware detection methodology and starting point for novice researchers.",Low
Amir Bidaki,Automatic,Springer,http://link.springer.com/article/10.1007/s11036-023-02104-y,Advancing Security in the Industrial Internet of Things Using Deep Progressive Neural Networks,2023,Type,Mobile Networks and Applications,Core,"A machine learning algorithm that can solve complex tasks by leveraging information through transfer learning while avoiding catastrophic forgetting of previously learned data. This clearly is a principal milestone to achieving human-level intelligence. The exponential growth of the Industrial Internet of Things (IIoT) has warranted an urgent need for reliable security solutions. In this paper, a novel deep progressive algorithm leveraging, both, progressive learning and deep neural networks has been proposed to achieve this. The numerous threats faced by IIoT devices are accurately classified by the proposed Deep Progressive Neural Network (DPNN), and new classes are efficiently added to the previously trained network by employing prior knowledge via lateral connections to the previously learned network. Through robust experimentation, it is shown that the DPNN model improves the efficiency and reliability of the classification of attacks and that the proposed architecture provides an accuracy of 94% compared to the 69.7% accuracy of the contemporary KNN model.",Artificial Intelligence,0,,,,"A machine learning algorithm that can solve complex tasks by leveraging information through transfer learning while avoiding catastrophic forgetting of previously learned data. This clearly is a principal milestone to achieving human-level intelligence. The exponential growth of the Industrial Internet of Things (IIoT) has warranted an urgent need for reliable security solutions. In this paper, a novel deep progressive algorithm leveraging, both, progressive learning and deep neural networks has been proposed to achieve this. The numerous threats faced by IIoT devices are accurately classified by the proposed Deep Progressive Neural Network (DPNN), and new classes are efficiently added to the previously trained network by employing prior knowledge via lateral connections to the previously learned network. Through robust experimentation, it is shown that the DPNN model improves the efficiency and reliability of the classification of attacks and that the proposed architecture provides an accuracy of 94% compared to the 69.7% accuracy of the contemporary KNN model.",High
Amir Bidaki,Automatic,Springer,http://link.springer.com/article/10.1007/s00500-020-05226-7,Hyperbox-based machine learning algorithms: a comprehensive survey,2021,Type,Soft Computing,Core,"With the rapid development of digital information, the data volume generated by humans and machines is growing exponentially. Along with this trend, machine learning algorithms have been formed and evolved continuously to discover new information and knowledge from different data sources. Learning algorithms using hyperboxes as fundamental representational and building blocks are a branch of machine learning methods. These algorithms have enormous potential for high scalability and online adaptation of predictors built using hyperbox data representations to the dynamically changing environments and streaming data. This paper aims to give a comprehensive survey of the literature on hyperbox-based machine learning models. In general, according to the architecture and characteristic features of the resulting models, the existing hyperbox-based learning algorithms may be grouped into three major categories: fuzzy min–max neural networks, hyperbox-based hybrid models and other algorithms based on hyperbox representations. Within each of these groups, this paper shows a brief description of the structure of models, associated learning algorithms and an analysis of their advantages and drawbacks. Main applications of these hyperbox-based models to the real-world problems are also described in this paper. Finally, we discuss some open problems and identify potential future research directions in this field.",Artificial Intelligence,0,,,,"With the rapid development of digital information, the data volume generated by humans and machines is growing exponentially. Along with this trend, machine learning algorithms have been formed and evolved continuously to discover new information and knowledge from different data sources. Learning algorithms using hyperboxes as fundamental representational and building blocks are a branch of machine learning methods. These algorithms have enormous potential for high scalability and online adaptation of predictors built using hyperbox data representations to the dynamically changing environments and streaming data. This paper aims to give a comprehensive survey of the literature on hyperbox-based machine learning models. In general, according to the architecture and characteristic features of the resulting models, the existing hyperbox-based learning algorithms may be grouped into three major categories: fuzzy min–max neural networks, hyperbox-based hybrid models and other algorithms based on hyperbox representations. Within each of these groups, this paper shows a brief description of the structure of models, associated learning algorithms and an analysis of their advantages and drawbacks. Main applications of these hyperbox-based models to the real-world problems are also described in this paper. Finally, we discuss some open problems and identify potential future research directions in this field.",Medium
Amir Bidaki,Automatic,Springer,http://link.springer.com/article/10.1007/s10489-023-04594-5,Metaheuristic-based time series clustering for anomaly detection in manufacturing industry,2023,Type,Applied Intelligence,Core,"Nowadays time series clustering is of great importance in manufacturing industries. Meanwhile, it is considerably challenging to achieve explainable solution as well as significant performance due to computation complexity and variable diversity. To efficaciously handle the difficulty, this paper presents a novel metaheuristic-based time series clustering method which can improve the effectiveness and logicality of existing clustering approaches. The proposed method collects candidate cluster references from hierarchical and partitional clustering through shape-based distance measure as well as dynamic time warping (DTW) on manufacturing time series data. By applying metaheuristics highlighting estimation of distribution algorithms (EDA), such as extended compact genetic algorithm (ECGA), on the collected candidate clusters, advanced cluster centroid combinations with minimal distances can be achieved. ECGA employs the least complicated and the most closely related probabilistic model structure regarding population space during generation cycle. This feature strengthens the comprehension of clustering results in how such optimal solutions were achieved. The proposed method was tested on real-world time series data, open to the public, from manufacturing industry, and showed noticeable performances compared to well-established methods. Accordingly, this paper demonstrates that obtaining both comprehensible result as well as prominent performance is feasible by employing metaheuristic techniques to time series data clustering methods.",Artificial Intelligence,4 Citations,,,,"Nowadays time series clustering is of great importance in manufacturing industries. Meanwhile, it is considerably challenging to achieve explainable solution as well as significant performance due to computation complexity and variable diversity. To efficaciously handle the difficulty, this paper presents a novel metaheuristic-based time series clustering method which can improve the effectiveness and logicality of existing clustering approaches. The proposed method collects candidate cluster references from hierarchical and partitional clustering through shape-based distance measure as well as dynamic time warping (DTW) on manufacturing time series data. By applying metaheuristics highlighting estimation of distribution algorithms (EDA), such as extended compact genetic algorithm (ECGA), on the collected candidate clusters, advanced cluster centroid combinations with minimal distances can be achieved. ECGA employs the least complicated and the most closely related probabilistic model structure regarding population space during generation cycle. This feature strengthens the comprehension of clustering results in how such optimal solutions were achieved. The proposed method was tested on real-world time series data, open to the public, from manufacturing industry, and showed noticeable performances compared to well-established methods. Accordingly, this paper demonstrates that obtaining both comprehensible result as well as prominent performance is feasible by employing metaheuristic techniques to time series data clustering methods.",Low
Amir Bidaki,Automatic,Springer,http://link.springer.com/article/10.1007/s12206-021-1105-z,Predictive maintenance of abnormal wind turbine events by using machine learning based on condition monitoring for anomaly detection,2021,Type,Journal of Mechanical Science and Technology,Core,"The predictive maintenance of wind turbines has become a critical issue with the rapid development of wind power generation. The early detection of abnormal operation conditions can prevent failure status, which takes a long time to recover. Energy waste can also be reduced while maintenance efficiency can be improved by using a supervisory control and data acquisition (SCADA) system to monitor the operation status of wind turbines. Massive data are generated from different sensors during wind turbine operation, and SCADA can be used to gather reports about hundreds of possible abnormal conditions. The popular maintenance methods have been mostly designed on the basis of statistical analysis and data mining. However, such schemes need not only big data but also sophisticated processing techniques. This study addresses the aforementioned challenges by proposing a deep learning model with comprehensive data preprocessing and hyperparameter tuning on batch size to achieve abnormal early detection. The necessary data preprocessing is initially conducted besides the conventional data cleaning and normalization steps, and time-series data windowing and label settings are also performed. Then, the imbalanced classes in the records are addressed by adopting an augmentation scheme called the synthetic minority oversampling technique. Principal component analysis is also used to enhance the training. Finally, the proposed deep learning method with fine-tuning is compared with three machine learning models for early anomaly event detection. Experimental results show that the proposed scheme can identify potential faults 72 hours before they occur, and the precision rate exceeds 90 %.","Wind turbine
Predictive maintenance
Anomaly detection
Machine learning
Data argumentation",0,,,,"The predictive maintenance of wind turbines has become a critical issue with the rapid development of wind power generation. The early detection of abnormal operation conditions can prevent failure status, which takes a long time to recover. Energy waste can also be reduced while maintenance efficiency can be improved by using a supervisory control and data acquisition (SCADA) system to monitor the operation status of wind turbines. Massive data are generated from different sensors during wind turbine operation, and SCADA can be used to gather reports about hundreds of possible abnormal conditions. The popular maintenance methods have been mostly designed on the basis of statistical analysis and data mining. However, such schemes need not only big data but also sophisticated processing techniques. This study addresses the aforementioned challenges by proposing a deep learning model with comprehensive data preprocessing and hyperparameter tuning on batch size to achieve abnormal early detection. The necessary data preprocessing is initially conducted besides the conventional data cleaning and normalization steps, and time-series data windowing and label settings are also performed. Then, the imbalanced classes in the records are addressed by adopting an augmentation scheme called the synthetic minority oversampling technique. Principal component analysis is also used to enhance the training. Finally, the proposed deep learning method with fine-tuning is compared with three machine learning models for early anomaly event detection. Experimental results show that the proposed scheme can identify potential faults 72 hours before they occur, and the precision rate exceeds 90 %.",Low
Amir Bidaki,Automatic,Springer,http://link.springer.com/article/10.1038/s41598-024-78976-1,A hybrid approach using support vector machine rule-based system: detecting cyber threats in internet of things,2024,Type,Scientific Reports,Core,"While the proliferation of the Internet of Things (IoT) has revolutionized several industries, it has also created severe data security concerns. The security of these network devices and the dependability of IoT networks depend on efficient threat detection. Device heterogeneity, computing resource constraints, and the ever-changing nature of cyber threats are a few of the obstacles that make detecting cyber threats in IoT systems difficult. Complex threats often go undetected by conventional security measures, requiring more sophisticated, adaptive detection methods. Therefore, this study presents the Hybrid approach based on the Support Vector Machines Rule-Based Detection (HSVMR-D) method for an all-encompassing approach to identifying cyber threats to the IoT. The HSVMR-D employs SVM to categorize known and unknown threats using attributes acquired from IoT data. Identifying known attack signatures and patterns using rule-based approaches improves detection efficiency without retraining by adapting pre-trained models to new IoT contexts. Moreover, protecting vital infrastructure and sensitive data, HSVMR-D provides a thorough and adaptable solution to improve the security posture of IoT deployments. Comprehensive experiment analysis and simulation results compared to the baseline study have confirmed the efficiency of the proposed HSVMR-D. Furthermore, increased resilience to completely novel changing threats, fewer false positives, and improved accuracy in threat detection are all outcomes that show the proposed work outperforms others. The HSVMR-D approach is helpful where the primary objective is a secure environment in the Internet of Things (IoT) when resources are limited.","Internet of things
Cyber threats detection
Integrating
Machine learning
Anomaly Detection
Heuristic algorithms
Transfer learning
Hybrid
Support Vector Machine",0,,,,"While the proliferation of the Internet of Things (IoT) has revolutionized several industries, it has also created severe data security concerns. The security of these network devices and the dependability of IoT networks depend on efficient threat detection. Device heterogeneity, computing resource constraints, and the ever-changing nature of cyber threats are a few of the obstacles that make detecting cyber threats in IoT systems difficult. Complex threats often go undetected by conventional security measures, requiring more sophisticated, adaptive detection methods. Therefore, this study presents the Hybrid approach based on the Support Vector Machines Rule-Based Detection (HSVMR-D) method for an all-encompassing approach to identifying cyber threats to the IoT. The HSVMR-D employs SVM to categorize known and unknown threats using attributes acquired from IoT data. Identifying known attack signatures and patterns using rule-based approaches improves detection efficiency without retraining by adapting pre-trained models to new IoT contexts. Moreover, protecting vital infrastructure and sensitive data, HSVMR-D provides a thorough and adaptable solution to improve the security posture of IoT deployments. Comprehensive experiment analysis and simulation results compared to the baseline study have confirmed the efficiency of the proposed HSVMR-D. Furthermore, increased resilience to completely novel changing threats, fewer false positives, and improved accuracy in threat detection are all outcomes that show the proposed work outperforms others. The HSVMR-D approach is helpful where the primary objective is a secure environment in the Internet of Things (IoT) when resources are limited.",Low
Amir Bidaki,Automatic,Springer,http://link.springer.com/article/10.1007/s10462-019-09685-9,The state of the art and taxonomy of big data analytics: view from new big data framework,2020,Type,Artificial Intelligence Review,Core,"Big data has become a significant research area due to the birth of enormous data generated from various sources like social media, internet of things and multimedia applications. Big data has played critical role in many decision makings and forecasting domains such as recommendation systems, business analysis, healthcare, web display advertising, clinicians, transportation, fraud detection and tourism marketing. The rapid development of various big data tools such as Hadoop, Storm, Spark, Flink, Kafka and Pig in research and industrial communities has allowed the huge number of data to be distributed, communicated and processed. Big data applications use big data analytics techniques to efficiently analyze large amounts of data. However, choosing the suitable big data tools based on batch and stream data processing and analytics techniques for development a big data system are difficult due to the challenges in processing and applying big data. Practitioners and researchers who are developing big data systems have inadequate information about the current technology and requirement concerning the big data platform. Hence, the strengths and weaknesses of big data technologies and effective solutions for Big Data challenges are needed to be discussed. Hence, due to that, this paper presents a review of the literature that analyzes the use of big data tools and big data analytics techniques in areas like health and medical care, social networking and internet, government and public sector, natural resource management, economic and business sector. The goals of this paper are to (1) understand the trend of big data-related research and current frames of big data technologies; (2) identify trends in the use or research of big data tools based on batch and stream processing and big data analytics techniques; (3) assist and provide new researchers and practitioners to place new research activity in this domain appropriately. The findings of this study will provide insights and knowledge on the existing big data platforms and their application domains, the advantages and disadvantages of big data tools, big data analytics techniques and their use, and new research opportunities in future development of big data systems.",Artificial Intelligence,0,,,,"Big data has become a significant research area due to the birth of enormous data generated from various sources like social media, internet of things and multimedia applications. Big data has played critical role in many decision makings and forecasting domains such as recommendation systems, business analysis, healthcare, web display advertising, clinicians, transportation, fraud detection and tourism marketing. The rapid development of various big data tools such as Hadoop, Storm, Spark, Flink, Kafka and Pig in research and industrial communities has allowed the huge number of data to be distributed, communicated and processed. Big data applications use big data analytics techniques to efficiently analyze large amounts of data. However, choosing the suitable big data tools based on batch and stream data processing and analytics techniques for development a big data system are difficult due to the challenges in processing and applying big data. Practitioners and researchers who are developing big data systems have inadequate information about the current technology and requirement concerning the big data platform. Hence, the strengths and weaknesses of big data technologies and effective solutions for Big Data challenges are needed to be discussed. Hence, due to that, this paper presents a review of the literature that analyzes the use of big data tools and big data analytics techniques in areas like health and medical care, social networking and internet, government and public sector, natural resource management, economic and business sector. The goals of this paper are to (1) understand the trend of big data-related research and current frames of big data technologies; (2) identify trends in the use or research of big data tools based on batch and stream processing and big data analytics techniques; (3) assist and provide new researchers and practitioners to place new research activity in this domain appropriately. The findings of this study will provide insights and knowledge on the existing big data platforms and their application domains, the advantages and disadvantages of big data tools, big data analytics techniques and their use, and new research opportunities in future development of big data systems.",Low
Amir Bidaki,Automatic,Springer,http://link.springer.com/article/10.1007/s11431-019-1607-7,Multi-objective optimization of feature selection using hybrid cat swarm optimization,2021,Type,Science China Technological Sciences,Core,"With the pervasive generation of information from a wide range of sensors and devices, there always exist a large number of input features in databases, thus complicating machine learning problem formulation. However, certain features are relatively impertinent to specific problems, which may degrade the performances of classifiers in terms of prediction accuracy, sensitivity, specificity, and recall rate. The main goal of a multi-objective optimization problem is to identify the subsets of the given features. To this end, a hybrid cat swarm optimization (HCSO) algorithm is proposed in our paper for performance improvement of the basic cat swarm optimization (CSO) that incorporates guided and competitive & inherent characteristics into the original CSO. The performance of HCSO has been tested by finding the optimal feature subset for 15 benchmark datasets. The number of class labels for these datasets vary between 2 to 40. The time complexity analysis of both CSO and HCSO has also been evaluated. Moreover, the performance of the proposed algorithm has been compared with that of simple CSO and other state-of-the-art techniques. The performances obtained by HCSO have an average 2.68% improvement with a standard deviation of 2.91. The maximum performance improvement is up to 10.09% in prediction accuracy. Tested on the same datasets, CSO has yielded improvements within the range of −7.27% to 8.51% with an average improvement 0.9% and standard deviation 3.96. The statistical tests carried out in the experiments prove that HCSO manifests a moderately better feature selection capacity than that of its counterparts.","feature selection
competition
cat swarm optimization
guided search
parameter evolution",19 Citations,,,,"With the pervasive generation of information from a wide range of sensors and devices, there always exist a large number of input features in databases, thus complicating machine learning problem formulation. However, certain features are relatively impertinent to specific problems, which may degrade the performances of classifiers in terms of prediction accuracy, sensitivity, specificity, and recall rate. The main goal of a multi-objective optimization problem is to identify the subsets of the given features. To this end, a hybrid cat swarm optimization (HCSO) algorithm is proposed in our paper for performance improvement of the basic cat swarm optimization (CSO) that incorporates guided and competitive & inherent characteristics into the original CSO. The performance of HCSO has been tested by finding the optimal feature subset for 15 benchmark datasets. The number of class labels for these datasets vary between 2 to 40. The time complexity analysis of both CSO and HCSO has also been evaluated. Moreover, the performance of the proposed algorithm has been compared with that of simple CSO and other state-of-the-art techniques. The performances obtained by HCSO have an average 2.68% improvement with a standard deviation of 2.91. The maximum performance improvement is up to 10.09% in prediction accuracy. Tested on the same datasets, CSO has yielded improvements within the range of −7.27% to 8.51% with an average improvement 0.9% and standard deviation 3.96. The statistical tests carried out in the experiments prove that HCSO manifests a moderately better feature selection capacity than that of its counterparts.",Low
Amir Bidaki,Automatic,Springer,http://link.springer.com/article/10.1007/s10796-020-10017-4,DeepRan: Attention-based BiLSTM and CRF for Ransomware Early Detection and Classification,2021,Type,Information Systems Frontiers,Core,"Ransomware is a self-propagating malware encrypting file systems of the compromised computers to extort victims for financial gains. Hundreds of schools, hospitals, and local government municipalities have been disrupted by ransomware that already caused 12.1 days of system downtime on average (Siegel 2019). This study aims at developing a deep learning-based detector DeepRan for ransomware early detection and classification to prevent network-wide data encryption. DeepRan applies an attention-based bi-directional Long Short Term Memory (BiLSTM) with a fully connected (FC) layer to model normalcy of hosts in an operational enterprise system and detects abnormal activity from a large volume of ambient host logging data collected from bare metal servers. DeepRan also classifies abnormal activity as one of the candidate ransomware attacks by extending attention-based BiLSTM with a Conditional Random Fields (CRF) model. The Term Frequency-Inverse Document Frequency (TF-IDF) method is applied to extract semantic information from high dimensional host logging data. An incremental learning technique is used to extend the model’s existing knowledge to prevent DeepRan quality degradation over time. We develop a testbed of bare metal servers and collect normal host logs of two users for 63 days (IRB-approved). 17 ransomware attacks are executed on the victim hosts, and the infected host logging data is used for validating DeepRan. Experimental results present that DeepRan produces 99.87% detection accuracy (F1-score of 99.02%) for ransomware early detection. The detector also achieves 96.5% accuracy to classify abnormal events as one of 17 candidate ransomware families. The application of incremental learning is validated as an efficient technique to enhance model quality over time.","Ransomware detection
Classification
Testbed design
Dataset collection
Deep learning",50 Citations,,,,"Ransomware is a self-propagating malware encrypting file systems of the compromised computers to extort victims for financial gains. Hundreds of schools, hospitals, and local government municipalities have been disrupted by ransomware that already caused 12.1 days of system downtime on average (Siegel 2019). This study aims at developing a deep learning-based detector DeepRan for ransomware early detection and classification to prevent network-wide data encryption. DeepRan applies an attention-based bi-directional Long Short Term Memory (BiLSTM) with a fully connected (FC) layer to model normalcy of hosts in an operational enterprise system and detects abnormal activity from a large volume of ambient host logging data collected from bare metal servers. DeepRan also classifies abnormal activity as one of the candidate ransomware attacks by extending attention-based BiLSTM with a Conditional Random Fields (CRF) model. The Term Frequency-Inverse Document Frequency (TF-IDF) method is applied to extract semantic information from high dimensional host logging data. An incremental learning technique is used to extend the model’s existing knowledge to prevent DeepRan quality degradation over time. We develop a testbed of bare metal servers and collect normal host logs of two users for 63 days (IRB-approved). 17 ransomware attacks are executed on the victim hosts, and the infected host logging data is used for validating DeepRan. Experimental results present that DeepRan produces 99.87% detection accuracy (F1-score of 99.02%) for ransomware early detection. The detector also achieves 96.5% accuracy to classify abnormal events as one of 17 candidate ransomware families. The application of incremental learning is validated as an efficient technique to enhance model quality over time.",Medium
Amir Bidaki,Automatic,Springer,http://link.springer.com/article/10.1007/s11277-021-08359-6,Malicious Traffic classification Using Long Short-Term Memory (LSTM) Model,2021,Type,Wireless Personal Communications,Core,"Malicious traffic classification is the initial and primary step for any network-based security systems. This traffic classification systems include behavior-based anomaly detection system and Intrusion Detection System. Existing methods always relies on the conventional techniques and process the data in the fixed sequence, which may leads to performance issues. Furthermore, conventional techniques require proper annotation to process the volumetric data. Relying on the data annotation for efficient traffic classification may leads to network loops and bandwidth issues within the network. To address the above-mentioned issues, this paper presents a novel solution based on artificial intelligence perspective. The key idea of this paper is to propose a novel malicious classification system using Long Short-Term Memory (LSTM) model. To validate the efficiency of the proposed model, an experimental setup along with experimental validation is carried out. From the experimental results, it is proven that the proposed model is better in terms of accuracy, throughput when compared to the state-of-the-art models. Further, the accuracy of the proposed model outperforms the existing state of the art models with increase in 5% and overall 99.5% in accuracy.","LSTM
Traffic classification
Artificial intelligence
Malicious traffic",0,,,,"Malicious traffic classification is the initial and primary step for any network-based security systems. This traffic classification systems include behavior-based anomaly detection system and Intrusion Detection System. Existing methods always relies on the conventional techniques and process the data in the fixed sequence, which may leads to performance issues. Furthermore, conventional techniques require proper annotation to process the volumetric data. Relying on the data annotation for efficient traffic classification may leads to network loops and bandwidth issues within the network. To address the above-mentioned issues, this paper presents a novel solution based on artificial intelligence perspective. The key idea of this paper is to propose a novel malicious classification system using Long Short-Term Memory (LSTM) model. To validate the efficiency of the proposed model, an experimental setup along with experimental validation is carried out. From the experimental results, it is proven that the proposed model is better in terms of accuracy, throughput when compared to the state-of-the-art models. Further, the accuracy of the proposed model outperforms the existing state of the art models with increase in 5% and overall 99.5% in accuracy.",Low
Amir Bidaki,Automatic,Springer,http://link.springer.com/article/10.1007/s10462-023-10679-x,Deepfakes: current and future trends,2024,Type,Artificial Intelligence Review,Core,"Advances in Deep Learning (DL), Big Data and image processing have facilitated online disinformation spreading through Deepfakes. This entails severe threats including public opinion manipulation, geopolitical tensions, chaos in financial markets, scams, defamation and identity theft among others. Therefore, it is imperative to develop techniques to prevent, detect, and stop the spreading of deepfake content. Along these lines, the goal of this paper is to present a big picture perspective of the deepfake paradigm, by reviewing current and future trends. First, a compact summary of DL techniques used for deepfakes is presented. Then, a review of the fight between generation and detection techniques is elaborated. Moreover, we delve into the potential that new technologies, such as distributed ledgers and blockchain, can offer with regard to cybersecurity and the fight against digital deception. Two scenarios of application, including online social networks engineering attacks and Internet of Things, are reviewed where main insights and open challenges are tackled. Finally, future trends and research lines are discussed, pointing out potential key agents and technologies.",Artificial Intelligence,0,,,,"Advances in Deep Learning (DL), Big Data and image processing have facilitated online disinformation spreading through Deepfakes. This entails severe threats including public opinion manipulation, geopolitical tensions, chaos in financial markets, scams, defamation and identity theft among others. Therefore, it is imperative to develop techniques to prevent, detect, and stop the spreading of deepfake content. Along these lines, the goal of this paper is to present a big picture perspective of the deepfake paradigm, by reviewing current and future trends. First, a compact summary of DL techniques used for deepfakes is presented. Then, a review of the fight between generation and detection techniques is elaborated. Moreover, we delve into the potential that new technologies, such as distributed ledgers and blockchain, can offer with regard to cybersecurity and the fight against digital deception. Two scenarios of application, including online social networks engineering attacks and Internet of Things, are reviewed where main insights and open challenges are tackled. Finally, future trends and research lines are discussed, pointing out potential key agents and technologies.",Low
Amir Bidaki,Automatic,Springer,http://link.springer.com/article/10.1007/s10586-024-04297-7,SDDA-IoT: storm-based distributed detection approach for IoT network traffic-based DDoS attacks,2024,Type,Cluster Computing,Core,"In the world of connected devices, there is huge growth of less secure Internet of Things (IoT) devices, and the ease of performing sophisticated cyberattacks using these devices has posed a serious threat to the security of Internet-based services or networks. Distributed Denial of Service (DDoS) attack is one of the most significant cyberattacks. It aims to damage or exhaust victims’ resources, services, or networks and make them unavailable to legitimate users. Several solutions are available in the literature to detect DDoS attacks. However, it is difficult to detect them in real-time due to today’s high speed or high volume of attack traffic. Therefore, this paper proposes an Apache Storm-based distributed detection approach for IoT network traffic-based DDoS attacks, namely SDDA-IoT. SDDA-IoT is composed of two primary modules: model development and model deployment. In the case of model development, we created five distributed detection models by utilizing a Hadoop cluster and the extremely scalable H2O.ai machine learning platform. In the case of model deployment, we deploy an efficient distributed detection model on the Apache Storm stream processing framework for analyzing ingress streaming data and classifying it into seven classes in near-real-time. To create new models or update existing ones, this module also saves the highly discriminating input features of each network flow along with the predicted outcome in the Hadoop Distributed File System (HDFS). The effectiveness of the SDDA-IoT approach has been examined using a variety of configured scenarios. The experimental results show that the SDDA-IoT approach detects DDoS attacks faster than recent state-of-the-art methods and more accurately with 99%+ accuracy.","Apache storm stream processing framework
Bot-IoT dataset
DDoS attacks
Hadoop cluster
IoT devices
H2O.ai machine learning platform
Internet of Things",0,,,,"In the world of connected devices, there is huge growth of less secure Internet of Things (IoT) devices, and the ease of performing sophisticated cyberattacks using these devices has posed a serious threat to the security of Internet-based services or networks. Distributed Denial of Service (DDoS) attack is one of the most significant cyberattacks. It aims to damage or exhaust victims’ resources, services, or networks and make them unavailable to legitimate users. Several solutions are available in the literature to detect DDoS attacks. However, it is difficult to detect them in real-time due to today’s high speed or high volume of attack traffic. Therefore, this paper proposes an Apache Storm-based distributed detection approach for IoT network traffic-based DDoS attacks, namely SDDA-IoT. SDDA-IoT is composed of two primary modules: model development and model deployment. In the case of model development, we created five distributed detection models by utilizing a Hadoop cluster and the extremely scalable H2O.ai machine learning platform. In the case of model deployment, we deploy an efficient distributed detection model on the Apache Storm stream processing framework for analyzing ingress streaming data and classifying it into seven classes in near-real-time. To create new models or update existing ones, this module also saves the highly discriminating input features of each network flow along with the predicted outcome in the Hadoop Distributed File System (HDFS). The effectiveness of the SDDA-IoT approach has been examined using a variety of configured scenarios. The experimental results show that the SDDA-IoT approach detects DDoS attacks faster than recent state-of-the-art methods and more accurately with 99%+ accuracy.",Low
Amir Bidaki,Automatic,Springer,http://link.springer.com/article/10.1007/s10115-009-0264-5,Visualizing temporal cluster changes using Relative Density Self-Organizing Maps,2010,Type,Knowledge and Information Systems,Core,"We introduce a Self-Organizing Map (SOM)-based visualization method that compares cluster structures in temporal datasets using Relative Density SOM (ReDSOM) visualization. ReDSOM visualizations combined with distance matrix-based visualizations and cluster color linking, is capable of visually identifying emerging clusters, disappearing clusters, split clusters, merged clusters, enlarging clusters, contracting clusters, the shifting of cluster centroids, and changes in cluster density. As an example, when a region in a SOM becomes significantly more dense compared to an earlier SOM, and is well separated from other regions, then the new region can be said to represent a new cluster. The capabilities of ReDSOM are demonstrated using synthetic datasets, as well as real-life datasets from the World Bank and the Australian Taxation Office. The results on the real-life datasets demonstrate that changes identified interactively can be related to actual changes. The identification of such cluster changes is important in many contexts, including the exploration of changes in population behavior in the context of compliance and fraud in taxation.","Temporal cluster analysis
Visual data exploration
Change analysis
Self-Organizing Map
Hot spots analysis",0,,,,"We introduce a Self-Organizing Map (SOM)-based visualization method that compares cluster structures in temporal datasets using Relative Density SOM (ReDSOM) visualization. ReDSOM visualizations combined with distance matrix-based visualizations and cluster color linking, is capable of visually identifying emerging clusters, disappearing clusters, split clusters, merged clusters, enlarging clusters, contracting clusters, the shifting of cluster centroids, and changes in cluster density. As an example, when a region in a SOM becomes significantly more dense compared to an earlier SOM, and is well separated from other regions, then the new region can be said to represent a new cluster. The capabilities of ReDSOM are demonstrated using synthetic datasets, as well as real-life datasets from the World Bank and the Australian Taxation Office. The results on the real-life datasets demonstrate that changes identified interactively can be related to actual changes. The identification of such cluster changes is important in many contexts, including the exploration of changes in population behavior in the context of compliance and fraud in taxation.",Low
Amir Bidaki,Automatic,Springer,http://link.springer.com/article/10.1007/s10489-018-1149-7,Improving semi-supervised co-forest algorithm in evolving data streams,2018,Type,Applied Intelligence,Core,"Semi-supervised learning, which uses a large amount of unlabeled data to improve the performance of a classifier when only a limited amount of labeled data is available, has become a hot topic in machine learning research recently. In this paper, we propose a semi-supervised ensemble of classifiers approach, for learning in time-varying data streams. This algorithm maintains all the desirable properties of the semi-supervised Co-trained random FOREST algorithm (Co-Forest) and extends it into evolving data streams. It assigns a weight to each example according to Poisson(1) to simulate the bootstrap sample method in data streams, which is used to keep the diversity of Random Forest. By utilizing incremental learning technology, it avoids unnecessary repetition training and improves the accuracy of base models. In addition, the ADaptive WINdowing (ADWIN2) is introduced to deal with concept drift, which makes it adapt to the varying environment. Empirical evaluation on both synthetic data and UCI data reveals that our proposed method outperforms state-of-the-art semi-supervised and supervised methods in time-varying data streams, and also achieves relatively high performance in stationary streams.",Artificial Intelligence,0,,,,"Semi-supervised learning, which uses a large amount of unlabeled data to improve the performance of a classifier when only a limited amount of labeled data is available, has become a hot topic in machine learning research recently. In this paper, we propose a semi-supervised ensemble of classifiers approach, for learning in time-varying data streams. This algorithm maintains all the desirable properties of the semi-supervised Co-trained random FOREST algorithm (Co-Forest) and extends it into evolving data streams. It assigns a weight to each example according to Poisson(1) to simulate the bootstrap sample method in data streams, which is used to keep the diversity of Random Forest. By utilizing incremental learning technology, it avoids unnecessary repetition training and improves the accuracy of base models. In addition, the ADaptive WINdowing (ADWIN2) is introduced to deal with concept drift, which makes it adapt to the varying environment. Empirical evaluation on both synthetic data and UCI data reveals that our proposed method outperforms state-of-the-art semi-supervised and supervised methods in time-varying data streams, and also achieves relatively high performance in stationary streams.",Medium
Amir Bidaki,Automatic,Springer,http://link.springer.com/article/10.1186/s40537-022-00616-0,Detecting Denial of Service attacks using machine learning algorithms,2022,Type,Journal of Big Data,Core,"Currently, Distributed Denial of Service Attacks are the most dangerous cyber danger. By inhibiting the server's ability to provide resources to genuine customers, the affected server's resources, such as bandwidth and buffer size, are slowed down. A mathematical model for distributed denial-of-service attacks is proposed in this study. Machine learning algorithms such as Logistic Regression and Naive Bayes, are used to detect attacks and normal scenarios. The CAIDA 2007 Dataset is used for experimental study. The machine learning algorithms are trained and tested using this dataset and the trained algorithms are validated. Weka data mining platform are used in this study for implementation and results of the same are analysed and compared. Other machine learning algorithms used with respect to denial of service attacks are compared with the existing work.","DDOS attacks
Machine learning for security
Mathematical model for Bandwidth Depletion
Throughput analysis of attack and normal scenario",39 Citations,,,,"Currently, Distributed Denial of Service Attacks are the most dangerous cyber danger. By inhibiting the server's ability to provide resources to genuine customers, the affected server's resources, such as bandwidth and buffer size, are slowed down. A mathematical model for distributed denial-of-service attacks is proposed in this study. Machine learning algorithms such as Logistic Regression and Naive Bayes, are used to detect attacks and normal scenarios. The CAIDA 2007 Dataset is used for experimental study. The machine learning algorithms are trained and tested using this dataset and the trained algorithms are validated. Weka data mining platform are used in this study for implementation and results of the same are analysed and compared. Other machine learning algorithms used with respect to denial of service attacks are compared with the existing work.",Low
Amir Bidaki,Automatic,Springer,http://link.springer.com/article/10.1007/s10586-022-03577-4,A vector convolutional deep autonomous learning classifier for detection of cyber attacks,2022,Type,Cluster Computing,Core,"Nowadays with the exponential rise of traffic over large scale networks, Internet is vulnerable to increased number of cyber attacks. The cyber attacks attempt to steal, alter, or destroy information through unauthorized access to systems. Recently, deep learning techniques have been proposed to detect cyber attacks. The existing deep learning based detection systems perform static detection of attacks failing to capture unknown attacks happening in evolving large network traffic. The unknown attacks could be detected on the fly if a generalizable model is designed for each evolving class of network traffic. This is effectively represented in the proposed Vector Convolutional Deep Autonomous Learning (VCDAL) classifier to detect cyber attacks in the network traffic data streams. The proposed VCDAL classifier extracts the features using vector convolutional neural network, learns the features automatically using incremental learning with distilled cross entropy, and classifies the evolving network traffic using softmax function. The proposed classifier was tested by conducting experiments on benchmark network traffic datasets and it is obvious that the proposed classifier can possibly recognize both known and unknown cyber attacks. Furthermore, it is observed from the comparative analysis that the proposed VCDAL classifier exhibits significant results compared to the existing base classifiers and state-of-the-art deep learning approaches.",Artificial Intelligence,0,,,,"Nowadays with the exponential rise of traffic over large scale networks, Internet is vulnerable to increased number of cyber attacks. The cyber attacks attempt to steal, alter, or destroy information through unauthorized access to systems. Recently, deep learning techniques have been proposed to detect cyber attacks. The existing deep learning based detection systems perform static detection of attacks failing to capture unknown attacks happening in evolving large network traffic. The unknown attacks could be detected on the fly if a generalizable model is designed for each evolving class of network traffic. This is effectively represented in the proposed Vector Convolutional Deep Autonomous Learning (VCDAL) classifier to detect cyber attacks in the network traffic data streams. The proposed VCDAL classifier extracts the features using vector convolutional neural network, learns the features automatically using incremental learning with distilled cross entropy, and classifies the evolving network traffic using softmax function. The proposed classifier was tested by conducting experiments on benchmark network traffic datasets and it is obvious that the proposed classifier can possibly recognize both known and unknown cyber attacks. Furthermore, it is observed from the comparative analysis that the proposed VCDAL classifier exhibits significant results compared to the existing base classifiers and state-of-the-art deep learning approaches.",Medium
Amir Bidaki,Automatic,Springer,http://link.springer.com/article/10.1186/s40345-015-0038-9,Big data are coming to psychiatry: a general introduction,2015,Type,International Journal of Bipolar Disorders,Core,"Big data are coming to the study of bipolar disorder and all of psychiatry. Data are coming from providers and payers (including EMR, imaging, insurance claims and pharmacy data), from omics (genomic, proteomic, and metabolomic data), and from patients and non-providers (data from smart phone and Internet activities, sensors and monitoring tools). Analysis of the big data will provide unprecedented opportunities for exploration, descriptive observation, hypothesis generation, and prediction, and the results of big data studies will be incorporated into clinical practice. Technical challenges remain in the quality, analysis and management of big data. This paper discusses some of the fundamental opportunities and challenges of big data for psychiatry.","Bipolar Disorder
Healthcare Data
Internet Activity
Remote Patient Monitoring
Cancer Biomedical Informatics Grid",57 Citations,,,,"Big data are coming to the study of bipolar disorder and all of psychiatry. Data are coming from providers and payers (including EMR, imaging, insurance claims and pharmacy data), from omics (genomic, proteomic, and metabolomic data), and from patients and non-providers (data from smart phone and Internet activities, sensors and monitoring tools). Analysis of the big data will provide unprecedented opportunities for exploration, descriptive observation, hypothesis generation, and prediction, and the results of big data studies will be incorporated into clinical practice. Technical challenges remain in the quality, analysis and management of big data. This paper discusses some of the fundamental opportunities and challenges of big data for psychiatry.",Low
Amir Bidaki,Automatic,Springer,http://link.springer.com/article/10.1007/s11227-023-05843-7,Iot traffic-based DDoS attacks detection mechanisms: A comprehensive review,2024,Type,The Journal of Supercomputing,Core,"The Internet of Things (IoT) has emerged as an inevitable part of human life, that includes online learning, smart homes, smart cars, smart grids, smart cities, agriculture, and e-healthcare. It allows us to operate them 24/7 from anywhere. These smart IoT devices streamline our daily lives by automating everything around us. Several security issues have arisen with the continuous growth of non-secure IoT devices. Distributed Denial of Service (DDoS) attack is one of the most prominent security threats to Internet-based services and IoT platforms. It has the potential to break down the victim’s server or network by transferring an immense amount of irrelevant traffic from the pool of compromised IoT devices. In this article, we present: (i) A comprehensive cyberattacks taxonomy for IoT platforms, (ii) Systematically demonstrate IoT technology: evolution, applications, and challenges, (iv) Systematic review of existing machine learning (ML) and deep learning (DL)-based detection approaches for large-scale IoT traffic-based DDoS attacks, (v) Characterize publicly available IoT-traffic-specific datasets, and (vi) Discuss various open research issues with possible solutions for detecting IoT traffic-based DDoS attacks, including future directions.","Internet of Things (IoT)
IoT platforms
Distributed denial of service (DDoS) attacks
Machine learning
Deep learning
Cyberattacks taxonomy.",0,,,,"The Internet of Things (IoT) has emerged as an inevitable part of human life, that includes online learning, smart homes, smart cars, smart grids, smart cities, agriculture, and e-healthcare. It allows us to operate them 24/7 from anywhere. These smart IoT devices streamline our daily lives by automating everything around us. Several security issues have arisen with the continuous growth of non-secure IoT devices. Distributed Denial of Service (DDoS) attack is one of the most prominent security threats to Internet-based services and IoT platforms. It has the potential to break down the victim’s server or network by transferring an immense amount of irrelevant traffic from the pool of compromised IoT devices. In this article, we present: (i) A comprehensive cyberattacks taxonomy for IoT platforms, (ii) Systematically demonstrate IoT technology: evolution, applications, and challenges, (iv) Systematic review of existing machine learning (ML) and deep learning (DL)-based detection approaches for large-scale IoT traffic-based DDoS attacks, (v) Characterize publicly available IoT-traffic-specific datasets, and (vi) Discuss various open research issues with possible solutions for detecting IoT traffic-based DDoS attacks, including future directions.",Low
Amir Bidaki,Automatic,Springer,http://link.springer.com/article/10.1007/s12065-024-00953-4,ARDOD: adaptive radius density-based outlier detection,2024,Type,Evolutionary Intelligence,Core,"Outlier detection has garnered considerable attention in recent years due to its wide-ranging applications across various research domains. This surge in interest has led to the development of numerous detection techniques, predominantly based on distance or density metrics. A notable limitation of these existing methods is their reliance on parameter adjustments, significantly affecting the outcome. Additionally, these methods exhibit intrinsic flaws: distance-based approaches struggle with clusters with varying local densities, while density-based methods fail to identify patterns within low-density areas. Moreover, most prior techniques are adept at identifying only one kind of outlier—local, global, or group of outliers. Addressing these challenges, we introduce the Adaptive Radius Density-Based Outlier Detection (ARDOD) method, which departs from the traditional parameter-dependent approach. ARDOD is a novel parameter-free algorithm that dynamically determines the necessary parameters based on the data distribution within the feature space. This innovative method demonstrates robust performance in detecting all three categories of outliers. The efficacy and superior performance of ARDOD are validated through an extensive experimental analysis involving various synthetic and real-world datasets. This analysis showcases ARDOD's advantages over seven established methods: Local Outlier Factor (LOF), Angle-Based Outlier Detection (ABOD), Robust Distance-Based Outlier Score (RDOS), Directed density ratio Changing Rate-based outlier detection (DCROD), Empirical-Cumulative-distribution-based Outlier Detection(ECOD), mean-shift outlier detector(MOD +),and Local–Global Outlier Detection (LGOD), underscoring its potential as a versatile tool in outlier detection research.",Artificial Intelligence,0,,,,"Outlier detection has garnered considerable attention in recent years due to its wide-ranging applications across various research domains. This surge in interest has led to the development of numerous detection techniques, predominantly based on distance or density metrics. A notable limitation of these existing methods is their reliance on parameter adjustments, significantly affecting the outcome. Additionally, these methods exhibit intrinsic flaws: distance-based approaches struggle with clusters with varying local densities, while density-based methods fail to identify patterns within low-density areas. Moreover, most prior techniques are adept at identifying only one kind of outlier—local, global, or group of outliers. Addressing these challenges, we introduce the Adaptive Radius Density-Based Outlier Detection (ARDOD) method, which departs from the traditional parameter-dependent approach. ARDOD is a novel parameter-free algorithm that dynamically determines the necessary parameters based on the data distribution within the feature space. This innovative method demonstrates robust performance in detecting all three categories of outliers. The efficacy and superior performance of ARDOD are validated through an extensive experimental analysis involving various synthetic and real-world datasets. This analysis showcases ARDOD's advantages over seven established methods: Local Outlier Factor (LOF), Angle-Based Outlier Detection (ABOD), Robust Distance-Based Outlier Score (RDOS), Directed density ratio Changing Rate-based outlier detection (DCROD), Empirical-Cumulative-distribution-based Outlier Detection(ECOD), mean-shift outlier detector(MOD +),and Local–Global Outlier Detection (LGOD), underscoring its potential as a versatile tool in outlier detection research.",Low
Amir Bidaki,Automatic,Springer,http://link.springer.com/article/10.1007/s10618-024-01011-4,Online concept evolution detection based on active learning,2024,Type,Data Mining and Knowledge Discovery,Core,"Concept evolution detection is an important and difficult problem in streaming data mining. When the labeled samples in streaming data insufficient to reflect the training data distribution, it will often further restrict the detection performance. This paper proposed a concept evolution detection method based on active learning (CE_AL). Firstly, the initial classifiers are constructed by a small number of labeled samples. The sample areas are divided into the automatic labeling and the active labeling areas according to the relationship between the classifiers of different categories. Secondly, for online new coming samples, according to their different areas, two strategies based on the automatic learning-based model labeling and active learning-based expert labeling are adopted respectively, which can improve the online learning performance with only a small number of labeled samples. Besides, the strategy of “data enhance” combined with “model enhance” is adopted to accelerate the convergence of the evolution category detection model. The experimental results show that the proposed CE_AL method can enhance the detection performance of concept evolution and realize efficient learning in an unstable environment by labeling a small number of key samples.",Artificial Intelligence,0,,,,"Concept evolution detection is an important and difficult problem in streaming data mining. When the labeled samples in streaming data insufficient to reflect the training data distribution, it will often further restrict the detection performance. This paper proposed a concept evolution detection method based on active learning (CE_AL). Firstly, the initial classifiers are constructed by a small number of labeled samples. The sample areas are divided into the automatic labeling and the active labeling areas according to the relationship between the classifiers of different categories. Secondly, for online new coming samples, according to their different areas, two strategies based on the automatic learning-based model labeling and active learning-based expert labeling are adopted respectively, which can improve the online learning performance with only a small number of labeled samples. Besides, the strategy of “data enhance” combined with “model enhance” is adopted to accelerate the convergence of the evolution category detection model. The experimental results show that the proposed CE_AL method can enhance the detection performance of concept evolution and realize efficient learning in an unstable environment by labeling a small number of key samples.",Medium
Amir Bidaki,Automatic,Springer,http://link.springer.com/article/10.1007/s00521-024-10281-4,Incremental federated learning for traffic flow classification in heterogeneous data scenarios,2024,Type,Neural Computing and Applications,Core,"This paper explores the comparative analysis of federated learning (FL) and centralized learning (CL) models in the context of multi-class traffic flow classification for network applications, a timely study in the context of increasing privacy preservation concerns. Unlike existing literature that often omits detailed class-wise performance evaluation, and consistent data handling and feature selection approaches, our study rectifies these gaps by implementing a feed-forward neural network and assessing FL performance under both independent and identically distributed (IID) and non-independent and identically distributed (non-IID) conditions, with a particular focus on incremental training. In our cross-silo experimental setup involving five clients per round, FL models exhibit notable adaptability. Under IID conditions, the accuracy of the FL model peaked at 96.65%, demonstrating its robustness. Moreover, despite the challenges presented by non-IID environments, our FL models demonstrated significant resilience, adapting incrementally over rounds to optimize performance; in most scenarios, our FL models performed comparably to the idealistic CL model regarding multiple well-established metrics. Through a comprehensive traffic flow classification use case, this work (i) contributes to a better understanding of the capabilities and limitations of FL, offering valuable insights for the real-world deployment of FL, and (ii) provides a novel, large, carefully curated traffic flow dataset for the research community.",Artificial Intelligence,0,,,,"This paper explores the comparative analysis of federated learning (FL) and centralized learning (CL) models in the context of multi-class traffic flow classification for network applications, a timely study in the context of increasing privacy preservation concerns. Unlike existing literature that often omits detailed class-wise performance evaluation, and consistent data handling and feature selection approaches, our study rectifies these gaps by implementing a feed-forward neural network and assessing FL performance under both independent and identically distributed (IID) and non-independent and identically distributed (non-IID) conditions, with a particular focus on incremental training. In our cross-silo experimental setup involving five clients per round, FL models exhibit notable adaptability. Under IID conditions, the accuracy of the FL model peaked at 96.65%, demonstrating its robustness. Moreover, despite the challenges presented by non-IID environments, our FL models demonstrated significant resilience, adapting incrementally over rounds to optimize performance; in most scenarios, our FL models performed comparably to the idealistic CL model regarding multiple well-established metrics. Through a comprehensive traffic flow classification use case, this work (i) contributes to a better understanding of the capabilities and limitations of FL, offering valuable insights for the real-world deployment of FL, and (ii) provides a novel, large, carefully curated traffic flow dataset for the research community.",Medium
Amir Bidaki,Automatic,Springer,http://link.springer.com/article/10.1007/s10462-018-09679-z,Machine Learning and Deep Learning frameworks and libraries for large-scale data mining: a survey,2019,Type,Artificial Intelligence Review,Core,"The combined impact of new computing resources and techniques with an increasing avalanche of large datasets, is transforming many research areas and may lead to technological breakthroughs that can be used by billions of people. In the recent years, Machine Learning and especially its subfield Deep Learning have seen impressive advances. Techniques developed within these two fields are now able to analyze and learn from huge amounts of real world examples in a disparate formats. While the number of Machine Learning algorithms is extensive and growing, their implementations through frameworks and libraries is also extensive and growing too. The software development in this field is fast paced with a large number of open-source software coming from the academy, industry, start-ups or wider open-source communities. This survey presents a recent time-slide comprehensive overview with comparisons as well as trends in development and usage of cutting-edge Artificial Intelligence software. It also provides an overview of massive parallelism support that is capable of scaling computation effectively and efficiently in the era of Big Data.",Artificial Intelligence,0,,,,"The combined impact of new computing resources and techniques with an increasing avalanche of large datasets, is transforming many research areas and may lead to technological breakthroughs that can be used by billions of people. In the recent years, Machine Learning and especially its subfield Deep Learning have seen impressive advances. Techniques developed within these two fields are now able to analyze and learn from huge amounts of real world examples in a disparate formats. While the number of Machine Learning algorithms is extensive and growing, their implementations through frameworks and libraries is also extensive and growing too. The software development in this field is fast paced with a large number of open-source software coming from the academy, industry, start-ups or wider open-source communities. This survey presents a recent time-slide comprehensive overview with comparisons as well as trends in development and usage of cutting-edge Artificial Intelligence software. It also provides an overview of massive parallelism support that is capable of scaling computation effectively and efficiently in the era of Big Data.",Low
Amir Bidaki,Automatic,Springer,http://link.springer.com/article/10.1007/s10586-023-04187-4,DDoS attack detection in IoT environment using optimized Elman recurrent neural networks based on chaotic bacterial colony optimization,2024,Type,Cluster Computing,Core,"The Internet of Things (IoT) is made up of billions of interconnected devices that can transmit and receive data over the Internet. IoT devices have many vulnerabilities that attackers could use to compromise their security because of the heterogeneity of device connectivity. Distributed denial-of-service (DDoS) attacks against those applications become more common as IoT applications continue to expand and devolve. Identifying DDoS attacks is a difficult process due to the variety of IoT devices connected. The present article proposed a new method to detect DDoS attacks using an optimized Elman recurrent neural network (ERNN) based on chaotic bacterial colony optimization (CBCO) called CBCO-ERNN. The proposed method uses CBCO for obtaining optimal parameters (weights and biases) and structure (number of hidden neurons) of ERNN architecture. The chaos theory is applied to improve BCO’s exploration and exploitation capabilities by initializing the bacterial population and selecting the appropriate chemotaxis step size value. The CBCO approach is used to train the ERNN model to avoid local optima and enhance the convergence rate. The performance of the CBCO-ERNN is tested and evaluated using four benchmark attack datasets such as the BoT-IoT, CIC-IDS2017, CIC-DDoS2019, and IoTID20 datasets, and five performance metrics are considered: accuracy, sensitivity, specificity, precision, and F-Score. According to the experimental results, the CBCO-ERNN method provides a high detection and a faster convergence rate when compared to earlier algorithms.","DDoS attack detection
Elman recurrent neural network
Bacterial colony optimization
Chaotic theory
Internet of Things (IoT)
Convergence rate",15 Citations,,,,"The Internet of Things (IoT) is made up of billions of interconnected devices that can transmit and receive data over the Internet. IoT devices have many vulnerabilities that attackers could use to compromise their security because of the heterogeneity of device connectivity. Distributed denial-of-service (DDoS) attacks against those applications become more common as IoT applications continue to expand and devolve. Identifying DDoS attacks is a difficult process due to the variety of IoT devices connected. The present article proposed a new method to detect DDoS attacks using an optimized Elman recurrent neural network (ERNN) based on chaotic bacterial colony optimization (CBCO) called CBCO-ERNN. The proposed method uses CBCO for obtaining optimal parameters (weights and biases) and structure (number of hidden neurons) of ERNN architecture. The chaos theory is applied to improve BCO’s exploration and exploitation capabilities by initializing the bacterial population and selecting the appropriate chemotaxis step size value. The CBCO approach is used to train the ERNN model to avoid local optima and enhance the convergence rate. The performance of the CBCO-ERNN is tested and evaluated using four benchmark attack datasets such as the BoT-IoT, CIC-IDS2017, CIC-DDoS2019, and IoTID20 datasets, and five performance metrics are considered: accuracy, sensitivity, specificity, precision, and F-Score. According to the experimental results, the CBCO-ERNN method provides a high detection and a faster convergence rate when compared to earlier algorithms.",Low
Amir Bidaki,Automatic,Springer,http://link.springer.com/article/10.1007/s13721-016-0135-4,"Big data analytics in bioinformatics: architectures, techniques, tools and issues",2016,Type,Network Modeling Analysis in Health Informatics and Bioinformatics,Core,"Bioinformatics research is characterized by voluminous and incremental datasets and complex data analytics methods. The machine learning methods used in bioinformatics are iterative and parallel. These methods can be scaled to handle big data using the distributed and parallel computing technologies. Usually big data tools perform computation in batch mode and are not optimized for iterative processing and high data dependency among operations. In the recent years, parallel, incremental, and multi-view machine learning algorithms have been proposed. Similarly, graph-based architectures and in-memory big data tools have been developed to minimize I/O cost and optimize iterative processing. However, standard big data architectures are still lacking. Also appropriate tools are not available for many important bioinformatics problems, such as fast construction of co-expression and regulatory networks and salient module identification, detection of complexes over growing protein-protein interaction data, fast analysis of massive DNA, RNA, and protein sequence data, and fast querying on incremental and heterogeneous disease networks. This paper addresses the issues and challenges posed by several big data problems in bioinformatics, and gives an overview of the state of the art and the future research opportunities.","Big data
Bioinformatics
Machine learning
MapReduce
Clustering
Gene regulatory network",25 Citations,,,,"Bioinformatics research is characterized by voluminous and incremental datasets and complex data analytics methods. The machine learning methods used in bioinformatics are iterative and parallel. These methods can be scaled to handle big data using the distributed and parallel computing technologies. Usually big data tools perform computation in batch mode and are not optimized for iterative processing and high data dependency among operations. In the recent years, parallel, incremental, and multi-view machine learning algorithms have been proposed. Similarly, graph-based architectures and in-memory big data tools have been developed to minimize I/O cost and optimize iterative processing. However, standard big data architectures are still lacking. Also appropriate tools are not available for many important bioinformatics problems, such as fast construction of co-expression and regulatory networks and salient module identification, detection of complexes over growing protein-protein interaction data, fast analysis of massive DNA, RNA, and protein sequence data, and fast querying on incremental and heterogeneous disease networks. This paper addresses the issues and challenges posed by several big data problems in bioinformatics, and gives an overview of the state of the art and the future research opportunities.",Low
Amir Bidaki,Automatic,Springer,http://link.springer.com/article/10.1007/s11277-024-11177-1,"IoT Data Stream Handling, Analysis, Communication and Security Issues: A Systematic Survey",2024,Type,Wireless Personal Communications,Core,"Massive data management or data stream handling has become a prominent study topic as a result of recent breakthroughs in the Internet of Things (IoT). With the developments in big data technologies and their applications in the IoT realm, the analyzing, processing,, and handling streams of data and information are attaining the vast attention of researchers. In IoT, continuous information generation, handling and secure communication are achieved through innumerable devices. This information is enormous and time-varying. In all IoT applications processing, analyzing, and handling such information are extremely critical, as the ceaseless flow of information and time-sensitive decisions may make it inefficacious to process after storage. Therefore, processing such massive or streams of information must be executed in real-time. A better understanding of the stream data handling, analysis and communication aspect can help in effective data management. This review work provides the vitality of secure data stream handling and analysis in IoT applications. It explains the critical features and requirements of IoT data for the analysis and security concerns in industrial applications like smart city, smart agriculture and smart transportation system. It explores various studies emphasizing massive data management and the complexities involved with it. This article further investigates the different models and techniques utilized for handling IoT stream data in existing research works and enumerates their merits in stream data performance management. The review work also describes the technical challenges and research gap existing in-stream data handling and crucial further developments needed in this arena for effective management of IoT stream data.","IoT
Big data
Stream data
Data processing
Streaming data analytics
Big data analytics
Stream data handling
Security
Communication",0,,,,"Massive data management or data stream handling has become a prominent study topic as a result of recent breakthroughs in the Internet of Things (IoT). With the developments in big data technologies and their applications in the IoT realm, the analyzing, processing,, and handling streams of data and information are attaining the vast attention of researchers. In IoT, continuous information generation, handling and secure communication are achieved through innumerable devices. This information is enormous and time-varying. In all IoT applications processing, analyzing, and handling such information are extremely critical, as the ceaseless flow of information and time-sensitive decisions may make it inefficacious to process after storage. Therefore, processing such massive or streams of information must be executed in real-time. A better understanding of the stream data handling, analysis and communication aspect can help in effective data management. This review work provides the vitality of secure data stream handling and analysis in IoT applications. It explains the critical features and requirements of IoT data for the analysis and security concerns in industrial applications like smart city, smart agriculture and smart transportation system. It explores various studies emphasizing massive data management and the complexities involved with it. This article further investigates the different models and techniques utilized for handling IoT stream data in existing research works and enumerates their merits in stream data performance management. The review work also describes the technical challenges and research gap existing in-stream data handling and crucial further developments needed in this arena for effective management of IoT stream data.",Medium
Amir Bidaki,Automatic,Springer,http://link.springer.com/article/10.1007/s00354-024-00286-x,Ensemble Learning with Extremely Randomized k-Nearest Neighbors for Accurate and Efficient Classification,2024,Type,New Generation Computing,Core,"Ensemble learning has emerged as a potent methodology for enhancing the accuracy, stability, and robustness of machine learning models. In this paper, we introduce a novel ensemble learning approach that primarily uses nearest neighbor modeling (kNN) and genetic algorithms (GA) to build a highly efficient classification model, which we call “Extra-kNNs”. This model operates across three distinct layers. First, we employ strong randomization of both bootstrap sampling and kNN hyperparameters to build a diverse collection of kNNs in the input layer. Next, we improve their efficiency using a clustering-based transformation layer. Finally, the output layer optimizes the ensemble’s performance using a GA algorithm to build an effective ensemble model. The proposed approach addresses the limitations of kNN, including sensitivity to noise, challenges in high-dimensional data, and its relative ineffectiveness in ensemble methods compared to decision trees. We tested our method rigorously on 15 real-world datasets, comparing its performance to several individual and ensemble models. Our empirical findings demonstrate that the suggested model achieves higher accuracy and classification efficiency than state-of-the-art algorithms.",Artificial Intelligence,0,,,,"Ensemble learning has emerged as a potent methodology for enhancing the accuracy, stability, and robustness of machine learning models. In this paper, we introduce a novel ensemble learning approach that primarily uses nearest neighbor modeling (kNN) and genetic algorithms (GA) to build a highly efficient classification model, which we call “Extra-kNNs”. This model operates across three distinct layers. First, we employ strong randomization of both bootstrap sampling and kNN hyperparameters to build a diverse collection of kNNs in the input layer. Next, we improve their efficiency using a clustering-based transformation layer. Finally, the output layer optimizes the ensemble’s performance using a GA algorithm to build an effective ensemble model. The proposed approach addresses the limitations of kNN, including sensitivity to noise, challenges in high-dimensional data, and its relative ineffectiveness in ensemble methods compared to decision trees. We tested our method rigorously on 15 real-world datasets, comparing its performance to several individual and ensemble models. Our empirical findings demonstrate that the suggested model achieves higher accuracy and classification efficiency than state-of-the-art algorithms.",Low
Amir Bidaki,Automatic,Springer,http://link.springer.com/article/10.1007/s10462-022-10375-2,"Heterogeneous graph neural networks analysis: a survey of techniques, evaluations and applications",2023,Type,Artificial Intelligence Review,Core,"Graph Neural Networks (GNNs) have achieved excellent performance of graph representation learning and attracted plenty of attentions in recent years. Most of GNNs aim to learn embedding vectors of the homogeneous graph which only contains single type of nodes and edges. However, the entities and their interactions in real world always have multiple types and naturally form the heterogeneous graph with rich structural and semantic information. As a result of this, it is beneficial to advance heterogeneous graph representation learning that can effectively promote the performance of complex network analysis. Existing survey papers of heterogeneous graph representation learning summarize all possible embedding techniques for graphs and make insufficient analysis for deep neural network models. To tackle this issue, in this paper, we systematically summarize and analyze existing heterogeneous graph neural networks (HGNNs) and categorize them based on their neural network architecture. Meanwhile, we collect commonly used heterogeneous graph datasets and summarize their statistical information. In addition, we compare the performances between HGNNs and shallow embedding models to show the powerful feature learning ability of HGNNs. Finally, we conclude the application scenarios of HGNNs and some possible future research directions. We hope that this paper can provide a useful framework for researchers who interested in HGNNs.",Artificial Intelligence,0,,,,"Graph Neural Networks (GNNs) have achieved excellent performance of graph representation learning and attracted plenty of attentions in recent years. Most of GNNs aim to learn embedding vectors of the homogeneous graph which only contains single type of nodes and edges. However, the entities and their interactions in real world always have multiple types and naturally form the heterogeneous graph with rich structural and semantic information. As a result of this, it is beneficial to advance heterogeneous graph representation learning that can effectively promote the performance of complex network analysis. Existing survey papers of heterogeneous graph representation learning summarize all possible embedding techniques for graphs and make insufficient analysis for deep neural network models. To tackle this issue, in this paper, we systematically summarize and analyze existing heterogeneous graph neural networks (HGNNs) and categorize them based on their neural network architecture. Meanwhile, we collect commonly used heterogeneous graph datasets and summarize their statistical information. In addition, we compare the performances between HGNNs and shallow embedding models to show the powerful feature learning ability of HGNNs. Finally, we conclude the application scenarios of HGNNs and some possible future research directions. We hope that this paper can provide a useful framework for researchers who interested in HGNNs.",Low
Amir Bidaki,Automatic,Springer,http://link.springer.com/article/10.1007/s10845-024-02536-7,Large scale foundation models for intelligent manufacturing applications: a survey,2025,Type,Journal of Intelligent Manufacturing,Core,"Although the applications of artificial intelligence especially deep learning have greatly improved various aspects of intelligent manufacturing, they still face challenges for broader adoption due to poor generalization ability, difficulties in establishing high-quality training datasets, and unsatisfactory performance of deep learning methods. The emergence of large scale foundation models (LSFMs) has transformed applications of deep learning models from single task, single-modal, limited data patterns to a new paradigm encompassing diverse tasks, multi-modal, and pre-training on massive datasets. Although LSFMs have demonstrated powerful generalization capabilities, efficient data utilization and superior performance in various domains, applications of LSFMs in intelligent manufacturing are still in their nascent stage. A systematic overview of this topic is lacking, especially regarding challenges of utilizing deep learning in intelligent manufacturing and how these challenges can be systematically tackled by LSFMs. To fill this gap, this survey provides a systematic overview of the current status of LSFMs and their advantages in the context of intelligent manufacturing. It also presents comprehensive comparisons between current deep learning models and LSFMs in various intelligent manufacturing applications. Subsequently, roadmaps for utilizing LSFMs to address these challenges are also outlined. Case studies of employing LSFMs in real-world intelligent manufacturing scenarios are also presented highlighting how LSFMs can enhance industry efficiency. Finally, the challenges currently faced by LSFMs in intelligent manufacturing are discussed, along with future research directions. The survey highlights that the application of LSFMs in intelligent manufacturing is rapidly gaining widespread attention and demonstrating potential across various tasks.",Artificial Intelligence,0,,,,"Although the applications of artificial intelligence especially deep learning have greatly improved various aspects of intelligent manufacturing, they still face challenges for broader adoption due to poor generalization ability, difficulties in establishing high-quality training datasets, and unsatisfactory performance of deep learning methods. The emergence of large scale foundation models (LSFMs) has transformed applications of deep learning models from single task, single-modal, limited data patterns to a new paradigm encompassing diverse tasks, multi-modal, and pre-training on massive datasets. Although LSFMs have demonstrated powerful generalization capabilities, efficient data utilization and superior performance in various domains, applications of LSFMs in intelligent manufacturing are still in their nascent stage. A systematic overview of this topic is lacking, especially regarding challenges of utilizing deep learning in intelligent manufacturing and how these challenges can be systematically tackled by LSFMs. To fill this gap, this survey provides a systematic overview of the current status of LSFMs and their advantages in the context of intelligent manufacturing. It also presents comprehensive comparisons between current deep learning models and LSFMs in various intelligent manufacturing applications. Subsequently, roadmaps for utilizing LSFMs to address these challenges are also outlined. Case studies of employing LSFMs in real-world intelligent manufacturing scenarios are also presented highlighting how LSFMs can enhance industry efficiency. Finally, the challenges currently faced by LSFMs in intelligent manufacturing are discussed, along with future research directions. The survey highlights that the application of LSFMs in intelligent manufacturing is rapidly gaining widespread attention and demonstrating potential across various tasks.",Low
Amir Bidaki,Automatic,Springer,http://link.springer.com/article/10.1007/s11664-024-11495-x,A Brief Review of Key Technologies for Cloud-Based Battery Management Systems,2024,Type,Journal of Electronic Materials,Core,"Key technologies in cloud-based battery management systems (CBMS) significantly enhance battery management efficiency and reliability compared to traditional battery management systems (BMS). This paper first reviews the development of CBMS, introducing their evolution from early BMS to the current, complex cloud-computing-integrated systems. It discusses in detail how technological advancements have driven innovations in CBMS, including data acquisition and sensing technologies, communication technologies and protocols, cloud computing, artificial intelligence and machine learning, digital twins, edge computing, intelligent decision support, system integration, and security technologies. For each key technology, the paper provides relevant definitions, functions, applications, pros, and cons. Then, a summary comparison of BMS and CBMS is presented, with CBMS highlighted for its advantages, which are supported by key technologies. Additionally, the paper discusses the current challenges and future research directions in CBMS from three perspectives: data, algorithms and models, and technologies. By reviewing these developments, the paper provides a foundation for understanding the technical underpinnings and future trends of CBMS.","Technology
cloud-based battery management systems
application
SOC
SOH",0,,,,"Key technologies in cloud-based battery management systems (CBMS) significantly enhance battery management efficiency and reliability compared to traditional battery management systems (BMS). This paper first reviews the development of CBMS, introducing their evolution from early BMS to the current, complex cloud-computing-integrated systems. It discusses in detail how technological advancements have driven innovations in CBMS, including data acquisition and sensing technologies, communication technologies and protocols, cloud computing, artificial intelligence and machine learning, digital twins, edge computing, intelligent decision support, system integration, and security technologies. For each key technology, the paper provides relevant definitions, functions, applications, pros, and cons. Then, a summary comparison of BMS and CBMS is presented, with CBMS highlighted for its advantages, which are supported by key technologies. Additionally, the paper discusses the current challenges and future research directions in CBMS from three perspectives: data, algorithms and models, and technologies. By reviewing these developments, the paper provides a foundation for understanding the technical underpinnings and future trends of CBMS.",Low
Amir Bidaki,Automatic,Springer,http://link.springer.com/article/10.1007/s11227-022-04847-z,Sequential semi-supervised active learning model in extremely low training set (SSSAL),2023,Type,The Journal of Supercomputing,Core,"With the rapid development of computing and multimedia technology, the volume of web traffic data, social networks, sensors and other types of resources is increasing rapidly. Most of the data are unlabeled, and only a very small percentage of it is labeled. In some cases, data tags can be obtained free or at a low cost, while for many complex tasks such as speech recognition, data extraction, classification and filtering, tagging is usually difficult, time-consuming and expensive, because it requires manual interpretation by human experts. Active learning examines the issue by selecting the most active data to query tags and achieving high learning accuracy with a small number of tagged items. There are different approaches for data classification which have disadvantages such as low accuracy or high computational time. In this study, by combining covariance-based self-expression correlation estimation and incremental active learning methods, a new three-step method to sequential semi-supervising classification is presented. Initially, a new method based on covariance is used to estimate the correlation between samples. Then, a semi-supervised active learning algorithm based on GAN (Generative Adversarial Networks) is introduced to perform the learning process. Finally, a new incremental learning model based on support vector machine is used for classification. We use 20-Newsgroups, Web KB and Image Net datasets to test the performance of our proposed method. The experimental results show the robustness and effectiveness of the proposed method in terms of accuracy, precision, recall and f-measure factors compared to other methods.","Semi-supervised learning
Active learning
Correlation estimation",0,,,,"With the rapid development of computing and multimedia technology, the volume of web traffic data, social networks, sensors and other types of resources is increasing rapidly. Most of the data are unlabeled, and only a very small percentage of it is labeled. In some cases, data tags can be obtained free or at a low cost, while for many complex tasks such as speech recognition, data extraction, classification and filtering, tagging is usually difficult, time-consuming and expensive, because it requires manual interpretation by human experts. Active learning examines the issue by selecting the most active data to query tags and achieving high learning accuracy with a small number of tagged items. There are different approaches for data classification which have disadvantages such as low accuracy or high computational time. In this study, by combining covariance-based self-expression correlation estimation and incremental active learning methods, a new three-step method to sequential semi-supervising classification is presented. Initially, a new method based on covariance is used to estimate the correlation between samples. Then, a semi-supervised active learning algorithm based on GAN (Generative Adversarial Networks) is introduced to perform the learning process. Finally, a new incremental learning model based on support vector machine is used for classification. We use 20-Newsgroups, Web KB and Image Net datasets to test the performance of our proposed method. The experimental results show the robustness and effectiveness of the proposed method in terms of accuracy, precision, recall and f-measure factors compared to other methods.",Medium
Amir Bidaki,Automatic,Springer,http://link.springer.com/article/10.1007/s11276-023-03573-5,Intelligent biomedical image classification in a big data architecture using metaheuristic optimization and gradient approximation,2024,Type,Wireless Networks,Core,"Medical imaging has experienced significant development in contemporary medicine and can now record a variety of biomedical pictures from patients to test and analyze the illness and its severity. Computer vision and artificial intelligence may outperform human diagnostic ability and uncover hidden information in biomedical images. In healthcare applications, fast prediction and reliability are of the utmost importance parameters to assure the timely detection of disease. The existing systems have poor classification accuracy, and higher computation time and the system complexity is higher. Low-quality images might impact the processing method, leading to subpar results. Furthermore, extensive preprocessing techniques are necessary for achieving accurate outcomes. Image contrast is one of the most essential visual parameters. Insufficient contrast may present many challenges for computer vision techniques. Traditional contrast adjustment techniques may not be adequate for many applications. Occasionally, these technologies create photos that lack crucial information. The primary contribution of this work is designing a Big Data Architecture (BDA) to improve the dependability of medical systems by producing real-time warnings and making precise forecasts about patient health conditions. A BDA-based Bio-Medical Image Classification (BDA-BMIC) system is designed to detect the illness of patients using Metaheuristic Optimization (Genetic Algorithm) and Gradient Approximation to improve the biomedical image classification process. Extensive tests are conducted on publicly accessible datasets to demonstrate that the suggested retrieval and categorization methods are superior to the current methods. The suggested BDA-BMIC system has average detection accuracy of 94.6% and a sensitivity of 97.3% in the simulation analysis.","Biomedical image classification
Metaheuristic optimization
Gradient approximation
Big data architecture",0,,,,"Medical imaging has experienced significant development in contemporary medicine and can now record a variety of biomedical pictures from patients to test and analyze the illness and its severity. Computer vision and artificial intelligence may outperform human diagnostic ability and uncover hidden information in biomedical images. In healthcare applications, fast prediction and reliability are of the utmost importance parameters to assure the timely detection of disease. The existing systems have poor classification accuracy, and higher computation time and the system complexity is higher. Low-quality images might impact the processing method, leading to subpar results. Furthermore, extensive preprocessing techniques are necessary for achieving accurate outcomes. Image contrast is one of the most essential visual parameters. Insufficient contrast may present many challenges for computer vision techniques. Traditional contrast adjustment techniques may not be adequate for many applications. Occasionally, these technologies create photos that lack crucial information. The primary contribution of this work is designing a Big Data Architecture (BDA) to improve the dependability of medical systems by producing real-time warnings and making precise forecasts about patient health conditions. A BDA-based Bio-Medical Image Classification (BDA-BMIC) system is designed to detect the illness of patients using Metaheuristic Optimization (Genetic Algorithm) and Gradient Approximation to improve the biomedical image classification process. Extensive tests are conducted on publicly accessible datasets to demonstrate that the suggested retrieval and categorization methods are superior to the current methods. The suggested BDA-BMIC system has average detection accuracy of 94.6% and a sensitivity of 97.3% in the simulation analysis.",Low
Amir Bidaki,Automatic,Springer,http://link.springer.com/article/10.1007/s10586-019-03031-y,An anomaly-based framework for mitigating effects of DDoS attacks using a third party auditor in cloud computing environments,2020,Type,Cluster Computing,Core,"Today, the providers of cloud computing services are among the most prominent service suppliers worldwide. Availability of cloud services is one of the most important concerns of cloud service providers (CSPs) and cloud users (CUs). Distributed Denial of Service (DDoS) attacks are common types of security issues which affect cloud services and consequently, can lead to unavailability of the services. Therefore, reducing the effects of DDoS attacks helps CSPs to provide high quality services to CUs. In this paper, first, we propose an anomaly-based DDoS attack detection framework in cloud environment using a third party auditor (TPA). Second, we provide multiple basic assumptions and configurations of cloud environments for establishing simulation tests to evaluate our proposed framework. Then, we provide results of simulation tests to analyze the feasibility of our approach. Simulation results demonstrate that our method for detecting DDoS attacks in CSPs has following advantages: efficiency, because of the low overhead of computations on CSPs for attack detection; rapid, due to informing a CSP about an attack in a short course of time regarding the maximum valid response time which is defined in a service level agreement (SLA); and precision, through no false positive detection as well as a low rate of false negative detection which is < 2% of all scenarios of the simulation tests. Finally, we present a table to compare characteristics of our framework with other ones in the literature.","DDoS attacks
DDoS mitigation
Anomaly detection
Third party auditor
Framework
Cloud computing",0,,,,"Today, the providers of cloud computing services are among the most prominent service suppliers worldwide. Availability of cloud services is one of the most important concerns of cloud service providers (CSPs) and cloud users (CUs). Distributed Denial of Service (DDoS) attacks are common types of security issues which affect cloud services and consequently, can lead to unavailability of the services. Therefore, reducing the effects of DDoS attacks helps CSPs to provide high quality services to CUs. In this paper, first, we propose an anomaly-based DDoS attack detection framework in cloud environment using a third party auditor (TPA). Second, we provide multiple basic assumptions and configurations of cloud environments for establishing simulation tests to evaluate our proposed framework. Then, we provide results of simulation tests to analyze the feasibility of our approach. Simulation results demonstrate that our method for detecting DDoS attacks in CSPs has following advantages: efficiency, because of the low overhead of computations on CSPs for attack detection; rapid, due to informing a CSP about an attack in a short course of time regarding the maximum valid response time which is defined in a service level agreement (SLA); and precision, through no false positive detection as well as a low rate of false negative detection which is < 2% of all scenarios of the simulation tests. Finally, we present a table to compare characteristics of our framework with other ones in the literature.",Low
Amir Bidaki,Automatic,Springer,http://link.springer.com/article/10.1007/s11082-023-05905-3,"RETRACTED ARTICLE: A multi-step APT attack
         detection using hidden Markov models by molecular magnetic sensors",2023,Type,Optical and Quantum Electronics,Core,"Numerous attacks have targeted government agencies and industrial companies, making cybersecurity a global issue. When combined with a wide range of
attack types, advanced-persistent threats (APTs) are becoming a significant threat to
cyber security. A new and highly complicated attack is the APT Attack, which is an
Advanced Persistent Threat that focuses on specific businesses and organizations and has
taken the place of multi-stage attacks (MSAs). Digital duplicity has arisen as a guarded
way to deal with securing our digital foundation from APTs. Our recommended approach
combines reactive (graph analysis) and proactive (cyber deception technology) defence to
thwart the attackers’ lateral movement. In light of organization follows and
Interruption Recognition Framework (IDS) alerts, the principal stage figures the most
probable assault way. The assault interpreting is the second phase of the suggested
structure. This stage determines the most likely succession of Able phases for a given
grouping of related cautions using the secret Markov model (Well). The evaluation’s
findings demonstrate that our approach can recognize the most likely attack paths and
thwart APT attempts. Here suggested approach offers an estimate of the APT stage
sequence with a prediction accuracy at its betterment.","Advanced persistent threats
Attack detection
Markov model
Thwart
Cyber deception",1 Citation,,,,"Numerous attacks have targeted government agencies and industrial companies, making cybersecurity a global issue. When combined with a wide range of
attack types, advanced-persistent threats (APTs) are becoming a significant threat to
cyber security. A new and highly complicated attack is the APT Attack, which is an
Advanced Persistent Threat that focuses on specific businesses and organizations and has
taken the place of multi-stage attacks (MSAs). Digital duplicity has arisen as a guarded
way to deal with securing our digital foundation from APTs. Our recommended approach
combines reactive (graph analysis) and proactive (cyber deception technology) defence to
thwart the attackers’ lateral movement. In light of organization follows and
Interruption Recognition Framework (IDS) alerts, the principal stage figures the most
probable assault way. The assault interpreting is the second phase of the suggested
structure. This stage determines the most likely succession of Able phases for a given
grouping of related cautions using the secret Markov model (Well). The evaluation’s
findings demonstrate that our approach can recognize the most likely attack paths and
thwart APT attempts. Here suggested approach offers an estimate of the APT stage
sequence with a prediction accuracy at its betterment.",Low
Amir Bidaki,Automatic,Springer,http://link.springer.com/article/10.1007/s00521-021-05731-2,A novel energy-based online sequential extreme learning machine to detect anomalies over real-time data streams,2022,Type,Neural Computing and Applications,Core,"Data flow learning algorithms must be very efficient in learning and predicting sequences. The model that monitors a sequence of data or events can predict the sequel and can act in such a way that it optimally achieves the desired result. Security and digital risk tracking systems are receiving a constant and unlimited input of observations. These data flows are characterized by high variability, as their properties can change drastically and unpredictably over time. Each incoming example can only be processed once, or it must be summarized with a small memory imprint. This research paper proposes the development of an intelligent system, for real-time detection of data flow anomalies related to information systems’ security. Specifically, it describes the implementation of an efficient and high-precision energy-based Online Sequential Extreme Learning Machine (e-b OSELM) that is proposed for the first time in the literature. It is an intelligent model that can detect data dependencies, by applying a measure of compatibility (scalable energy) to each configuration of its variables. It assigns low energy to the correct values and higher energy to the divergent (abnormal) ones. The innovative combination of energy models and ELMs offers high learning speed, ease of execution, minimum human involvement and minimum computational power and resources for anomaly detection and identification.",Artificial Intelligence,0,,,,"Data flow learning algorithms must be very efficient in learning and predicting sequences. The model that monitors a sequence of data or events can predict the sequel and can act in such a way that it optimally achieves the desired result. Security and digital risk tracking systems are receiving a constant and unlimited input of observations. These data flows are characterized by high variability, as their properties can change drastically and unpredictably over time. Each incoming example can only be processed once, or it must be summarized with a small memory imprint. This research paper proposes the development of an intelligent system, for real-time detection of data flow anomalies related to information systems’ security. Specifically, it describes the implementation of an efficient and high-precision energy-based Online Sequential Extreme Learning Machine (e-b OSELM) that is proposed for the first time in the literature. It is an intelligent model that can detect data dependencies, by applying a measure of compatibility (scalable energy) to each configuration of its variables. It assigns low energy to the correct values and higher energy to the divergent (abnormal) ones. The innovative combination of energy models and ELMs offers high learning speed, ease of execution, minimum human involvement and minimum computational power and resources for anomaly detection and identification.",High
Amir Bidaki,Automatic,Springer,http://link.springer.com/article/10.1007/s11036-023-02143-5,"Security and Privacy in 5G-IIoT Smart Factories: Novel Approaches, Trends, and Challenges",2023,Type,Mobile Networks and Applications,Core,"To implement various artificial intelligence and automation applications in smart factories, edge computing and industrial Internet of Things (IIoT) devices must be widely deployed, so as to increase the demand of coping with huge-scale and high-diversity data. Through deployment of fifth-generation (5G) networks (providing wide broadband, low latency, and massive machine type communications), industrial wireless networks, cloud, and fixed/mobile end devices in smart factories are interoperated in a harmony. However, with the huge-scale deployment of 5G networks and the IIoT in smart factories, threats and attacks against various vulnerabilities increase enormously, and cause considerable security and privacy challenges. Consequently, this article investigates crucial security and privacy issues for 5G-IIoT smart factories in three entities (i.e., physical layer, data layer and application layer), and further surveys recent approaches based on deep learning, reinforcement learning, and blockchain. In addition, this article provides future perspectives and challenges along this line of research.","5G
Security
Privacy
Smart factory
Deep learning
Smart manufacturing
Industrial Internet of things
Blockchain
Edge computing
Cloud computing",0,,,,"To implement various artificial intelligence and automation applications in smart factories, edge computing and industrial Internet of Things (IIoT) devices must be widely deployed, so as to increase the demand of coping with huge-scale and high-diversity data. Through deployment of fifth-generation (5G) networks (providing wide broadband, low latency, and massive machine type communications), industrial wireless networks, cloud, and fixed/mobile end devices in smart factories are interoperated in a harmony. However, with the huge-scale deployment of 5G networks and the IIoT in smart factories, threats and attacks against various vulnerabilities increase enormously, and cause considerable security and privacy challenges. Consequently, this article investigates crucial security and privacy issues for 5G-IIoT smart factories in three entities (i.e., physical layer, data layer and application layer), and further surveys recent approaches based on deep learning, reinforcement learning, and blockchain. In addition, this article provides future perspectives and challenges along this line of research.",Low
Amir Bidaki,Automatic,Springer,http://link.springer.com/article/10.1007/s11301-024-00441-0,Out of the fog: fog computing-enabled AI to support smart marketing management,2024,Type,Management Review Quarterly,Core,"Marketing and consumer research use a variety of data and electronic measurement devices for research, theory-building, and applied decision-making. Managing data deluge produced by ‘smart devices’ and internet of things (IoT) actuators and sensors is one of the challenges faced by managers when using IoT systems. With the advent of the cloud-based IoT and artificial intelligence, which are advancing a ‘smart world’ and introducing automation in many application areas, such as ‘smart marketing,’ a need has arisen for various modifications to support the IoT devices that are at the center of the automation world, including recent language models like, ChatGPT and Bart, and technologies like nanotechnology. The article introduces the marketing community to a recent computing development: IoT-driven fog computing (FC)—an emerging concept that decentralizes operations, management, and data into the network utilizing a distributed and federated computing paradigm. Although numerous research studies have been published on ‘smart’ applications, none hitherto have been conducted on fog-based smart marketing. FC is considered a novel computational system, which can mitigate latency and improve bandwidth utilization for autonomous marketing applications requiring real-time processing of ‘big data’ typical of smart marketing ecosystems.","Smart marketing
Fog computing
Marketing internet of things (MIoT)
Digital marketing
Edge computing
Artificial intelligence (AI)
Software defined networks (SDN)",0,,,,"Marketing and consumer research use a variety of data and electronic measurement devices for research, theory-building, and applied decision-making. Managing data deluge produced by ‘smart devices’ and internet of things (IoT) actuators and sensors is one of the challenges faced by managers when using IoT systems. With the advent of the cloud-based IoT and artificial intelligence, which are advancing a ‘smart world’ and introducing automation in many application areas, such as ‘smart marketing,’ a need has arisen for various modifications to support the IoT devices that are at the center of the automation world, including recent language models like, ChatGPT and Bart, and technologies like nanotechnology. The article introduces the marketing community to a recent computing development: IoT-driven fog computing (FC)—an emerging concept that decentralizes operations, management, and data into the network utilizing a distributed and federated computing paradigm. Although numerous research studies have been published on ‘smart’ applications, none hitherto have been conducted on fog-based smart marketing. FC is considered a novel computational system, which can mitigate latency and improve bandwidth utilization for autonomous marketing applications requiring real-time processing of ‘big data’ typical of smart marketing ecosystems.",Low
Amir Bidaki,Automatic,Springer,http://link.springer.com/article/10.1038/s41598-024-51941-8,Stabilized quantum-enhanced SIEM architecture and speed-up through Hoeffding tree algorithms enable quantum cybersecurity analytics in botnet detection,2024,Type,Scientific Reports,Core,"For the first time, we enable the execution of hybrid quantum machine learning (HQML) methods on real quantum computers with 100 data samples and real-device-based simulations with 5000 data samples, thereby outperforming the current state of research of Suryotrisongko and Musashi from 2022 who were dealing with 1000 data samples and quantum simulators (pure software-based emulators) only. Additionally, we beat their reported accuracy of 76.8% by an average accuracy of 91.2%, all within a total execution time of 1687 s. We achieve this significant progress through two-step strategy: Firstly, we establish a stable quantum architecture that enables us to execute HQML algorithms on real quantum devices. Secondly, we introduce new hybrid quantum binary classifiers (HQBCs) based on Hoeffding decision tree algorithms. These algorithms speed up the process via batch-wise execution, reducing the number of shots required on real quantum devices compared to conventional loop-based optimizers. Their incremental nature serves the purpose of online large-scale data streaming for domain generation algorithm (DGA) botnet detection, and allows us to apply HQML to the field of cybersecurity analytics. We conduct our experiments using the Qiskit library with the Aer quantum simulator, and on three different real quantum devices from Azure Quantum: IonQ, Rigetti, and Quantinuum. This is the first time these tools are combined in this manner.",Quantum Computing,1 Citation,,,,"For the first time, we enable the execution of hybrid quantum machine learning (HQML) methods on real quantum computers with 100 data samples and real-device-based simulations with 5000 data samples, thereby outperforming the current state of research of Suryotrisongko and Musashi from 2022 who were dealing with 1000 data samples and quantum simulators (pure software-based emulators) only. Additionally, we beat their reported accuracy of 76.8% by an average accuracy of 91.2%, all within a total execution time of 1687 s. We achieve this significant progress through two-step strategy: Firstly, we establish a stable quantum architecture that enables us to execute HQML algorithms on real quantum devices. Secondly, we introduce new hybrid quantum binary classifiers (HQBCs) based on Hoeffding decision tree algorithms. These algorithms speed up the process via batch-wise execution, reducing the number of shots required on real quantum devices compared to conventional loop-based optimizers. Their incremental nature serves the purpose of online large-scale data streaming for domain generation algorithm (DGA) botnet detection, and allows us to apply HQML to the field of cybersecurity analytics. We conduct our experiments using the Qiskit library with the Aer quantum simulator, and on three different real quantum devices from Azure Quantum: IonQ, Rigetti, and Quantinuum. This is the first time these tools are combined in this manner.",Medium
Amir Bidaki,Automatic,Springer,http://link.springer.com/article/10.1007/s10586-022-03657-5,On detecting distributed denial of service attacks using fuzzy inference system,2023,Type,Cluster Computing,Core,"Nowadays, attackers are constantly targeting the modern aspects of technology and attempting to abuse these technologies using different attacks types such as the distributed denial of service attack (DDoS). Therefore, protecting web services is not an easy task. There is a critical demand to detect and prevent DDoS attacks. This paper introduces a fuzzy inference-based anomaly-based intrusion detection (IDS) system to detect DDoS attacks. The aim of using the fuzzy inference system is to avoid binary decisions and, meanwhile, to avoid the issues associated with the deficiencies of IDS alert system awareness. This benefit could improve the IDS alert system’s robustness and effectively produce more readable and understandable IDS alerts. The proposed detection model was applied to a recent open-source DDoS dataset. At the early stage of designing the proposed detection model, the DDoS dataset was preprocessed using the Info-gain features selection algorithm to deal with the relevant features only and reduce the complexity of the fuzzy inference system. The proposed detection model was tested, evaluated, and obtained a 96.25% accuracy rate and a false-positive rate of 0.006%. Moreover, it effectively smoothes the boundaries between normal and DDoS traffic. In addition, the results obtained from the proposed detection model were compared with other literature results. The results indicated that the detection accuracy of this work is competitive with other methods. In addition to this, this work offers more elements of trust in DDoS attack detection by following the strategy to avoid the binary decision and offering the required extension of the binary decision to the continuous space; hence, the attack level could be easily measured.","Intrusion detection system (IDS)
Fuzzy inference system
Distributed denial of service attack (DDoS)
Machine learning
Intrusion datasets",0,,,,"Nowadays, attackers are constantly targeting the modern aspects of technology and attempting to abuse these technologies using different attacks types such as the distributed denial of service attack (DDoS). Therefore, protecting web services is not an easy task. There is a critical demand to detect and prevent DDoS attacks. This paper introduces a fuzzy inference-based anomaly-based intrusion detection (IDS) system to detect DDoS attacks. The aim of using the fuzzy inference system is to avoid binary decisions and, meanwhile, to avoid the issues associated with the deficiencies of IDS alert system awareness. This benefit could improve the IDS alert system’s robustness and effectively produce more readable and understandable IDS alerts. The proposed detection model was applied to a recent open-source DDoS dataset. At the early stage of designing the proposed detection model, the DDoS dataset was preprocessed using the Info-gain features selection algorithm to deal with the relevant features only and reduce the complexity of the fuzzy inference system. The proposed detection model was tested, evaluated, and obtained a 96.25% accuracy rate and a false-positive rate of 0.006%. Moreover, it effectively smoothes the boundaries between normal and DDoS traffic. In addition, the results obtained from the proposed detection model were compared with other literature results. The results indicated that the detection accuracy of this work is competitive with other methods. In addition to this, this work offers more elements of trust in DDoS attack detection by following the strategy to avoid the binary decision and offering the required extension of the binary decision to the continuous space; hence, the attack level could be easily measured.",Low
Amir Bidaki,Automatic,Springer,http://link.springer.com/article/10.1007/s00330-023-09860-1,Cybersecurity considerations for radiology departments involved with artificial intelligence,2023,Type,European Radiology,Core,"Radiology artificial intelligence (AI) projects involve the integration of integrating numerous medical devices, wireless technologies, data warehouses, and social networks. While cybersecurity threats are not new to healthcare, their prevalence has increased with the rise of AI research for applications in radiology, making them one of the major healthcare risks of 2021. Radiologists have extensive experience with the interpretation of medical imaging data but radiologists may not have the required level of awareness or training related to AI-specific cybersecurity concerns. Healthcare providers and device manufacturers can learn from other industry sector industries that have already taken steps to improve their cybersecurity systems. This review aims to introduce cybersecurity concepts as it relates to medical imaging and to provide background information on general and healthcare-specific cybersecurity challenges. We discuss approaches to enhancing the level and effectiveness of security through detection and prevention techniques, as well as ways that technology can improve security while mitigating risks. We first review general cybersecurity concepts and regulatory issues before examining these topics in the context of radiology AI, with a specific focus on data, training, data, training, implementation, and auditability. Finally, we suggest potential risk mitigation strategies. By reading this review, healthcare providers, researchers, and device developers can gain a better understanding of the potential risks associated with radiology AI projects, as well as strategies to improve cybersecurity and reduce potential associated risks.",Medical Imaging,6 Citations,,,,"Radiology artificial intelligence (AI) projects involve the integration of integrating numerous medical devices, wireless technologies, data warehouses, and social networks. While cybersecurity threats are not new to healthcare, their prevalence has increased with the rise of AI research for applications in radiology, making them one of the major healthcare risks of 2021. Radiologists have extensive experience with the interpretation of medical imaging data but radiologists may not have the required level of awareness or training related to AI-specific cybersecurity concerns. Healthcare providers and device manufacturers can learn from other industry sector industries that have already taken steps to improve their cybersecurity systems. This review aims to introduce cybersecurity concepts as it relates to medical imaging and to provide background information on general and healthcare-specific cybersecurity challenges. We discuss approaches to enhancing the level and effectiveness of security through detection and prevention techniques, as well as ways that technology can improve security while mitigating risks. We first review general cybersecurity concepts and regulatory issues before examining these topics in the context of radiology AI, with a specific focus on data, training, data, training, implementation, and auditability. Finally, we suggest potential risk mitigation strategies. By reading this review, healthcare providers, researchers, and device developers can gain a better understanding of the potential risks associated with radiology AI projects, as well as strategies to improve cybersecurity and reduce potential associated risks.",Low
Amir Bidaki,Automatic,Springer,http://link.springer.com/article/10.1007/s00521-021-05798-x,Multi-disease big data analysis using beetle swarm optimization and an adaptive neuro-fuzzy inference system,2021,Type,Neural Computing and Applications,Core,"Healthcare organizations and Health Monitoring Systems generate large volumes of complex data, which offer the opportunity for innovative investigations in medical decision making. In this paper, we propose a beetle swarm optimization and adaptive neuro-fuzzy inference system (BSO-ANFIS) model for heart disease and multi-disease diagnosis. The main components of our analytics pipeline are the modified crow search algorithm, used for feature extraction, and an ANFIS classification model whose parameters are optimized by means of a BSO algorithm. The accuracy achieved in heart disease detection is \(99.1\%\) with \(99.37\%\) precision. In multi-disease classification, the accuracy achieved is \(96.08\%\) with \(98.63\%\) precision.
The results from both tasks prove the comparative advantage of the proposed BSO-ANFIS algorithm over the competitor models.",Artificial Intelligence,0,,,,"Healthcare organizations and Health Monitoring Systems generate large volumes of complex data, which offer the opportunity for innovative investigations in medical decision making. In this paper, we propose a beetle swarm optimization and adaptive neuro-fuzzy inference system (BSO-ANFIS) model for heart disease and multi-disease diagnosis. The main components of our analytics pipeline are the modified crow search algorithm, used for feature extraction, and an ANFIS classification model whose parameters are optimized by means of a BSO algorithm. The accuracy achieved in heart disease detection is \(99.1\%\) with \(99.37\%\) precision. In multi-disease classification, the accuracy achieved is \(96.08\%\) with \(98.63\%\) precision.
The results from both tasks prove the comparative advantage of the proposed BSO-ANFIS algorithm over the competitor models.",Low
Amir Bidaki,Automatic,Springer,http://link.springer.com/article/10.1007/s41060-018-0117-y,Elliptical modeling and pattern analysis for perturbation models and classification,2019,Type,International Journal of Data Science and Analytics,Core,"The characteristics of a feature vector in the transform domain of a perturbation model differ significantly from those of its corresponding feature vector in the input domain. These differences—caused by the perturbation techniques used for the transformation of feature patterns—degrade the performance of machine learning techniques in the transform domain. In this paper, we proposed a semi-parametric perturbation model that transforms the input feature patterns to a set of elliptical patterns and studied the performance degradation issues associated with random forest classification technique using both the input and transform domain features. Compared with the linear transformation such as principal component analysis (PCA), the proposed method requires less statistical assumptions and is highly suitable for the applications such as data privacy and security due to the difficulty of inverting the elliptical patterns from the transform domain to the input domain. In addition, we adopted a flexible block-wise dimensionality reduction step in the proposed method to accommodate the possible high-dimensional data in modern applications. We evaluated the empirical performance of the proposed method on a network intrusion data set and a biological data set, and compared the results with PCA in terms of classification performance and data privacy protection (measured by the blind source separation attack and signal interference ratio). Both results confirmed the superior performance of the proposed elliptical transformation.",Artificial Intelligence,0,,,,"The characteristics of a feature vector in the transform domain of a perturbation model differ significantly from those of its corresponding feature vector in the input domain. These differences—caused by the perturbation techniques used for the transformation of feature patterns—degrade the performance of machine learning techniques in the transform domain. In this paper, we proposed a semi-parametric perturbation model that transforms the input feature patterns to a set of elliptical patterns and studied the performance degradation issues associated with random forest classification technique using both the input and transform domain features. Compared with the linear transformation such as principal component analysis (PCA), the proposed method requires less statistical assumptions and is highly suitable for the applications such as data privacy and security due to the difficulty of inverting the elliptical patterns from the transform domain to the input domain. In addition, we adopted a flexible block-wise dimensionality reduction step in the proposed method to accommodate the possible high-dimensional data in modern applications. We evaluated the empirical performance of the proposed method on a network intrusion data set and a biological data set, and compared the results with PCA in terms of classification performance and data privacy protection (measured by the blind source separation attack and signal interference ratio). Both results confirmed the superior performance of the proposed elliptical transformation.",Low
Amir Bidaki,Automatic,Springer,http://link.springer.com/article/10.1186/s40537-016-0040-9,Role of big-data in classification and novel class detection in data streams,2016,Type,Journal of Big Data,Core,"“Data streams” is defined as class of data generated over “text, audio and video” channel in continuous form. The streams are of infinite length and may comprise of structured or unstructured data. With these features, it is difficult to store and process data streams with simple and static strategies. The processing of data stream poses four main challenges to researchers. These are infinite length, concept-evolution, concept-drift and feature evolution. Infinite-length is because the amount of data has no bounds. Concept-drift is due to slow changes in the concept of stream. Concept-evolution occurs due to presence of unknown classes in data. Feature-evolution is due to progression new features and regression of old features. To perform any analytics data streams, the conversion to knowledgable form is essential. The researcher in past have proposed various strategies, most of the research is focussed on problem of infinite-length and concept-drift. The research work presented in the paper describes a efficient string based methodology to process “data streams” and control the challenges of infinite-length, concept-evolution and concept-drift.","Data stream
Data mining
Concept-drift
Concept-evolution
Novel
Features",20 Citations,,,,"“Data streams” is defined as class of data generated over “text, audio and video” channel in continuous form. The streams are of infinite length and may comprise of structured or unstructured data. With these features, it is difficult to store and process data streams with simple and static strategies. The processing of data stream poses four main challenges to researchers. These are infinite length, concept-evolution, concept-drift and feature evolution. Infinite-length is because the amount of data has no bounds. Concept-drift is due to slow changes in the concept of stream. Concept-evolution occurs due to presence of unknown classes in data. Feature-evolution is due to progression new features and regression of old features. To perform any analytics data streams, the conversion to knowledgable form is essential. The researcher in past have proposed various strategies, most of the research is focussed on problem of infinite-length and concept-drift. The research work presented in the paper describes a efficient string based methodology to process “data streams” and control the challenges of infinite-length, concept-evolution and concept-drift.",Medium
Amir Bidaki,Automatic,Springer,http://link.springer.com/article/10.1007/s10207-024-00977-y,Enhancing time-series access control using deep recurrent neural networks and generative adversarial networks,2025,Type,International Journal of Information Security,Core,"This research presents a novel Attribute-Based Access Control (ABAC) system that integrates ""access history"" as a key attribute, along with advanced deep learning techniques to improve decision-making in access control. The proposed system utilizes Long Short-Term Memory (LSTM) and Gated Recurrent Unit (GRU) networks to model time-series data, while Generative Adversarial Networks (GANs) generate synthetic data to address class imbalance issues. The architecture includes Policy Decision and Enforcement Points, real-time validation using an offline-trained model, and continuous learning from access logs. Extensive experiments on both real-world and synthetic datasets demonstrate that the proposed system achieves an accuracy of 98.5%, outperforming baseline models in handling complex access scenarios. The use of GANs enhances the model's robustness and adaptability, making it suitable for dynamic and evolving environments.",Artificial Intelligence,0,,,,"This research presents a novel Attribute-Based Access Control (ABAC) system that integrates ""access history"" as a key attribute, along with advanced deep learning techniques to improve decision-making in access control. The proposed system utilizes Long Short-Term Memory (LSTM) and Gated Recurrent Unit (GRU) networks to model time-series data, while Generative Adversarial Networks (GANs) generate synthetic data to address class imbalance issues. The architecture includes Policy Decision and Enforcement Points, real-time validation using an offline-trained model, and continuous learning from access logs. Extensive experiments on both real-world and synthetic datasets demonstrate that the proposed system achieves an accuracy of 98.5%, outperforming baseline models in handling complex access scenarios. The use of GANs enhances the model's robustness and adaptability, making it suitable for dynamic and evolving environments.",Medium
Amir Bidaki,Automatic,Springer,http://link.springer.com/article/10.1057/s41599-024-03920-7,Detecting terrorist influencers using reciprocal human-machine learning: The case of militant Jihadist Da’wa on the Darknet,2024,Type,Humanities and Social Sciences Communications,Core,"Over the past decade, social media has significantly impacted terrorism and counterterrorism, serving as a platform for incitement to violence under the guise of religious preaching. This study explores the critical role of preachers preaching righteous behavior, a process known as Da’wa in Islam. Focusing on Militant Jihadist Da’wa calling to violence, the research analyzes 6000 posts from Darknet forums associated with Jihadist groups from 2017 to 2018. The study improves the detection and understanding of militant Jihadist preaching by using an advanced method called Reciprocal Human-Machine Learning. The study demonstrates the feasibility of better detection and a deeper understanding of influencing terrorists.",,0,,,,"Over the past decade, social media has significantly impacted terrorism and counterterrorism, serving as a platform for incitement to violence under the guise of religious preaching. This study explores the critical role of preachers preaching righteous behavior, a process known as Da’wa in Islam. Focusing on Militant Jihadist Da’wa calling to violence, the research analyzes 6000 posts from Darknet forums associated with Jihadist groups from 2017 to 2018. The study improves the detection and understanding of militant Jihadist preaching by using an advanced method called Reciprocal Human-Machine Learning. The study demonstrates the feasibility of better detection and a deeper understanding of influencing terrorists.",Low
Amir Bidaki,Automatic,Springer,http://link.springer.com/article/10.2991/ijndc.k.201231.001,An Empirical Study on Darknet Visualization Based on Topological Data Analysis,2021,Type,International Journal of Networked and Distributed Computing,Core,"We are experiencing the true dawn of an Internet of Things society, in which all things are connected to the Internet. While this enables us to receive a wide variety of useful services via the Internet, we cannot ignore the fact that this means the number of devices targeted for Internet attacks has also increased. One known method for handling such issues is the utilization of a darknet monitoring system, which urgently provides information on attack trends occurring on the Internet. This system monitors and analyzes malicious packets in the unused IP address space and provides security related information to both network administrators and ordinary users. In this paper, Topological Data Analysis (TDA) Mapper is utilized to analyze malicious packets on the darknet, which grow increasingly complexity every day from a new perspective. TDA Mapper is a method of TDA that has continued to attract attention in recent years. In an evaluation experiment, by applying TDA to malicious packets monitored using the actual darknet, the malicious packets were able to be visualized. In this study, the author considers the overall image of the visualized malicious packets and examples extracted from the relationships among packets and reports on the effectiveness of the proposed method.","Darknet monitoring
topological data analysis
clustering
visualization",0,,,,"We are experiencing the true dawn of an Internet of Things society, in which all things are connected to the Internet. While this enables us to receive a wide variety of useful services via the Internet, we cannot ignore the fact that this means the number of devices targeted for Internet attacks has also increased. One known method for handling such issues is the utilization of a darknet monitoring system, which urgently provides information on attack trends occurring on the Internet. This system monitors and analyzes malicious packets in the unused IP address space and provides security related information to both network administrators and ordinary users. In this paper, Topological Data Analysis (TDA) Mapper is utilized to analyze malicious packets on the darknet, which grow increasingly complexity every day from a new perspective. TDA Mapper is a method of TDA that has continued to attract attention in recent years. In an evaluation experiment, by applying TDA to malicious packets monitored using the actual darknet, the malicious packets were able to be visualized. In this study, the author considers the overall image of the visualized malicious packets and examples extracted from the relationships among packets and reports on the effectiveness of the proposed method.",Low
Amir Bidaki,Automatic,Springer,http://link.springer.com/article/10.1007/s44163-024-00154-z,Transfer learning with ResNet50 for malicious domains classification using image visualization,2024,Type,Discover Artificial Intelligence,Core,"The Internet has become a vital part of our daily lives, serving as a hub for global connectivity and a facilitator for seamless communication and information exchange. However, the rise of malicious domains presents a serious challenge, undermining the reliability of the Internet and posing risks to user safety. These malicious activities exploit the Domain Name System (DNS) to deceive users, leading to harmful activities such as spreading drive-by-download malware, operating botnets, creating phishing sites, and sending spam. In response to this growing threat, the application of Machine Learning (ML) techniques has proven to be highly effective. These methods excel in quickly and accurately detecting, classifying, and analyzing such threats. This paper explores the latest developments in using transfer learning for the classification of malicious domains, with a focus on image visualization as a key methodological approach. Our proposed solution has achieved a remarkable testing accuracy rate of 98.67%, demonstrating its effectiveness in detecting and classifying malicious domains.",Artificial Intelligence,0,,,,"The Internet has become a vital part of our daily lives, serving as a hub for global connectivity and a facilitator for seamless communication and information exchange. However, the rise of malicious domains presents a serious challenge, undermining the reliability of the Internet and posing risks to user safety. These malicious activities exploit the Domain Name System (DNS) to deceive users, leading to harmful activities such as spreading drive-by-download malware, operating botnets, creating phishing sites, and sending spam. In response to this growing threat, the application of Machine Learning (ML) techniques has proven to be highly effective. These methods excel in quickly and accurately detecting, classifying, and analyzing such threats. This paper explores the latest developments in using transfer learning for the classification of malicious domains, with a focus on image visualization as a key methodological approach. Our proposed solution has achieved a remarkable testing accuracy rate of 98.67%, demonstrating its effectiveness in detecting and classifying malicious domains.",Low
Amir Bidaki,Automatic,Springer,http://link.springer.com/article/10.1007/s11277-022-10079-4,A Study on Different Deep Learning Algorithms Used in Deep Neural Nets: MLP SOM and DBN,2023,Type,Wireless Personal Communications,Core,"Deep learning is a wildly popular topic in machine learning and is structured as a series of nonlinear layers that learns various levels of data representations. Deep learning employs numerous layers to represent data abstractions to implement various computer models. Deep learning approaches like generative, discriminative models and model transfer have transformed information processing. This article proposes a comprehensive review of various deep learning algorithms Multi layer perception, Self-organizing map and deep belief networks algorithms. It first briefly introduces historical and recent state-of-the-art reviews with suitable architectures and implementation steps. Moreover, the various applications of those algorithms in various fields such as wireless networks, Adhoc networks, Mobile ad-hoc and vehicular ad-hoc networks, speech recognition engineering, medical applications, natural language processing, material science and remote sensing applications, etc. are classified.",Artificial Intelligence,0,,,,"Deep learning is a wildly popular topic in machine learning and is structured as a series of nonlinear layers that learns various levels of data representations. Deep learning employs numerous layers to represent data abstractions to implement various computer models. Deep learning approaches like generative, discriminative models and model transfer have transformed information processing. This article proposes a comprehensive review of various deep learning algorithms Multi layer perception, Self-organizing map and deep belief networks algorithms. It first briefly introduces historical and recent state-of-the-art reviews with suitable architectures and implementation steps. Moreover, the various applications of those algorithms in various fields such as wireless networks, Adhoc networks, Mobile ad-hoc and vehicular ad-hoc networks, speech recognition engineering, medical applications, natural language processing, material science and remote sensing applications, etc. are classified.",Low
Amir Bidaki,Automatic,Springer,http://link.springer.com/article/10.1007/s11721-007-0008-7,Ant-based and swarm-based clustering,2007,Type,Swarm Intelligence,Core,"Clustering with swarm-based algorithms is emerging as an alternative to more conventional clustering methods, such as hierarchical clustering and k-means. Ant-based clustering stands out as the most widely used group of swarm-based clustering algorithms. Broadly speaking, there are two main types of ant-based clustering: the first group of methods directly mimics the clustering behavior observed in real ant colonies. The second group is less directly inspired by nature: the clustering task is reformulated as an optimization task and general purpose ant-based optimization heuristics are utilized to find good or near-optimal clusterings. This papers reviews both approaches and places these methods in the wider context of general swarm-based clustering approaches.",Artificial Intelligence,0,,,,"Clustering with swarm-based algorithms is emerging as an alternative to more conventional clustering methods, such as hierarchical clustering and k-means. Ant-based clustering stands out as the most widely used group of swarm-based clustering algorithms. Broadly speaking, there are two main types of ant-based clustering: the first group of methods directly mimics the clustering behavior observed in real ant colonies. The second group is less directly inspired by nature: the clustering task is reformulated as an optimization task and general purpose ant-based optimization heuristics are utilized to find good or near-optimal clusterings. This papers reviews both approaches and places these methods in the wider context of general swarm-based clustering approaches.",Low
Amir Bidaki,Automatic,Springer,http://link.springer.com/article/10.1007/s11277-024-11372-0,Research Progress on Security and Privacy of Federated Learning: A Survey,2024,Type,Wireless Personal Communications,Core,"Federated Learning (FL) is an emerging distributed machine learning paradigm designed to resolve the conflict between data sharing and privacy. It allows each client device to train shared models locally and perform global model aggregation on cloud servers without users having to share their data. However, there are still many security risks and malicious attacks that could breach the data privacy and confidentiality in the process of local training and information interaction. This paper investigates the security and the privacy challenges faced by FL and the corresponding defense methods. First, existing works about the FL-related surveys are studied; second, the basic concepts, the algorithm principle and the scenario classification of FL are introduced; next, examples are provided to illustrate the relevant attacks and defense knowledge of FL; then, the aggressive behaviors in FL are classified from four perspectives: the poisoning attack, the inference attack, the model attack and the adversarial attack, and the sub-aggressive behaviors are also com bed out; subsequently, the defense methods are divided according to the two directions of attack behaviors and privacy-protection technologies, and the application of different defense methods is investigated. Eventually, the future research directions on both attack problems and defense strategies in FL systems are discussed.","Federated learning
Privacy preservation
Security
Attack-and-defense strategies
Survey",0,,,,"Federated Learning (FL) is an emerging distributed machine learning paradigm designed to resolve the conflict between data sharing and privacy. It allows each client device to train shared models locally and perform global model aggregation on cloud servers without users having to share their data. However, there are still many security risks and malicious attacks that could breach the data privacy and confidentiality in the process of local training and information interaction. This paper investigates the security and the privacy challenges faced by FL and the corresponding defense methods. First, existing works about the FL-related surveys are studied; second, the basic concepts, the algorithm principle and the scenario classification of FL are introduced; next, examples are provided to illustrate the relevant attacks and defense knowledge of FL; then, the aggressive behaviors in FL are classified from four perspectives: the poisoning attack, the inference attack, the model attack and the adversarial attack, and the sub-aggressive behaviors are also com bed out; subsequently, the defense methods are divided according to the two directions of attack behaviors and privacy-protection technologies, and the application of different defense methods is investigated. Eventually, the future research directions on both attack problems and defense strategies in FL systems are discussed.",Low
Amir Bidaki,Automatic,Springer,http://link.springer.com/article/10.1007/s00521-020-05457-7,Dynamic traffic classification algorithm and simulation of energy Internet of things based on machine learning,2021,Type,Neural Computing and Applications,Core,"With the rapid development of information technology, a large amount of traffic generated by various Internet applications occupies a large amount of network resources. It poses a huge challenge to service quality and has a negative impact on Internet security. In order to utilize network resources effectively and provide effective management and control measures for network administrators, network traffic classification technologies is a hot topic for scientists to identify application layer protocols. Today, there are more and more applications based on TCP/IP. With the emergence of various anti-surveillance applications, traditional port and application-based identification methods are difficult to meet current or future traffic identification requirements. It has become a very challenging problem to require more efficient, accurate, intelligent and real-time Internet traffic identification. The Internet of Things is a new network concept proposed by people who based on Internet prototypes. It enables the end user of the system can carry out communication and exchange of information and data between any project. In recent years, with the continuous advancement of Internet of Things technology, the coverage of the Internet of Things has become very wide, and the number of different types of networks that make up the Internet of Things is also increasing. This paper aims to find the dynamic network traffic classification problem of hybrid fixed in dynamic network and dynamic network in mobile network, and gives a reasonable mapping scheme. The dynamics of network traffic for Internet of Things are reflected fully and will not cause route flapping. The simulation results show that the decision tree classification algorithm in machine learning has higher efficiency, and improves the utilization of network resources.",Artificial Intelligence,10 Citations,,,,"With the rapid development of information technology, a large amount of traffic generated by various Internet applications occupies a large amount of network resources. It poses a huge challenge to service quality and has a negative impact on Internet security. In order to utilize network resources effectively and provide effective management and control measures for network administrators, network traffic classification technologies is a hot topic for scientists to identify application layer protocols. Today, there are more and more applications based on TCP/IP. With the emergence of various anti-surveillance applications, traditional port and application-based identification methods are difficult to meet current or future traffic identification requirements. It has become a very challenging problem to require more efficient, accurate, intelligent and real-time Internet traffic identification. The Internet of Things is a new network concept proposed by people who based on Internet prototypes. It enables the end user of the system can carry out communication and exchange of information and data between any project. In recent years, with the continuous advancement of Internet of Things technology, the coverage of the Internet of Things has become very wide, and the number of different types of networks that make up the Internet of Things is also increasing. This paper aims to find the dynamic network traffic classification problem of hybrid fixed in dynamic network and dynamic network in mobile network, and gives a reasonable mapping scheme. The dynamics of network traffic for Internet of Things are reflected fully and will not cause route flapping. The simulation results show that the decision tree classification algorithm in machine learning has higher efficiency, and improves the utilization of network resources.",Low
Amir Bidaki,Automatic,Springer,http://link.springer.com/article/10.1007/s00521-022-07189-2,Application of loan lost-linking customer path correlated index model and network sorting search algorithm based on big data environment,2023,Type,Neural Computing and Applications,Core,"In order to track the loan lost-linking customers, we analyzed their historical daily consumption transaction network records (DCTNR), which include bank card transaction records, third-party payment transaction records, and network trading system order details records. We extracted the transaction date, time and address information from their daily consumption transaction path, analyzed the key factors affecting the tracking work, and constructed loan lost-linking customer path correlated index model which is applied to quantify the correlation between the initial search address and other addresses. In addition, we also establish loan customer daily consumption transaction network based on big data environment, propose the network sorting rules and searching rules, and construct the network sorting search algorithm to track loan lost-linking customers in different address types. In the case study, we analyzed the historical DCTNR data of a Chinese bank’s loan lost-linking customer, and applied loan lost-linking customer path correlated index model and network sorting search algorithm to track him in big data environment. The results represent that the method can achieve the purpose of tracking, and the tracking time and cost can be reduced by using network sorting rules and searching rules. It is of great practical significance and scientific guiding significance for banks, financial institutions and major financial platforms to apply big data, artificial intelligence and other information technologies to track loan lost-linking customers and recover economic losses.",Artificial Intelligence,0,,,,"In order to track the loan lost-linking customers, we analyzed their historical daily consumption transaction network records (DCTNR), which include bank card transaction records, third-party payment transaction records, and network trading system order details records. We extracted the transaction date, time and address information from their daily consumption transaction path, analyzed the key factors affecting the tracking work, and constructed loan lost-linking customer path correlated index model which is applied to quantify the correlation between the initial search address and other addresses. In addition, we also establish loan customer daily consumption transaction network based on big data environment, propose the network sorting rules and searching rules, and construct the network sorting search algorithm to track loan lost-linking customers in different address types. In the case study, we analyzed the historical DCTNR data of a Chinese bank’s loan lost-linking customer, and applied loan lost-linking customer path correlated index model and network sorting search algorithm to track him in big data environment. The results represent that the method can achieve the purpose of tracking, and the tracking time and cost can be reduced by using network sorting rules and searching rules. It is of great practical significance and scientific guiding significance for banks, financial institutions and major financial platforms to apply big data, artificial intelligence and other information technologies to track loan lost-linking customers and recover economic losses.",Low
Amir Bidaki,Automatic,Springer,http://link.springer.com/article/10.1007/s12008-020-00672-x,Engineering education for smart 4.0 technology: a review,2020,Type,International Journal on Interactive Design and Manufacturing (IJIDeM),Core,"Industry 4.0 (I4), defined as the integration of information and communication technology with industrial advances to develop digital factories, will transform production processes by making them more efficient, green, and flexible. Manufacturers must overcome barriers to implement this approach; one of them is the lack of qualified talent to manage I4 systems. This paper is a review of the state-of-art of I4. The goal of this research is to describe the technologies that are enablers of I4, analyze the human talent needed and the qualifications (competencies/skills) required to manage I4 systems, describe the courses and practices that leading universities in engineering and technology are teaching for developing the I4 workforce, and provide a look to the future by presenting trends related to education in I4. Autonomous robots, simulation, horizontal and vertical integration, the Industrial Internet of Things, additive manufacturing, augmented reality, cybersecurity, the cloud, and Big Data and analytics are technologies considered pillars of the I4 approach. Nowadays, there are universities already offering programs related to I4. These range from those aimed at presenting the concepts of I4 to those in which students learn how to handle related technologies in real work in global enterprises. I4 is still evolving, and we present only a glimpse of what is required based on its current state. Continuous review of the I4 environment is necessary to determine opportunely the path that it will follow and, in consequence, the needs that will emerge in areas such as curricula content, competency requirements, and technologies.","Engineering education
Industry 4.0
Smart 4.0 industry
Smart manufacturing
Educational innovation
Higher education",75 Citations,,,,"Industry 4.0 (I4), defined as the integration of information and communication technology with industrial advances to develop digital factories, will transform production processes by making them more efficient, green, and flexible. Manufacturers must overcome barriers to implement this approach; one of them is the lack of qualified talent to manage I4 systems. This paper is a review of the state-of-art of I4. The goal of this research is to describe the technologies that are enablers of I4, analyze the human talent needed and the qualifications (competencies/skills) required to manage I4 systems, describe the courses and practices that leading universities in engineering and technology are teaching for developing the I4 workforce, and provide a look to the future by presenting trends related to education in I4. Autonomous robots, simulation, horizontal and vertical integration, the Industrial Internet of Things, additive manufacturing, augmented reality, cybersecurity, the cloud, and Big Data and analytics are technologies considered pillars of the I4 approach. Nowadays, there are universities already offering programs related to I4. These range from those aimed at presenting the concepts of I4 to those in which students learn how to handle related technologies in real work in global enterprises. I4 is still evolving, and we present only a glimpse of what is required based on its current state. Continuous review of the I4 environment is necessary to determine opportunely the path that it will follow and, in consequence, the needs that will emerge in areas such as curricula content, competency requirements, and technologies.",Low
Amir Bidaki,Automatic,Springer,http://link.springer.com/article/10.1631/FITEE.1601516,Situational awareness architecture for smart grids developed in accordance with dispatcher’s thought process: a review,2016,Type,Frontiers of Information Technology & Electronic Engineering,Core,"The operational environment of today’s smart grids is becoming more complicated than ever before. A number of factors, including renewable penetration, marketization, cyber security, and hazards of nature, bring challenges and even threats to control centers. New techniques are anticipated to help dispatchers become aware of the accurate situations as they manipulate and navigate the situations as quickly as possible. To address the issues, we first introduce the background for this topic as well as the emerging technical demands of situational awareness in the dispatcher’s environment. The general concepts and technical requirements of situational awareness are then summarized, aimed at offering an overview for readers to understand the state-of-the-art progress in this area. In addition, we discuss the importance of integrating the architecture of support tools in accordance with the dispatcher’s thought process, which in fact guides correct and swift reactions in real-time operations. Finally, the prospects for situational awareness architecture are investigated with the goal of presenting situational awareness modules in an advanced and visualized manner.","Smart grid
Situational awareness
Dispatcher’s thought process
Technical architecture",0,,,,"The operational environment of today’s smart grids is becoming more complicated than ever before. A number of factors, including renewable penetration, marketization, cyber security, and hazards of nature, bring challenges and even threats to control centers. New techniques are anticipated to help dispatchers become aware of the accurate situations as they manipulate and navigate the situations as quickly as possible. To address the issues, we first introduce the background for this topic as well as the emerging technical demands of situational awareness in the dispatcher’s environment. The general concepts and technical requirements of situational awareness are then summarized, aimed at offering an overview for readers to understand the state-of-the-art progress in this area. In addition, we discuss the importance of integrating the architecture of support tools in accordance with the dispatcher’s thought process, which in fact guides correct and swift reactions in real-time operations. Finally, the prospects for situational awareness architecture are investigated with the goal of presenting situational awareness modules in an advanced and visualized manner.",Low
Amir Bidaki,Automatic,Springer,http://link.springer.com/article/10.1007/s10994-023-06403-z,Cost-sensitive sparse group online learning for imbalanced data streams,2024,Type,Machine Learning,Core,"Effective streaming feature selection in dynamic online environments is essential in numerous applications. However, most existing methods evaluate high-dimensional features individually and ignore the potentially pertainable group structures of features. Moreover, the class imbalance underlying streaming data may further decrease the discriminative efficacy of the selected features, resulting in deteriorated classification performance. Motivated by this observation, we propose a cost-sensitive sparse group online learning (CSGOL) framework and its proximal version (PCSGOL) to handle imbalanced and high-dimensional streaming data. We formulate this issue as a new cost-sensitive online optimization problem by leveraging the \(\ell _2\)-norm, \(\ell _1\)-norm, and groupwise sparsity constraints in the dual averaging regularization. Inspired by the proximal optimization, we further introduce the average weighted distance in CSGOL and develop the PCSGOL method to achieve stable prediction results. We mathematically derive closed-form solutions to the optimization problems with four modified hinge loss functions, leading to four variants of CSGOL and PCSGOL. Extensive empirical studies on real-world streaming datasets and online anomaly detection tasks demonstrate the effectiveness of our proposed methods.",Artificial Intelligence,0,,,,"Effective streaming feature selection in dynamic online environments is essential in numerous applications. However, most existing methods evaluate high-dimensional features individually and ignore the potentially pertainable group structures of features. Moreover, the class imbalance underlying streaming data may further decrease the discriminative efficacy of the selected features, resulting in deteriorated classification performance. Motivated by this observation, we propose a cost-sensitive sparse group online learning (CSGOL) framework and its proximal version (PCSGOL) to handle imbalanced and high-dimensional streaming data. We formulate this issue as a new cost-sensitive online optimization problem by leveraging the \(\ell _2\)-norm, \(\ell _1\)-norm, and groupwise sparsity constraints in the dual averaging regularization. Inspired by the proximal optimization, we further introduce the average weighted distance in CSGOL and develop the PCSGOL method to achieve stable prediction results. We mathematically derive closed-form solutions to the optimization problems with four modified hinge loss functions, leading to four variants of CSGOL and PCSGOL. Extensive empirical studies on real-world streaming datasets and online anomaly detection tasks demonstrate the effectiveness of our proposed methods.",Medium
Amir Bidaki,Automatic,Springer,http://link.springer.com/article/10.1007/s10115-010-0345-5,A hybrid decision tree training method using data streams,2011,Type,Knowledge and Information Systems,Core,"Classical classification methods usually assume that pattern recognition models do not depend on the timing of the data. However, this assumption is not valid in cases where new data frequently become available. Such situations are common in practice, for example, spam filtering or fraud detection, where dependencies between feature values and class numbers are continually changing. Unfortunately, most classical machine learning methods (such as decision trees) do not take into consideration the possibility of the model changing, as a result of so-called concept drift and they cannot adapt to a new classification model. This paper focuses on the problem of concept drift, which is a very important issue, especially in data mining methods that use complex structures (such as decision trees) for making decisions. We propose an algorithm that is able to co-train decision trees using a modified NGE (Nested Generalized Exemplar) algorithm. The potential for adaptation of the proposed algorithm and the quality thereof are evaluated through computer experiments, carried out on benchmark datasets from the UCI Machine Learning Repository.","Nested generalized exemplar
Nearest hyperrectangle
Concept drift
Decision tree
Incremental learning
Pattern recognition",40 Citations,,,,"Classical classification methods usually assume that pattern recognition models do not depend on the timing of the data. However, this assumption is not valid in cases where new data frequently become available. Such situations are common in practice, for example, spam filtering or fraud detection, where dependencies between feature values and class numbers are continually changing. Unfortunately, most classical machine learning methods (such as decision trees) do not take into consideration the possibility of the model changing, as a result of so-called concept drift and they cannot adapt to a new classification model. This paper focuses on the problem of concept drift, which is a very important issue, especially in data mining methods that use complex structures (such as decision trees) for making decisions. We propose an algorithm that is able to co-train decision trees using a modified NGE (Nested Generalized Exemplar) algorithm. The potential for adaptation of the proposed algorithm and the quality thereof are evaluated through computer experiments, carried out on benchmark datasets from the UCI Machine Learning Repository.",Medium
Amir Bidaki,Automatic,Springer,http://link.springer.com/article/10.1007/s13132-024-01868-2,Enhancing Efficiency and Decision-Making in Higher Education Through Intelligent Commercial Integration: Leveraging Artificial Intelligence,2024,Type,Journal of the Knowledge Economy,Core,"The integration of artificial intelligence (AI) into financial management processes within academic institutions has ushered in a transformative era. This research paper delves into the profound implications of AI-driven financial integration, emphasizing its significance in aligning academic financial service functions with greater cohesion and efficiency. The establishment of intelligent financial systems, underpinned by meticulous data management and AI capabilities, has redefined the landscape of financial management in colleges and universities. This study explores the intricate relationship between AI-driven financial integration and its impact on job responsibilities and financial positions. It uncovers gaps in existing research and formulates pertinent questions to deepen our understanding of the integration process between industry and finance in the intelligent era. The research revolves around constructing a three-level financial management intelligent system, encompassing fine management, cost-effective operations, and enhancing links in financial processes. The findings underscore the transformative potential of AI in streamlining financial operations, emphasizing its role in liberating financial personnel from routine tasks. This paradigm shift not only streamlines financial operations but also augments financial productivity, unlocking the full potential of financial management within academic institutions. Theoretical implications highlight the need for ongoing theoretical development to accommodate the evolving role of AI in financial ecosystems. Managerial implications advocate for the strategic adoption of AI-driven financial platforms, fostering a culture of creativity and strategic contributions. Proactive managerial involvement in AI adoption can yield substantial benefits, requiring a nuanced approach to organizational change management and continuous innovation. This research paper paves the way for a more intelligent and integrated future in academic financial management, with AI driving efficiency and adaptability.","Intelligent financial sharing platform
University financial management
Artificial intelligence in education
Operational efficiency
Financial system innovation
Neural network learning",0,,,,"The integration of artificial intelligence (AI) into financial management processes within academic institutions has ushered in a transformative era. This research paper delves into the profound implications of AI-driven financial integration, emphasizing its significance in aligning academic financial service functions with greater cohesion and efficiency. The establishment of intelligent financial systems, underpinned by meticulous data management and AI capabilities, has redefined the landscape of financial management in colleges and universities. This study explores the intricate relationship between AI-driven financial integration and its impact on job responsibilities and financial positions. It uncovers gaps in existing research and formulates pertinent questions to deepen our understanding of the integration process between industry and finance in the intelligent era. The research revolves around constructing a three-level financial management intelligent system, encompassing fine management, cost-effective operations, and enhancing links in financial processes. The findings underscore the transformative potential of AI in streamlining financial operations, emphasizing its role in liberating financial personnel from routine tasks. This paradigm shift not only streamlines financial operations but also augments financial productivity, unlocking the full potential of financial management within academic institutions. Theoretical implications highlight the need for ongoing theoretical development to accommodate the evolving role of AI in financial ecosystems. Managerial implications advocate for the strategic adoption of AI-driven financial platforms, fostering a culture of creativity and strategic contributions. Proactive managerial involvement in AI adoption can yield substantial benefits, requiring a nuanced approach to organizational change management and continuous innovation. This research paper paves the way for a more intelligent and integrated future in academic financial management, with AI driving efficiency and adaptability.",Low
Amir Bidaki,Automatic,Springer,http://link.springer.com/article/10.1007/s00500-022-07020-z,Stratified hyperparameters optimization of feed-forward neural network for social network spam detection (SON2S),2022,Type,Soft Computing,Core,"Over the last decade, popularity and fascination for social networks have exponentially increased. This rapid growth has triggered cybercriminals to utilize social networks for their malicious activities like social network spam. This risk and danger in social networking sites urge the need for an accurate and efficient spam detection model. Traditionally, supervised and unsupervised classification algorithms are used to identify social network spam. But spammers often change their behavior to evade spam filtering techniques. This results in huge data volatility. Traditional techniques are, therefore, sometimes ineffective in filtering spam in the social network. Hence, this work proposes to use a feed-forward neural network that can use the hidden relationship in this complex data for spam detection model. To improve the model’s accuracy and to speed up the training process, important hyperparameters such as learning rate, momentum term, architecture of neural network, activation function, training algorithm, initial weights ranges, and initial tuning of weights are necessary. As there is no general and predefined method available for this process, a reinforcement learning and k-Norm factor-based shuffled frog leaping algorithm to find the optimum set of neural network parameters is proposed in this paper. In the first stage, learning rate and momentum parameters in the continuous variable range are tuned using reinforcement learning. In the second stage, the best possible combinations of remaining parameter values are chosen using the proposed modified shuffled frog leaping algorithm that uses k-Norm to improve the exploitation. Experiments were carried out for the Tip spam dataset and Twitter dataset on a feed-forward neural network with tuned parameters. The results prove that the proposed algorithm achieves higher accuracy and lower false positive rate when compared to other existing techniques.",Artificial Intelligence,3 Citations,,,,"Over the last decade, popularity and fascination for social networks have exponentially increased. This rapid growth has triggered cybercriminals to utilize social networks for their malicious activities like social network spam. This risk and danger in social networking sites urge the need for an accurate and efficient spam detection model. Traditionally, supervised and unsupervised classification algorithms are used to identify social network spam. But spammers often change their behavior to evade spam filtering techniques. This results in huge data volatility. Traditional techniques are, therefore, sometimes ineffective in filtering spam in the social network. Hence, this work proposes to use a feed-forward neural network that can use the hidden relationship in this complex data for spam detection model. To improve the model’s accuracy and to speed up the training process, important hyperparameters such as learning rate, momentum term, architecture of neural network, activation function, training algorithm, initial weights ranges, and initial tuning of weights are necessary. As there is no general and predefined method available for this process, a reinforcement learning and k-Norm factor-based shuffled frog leaping algorithm to find the optimum set of neural network parameters is proposed in this paper. In the first stage, learning rate and momentum parameters in the continuous variable range are tuned using reinforcement learning. In the second stage, the best possible combinations of remaining parameter values are chosen using the proposed modified shuffled frog leaping algorithm that uses k-Norm to improve the exploitation. Experiments were carried out for the Tip spam dataset and Twitter dataset on a feed-forward neural network with tuned parameters. The results prove that the proposed algorithm achieves higher accuracy and lower false positive rate when compared to other existing techniques.",Low
Amir Bidaki,Automatic,Springer,http://link.springer.com/article/10.1007/s41060-021-00278-w,Data science and AI in FinTech: an overview,2021,Type,International Journal of Data Science and Analytics,Core,"Financial technology (FinTech) has been playing an increasingly critical role in driving modern economies, society, technology, and many other areas. Smart FinTech is the new-generation FinTech, largely inspired and empowered by data science and artificial intelligence (DSAI) techniques. Smart FinTech synthesizes broad DSAI and transforms finance and economies to drive intelligent, automated, whole-of-business and personalized economic and financial businesses, services and systems. The research on data science and AI in FinTech involves many latest progress made in smart FinTech for BankingTech, TradeTech, LendTech, InsurTech, WealthTech, PayTech, RiskTech, cryptocurrencies, and blockchain, and the DSAI techniques including complex system methods, quantitative methods, intelligent interactions, recognition and responses, data analytics, deep learning, federated learning, privacy-preserving processing, augmentation, optimization, and system intelligence enhancement. Here, we present a highly dense research overview of smart financial businesses and their challenges, the smart FinTech ecosystem, the DSAI techniques to enable smart FinTech, and some research directions of smart FinTech futures to the DSAI communities.",Artificial Intelligence,0,,,,"Financial technology (FinTech) has been playing an increasingly critical role in driving modern economies, society, technology, and many other areas. Smart FinTech is the new-generation FinTech, largely inspired and empowered by data science and artificial intelligence (DSAI) techniques. Smart FinTech synthesizes broad DSAI and transforms finance and economies to drive intelligent, automated, whole-of-business and personalized economic and financial businesses, services and systems. The research on data science and AI in FinTech involves many latest progress made in smart FinTech for BankingTech, TradeTech, LendTech, InsurTech, WealthTech, PayTech, RiskTech, cryptocurrencies, and blockchain, and the DSAI techniques including complex system methods, quantitative methods, intelligent interactions, recognition and responses, data analytics, deep learning, federated learning, privacy-preserving processing, augmentation, optimization, and system intelligence enhancement. Here, we present a highly dense research overview of smart financial businesses and their challenges, the smart FinTech ecosystem, the DSAI techniques to enable smart FinTech, and some research directions of smart FinTech futures to the DSAI communities.",Low
Amir Bidaki,Automatic,Springer,http://link.springer.com/article/10.1007/s13369-024-09144-w,Distributed Ensemble Method Using Deep Learning to Detect DDoS Attacks in IoT Networks,2025,Type,Arabian Journal for Science and Engineering,Core,"The widespread adoption of Internet of Things (IoT) devices has increased exponentially in recent years. Consequently, the security risks and vulnerabilities related to these unsecured IoT devices are also continuously increasing. Among the significant challenges facing the IoT environment is the threat of Distributed Denial of Service (DDoS) attacks. Several solutions are available in the literature to detect DDoS attacks. However, these detection mechanisms can easily be evaded by attackers using advanced tools and techniques, posing difficulty in detecting such lethal attacks in real time. Therefore, this paper proposes a novel distributed ensemble method for detecting lethal IoT traffic-based DDoS attacks. This method comprises two key stages: first, developing a distributed ensemble method using the breathtaking capabilities of the H2O.ai distributed machine learning platform and the ensemble learning technique. Secondly, this method was deployed on the Apache Storm stream processing framework, to swiftly analyze incoming network streams and categorize them into eleven distinct classes, including benign traffic and ten types of attacks, in near real time. The proposed method accurately identifies specific target categories within a multi-attack classification scenario by utilizing the expertise of various models. Ultimately, the prediction for a target class is determined based on the model with the highest detection rate. The effectiveness of this method has been examined using different configured scenarios. The experimental results show that our method can identify various attack categories more accurately with 99%+ accuracy and 8.45 s quicker than non-ensemble methods.","Apache storm stream processing framework
DDoS attack detection
Ensemble learning technique
Hadoop cluster
IoT devices
H2O.ai distributed platform
Internet of Things (IoT)",0,,,,"The widespread adoption of Internet of Things (IoT) devices has increased exponentially in recent years. Consequently, the security risks and vulnerabilities related to these unsecured IoT devices are also continuously increasing. Among the significant challenges facing the IoT environment is the threat of Distributed Denial of Service (DDoS) attacks. Several solutions are available in the literature to detect DDoS attacks. However, these detection mechanisms can easily be evaded by attackers using advanced tools and techniques, posing difficulty in detecting such lethal attacks in real time. Therefore, this paper proposes a novel distributed ensemble method for detecting lethal IoT traffic-based DDoS attacks. This method comprises two key stages: first, developing a distributed ensemble method using the breathtaking capabilities of the H2O.ai distributed machine learning platform and the ensemble learning technique. Secondly, this method was deployed on the Apache Storm stream processing framework, to swiftly analyze incoming network streams and categorize them into eleven distinct classes, including benign traffic and ten types of attacks, in near real time. The proposed method accurately identifies specific target categories within a multi-attack classification scenario by utilizing the expertise of various models. Ultimately, the prediction for a target class is determined based on the model with the highest detection rate. The effectiveness of this method has been examined using different configured scenarios. The experimental results show that our method can identify various attack categories more accurately with 99%+ accuracy and 8.45 s quicker than non-ensemble methods.",Medium
Amir Bidaki,Automatic,Springer,http://link.springer.com/article/10.1057/jors.2008.171,50 years of data mining and OR: upcoming trends and challenges,2009,Type,Journal of the Operational Research Society,Core,"Data mining involves extracting interesting patterns from data and can be found at the heart of operational research (OR), as its aim is to create and enhance decision support systems. Even in the early days, some data mining approaches relied on traditional OR methods such as linear programming and forecasting, and modern data mining methods are based on a wide variety of OR methods including linear and quadratic optimization, genetic algorithms and concepts based on artificial ant colonies. The use of data mining has rapidly become widespread, with applications in domains ranging from credit risk, marketing, and fraud detection to counter-terrorism. In all of these, data mining is increasingly playing a key role in decision making. Nonetheless, many challenges still need to be tackled, ranging from data quality issues to the problem of how to include domain experts' knowledge, or how to monitor model performance. In this paper, we outline a series of upcoming trends and challenges for data mining and its role within OR.","data mining
learning algorithms
decision support systems
applications
prediction",0,,,,"Data mining involves extracting interesting patterns from data and can be found at the heart of operational research (OR), as its aim is to create and enhance decision support systems. Even in the early days, some data mining approaches relied on traditional OR methods such as linear programming and forecasting, and modern data mining methods are based on a wide variety of OR methods including linear and quadratic optimization, genetic algorithms and concepts based on artificial ant colonies. The use of data mining has rapidly become widespread, with applications in domains ranging from credit risk, marketing, and fraud detection to counter-terrorism. In all of these, data mining is increasingly playing a key role in decision making. Nonetheless, many challenges still need to be tackled, ranging from data quality issues to the problem of how to include domain experts' knowledge, or how to monitor model performance. In this paper, we outline a series of upcoming trends and challenges for data mining and its role within OR.",Low
Amir Bidaki,Automatic,Springer,http://link.springer.com/article/10.1007/s13132-024-02128-z,Comparative Analysis of Digital Economy-Driven Innovation Development in China: An International Perspective,2024,Type,Journal of the Knowledge Economy,Core,"The emergence of e-commerce, digital technology, and the internet have all contributed to a revolutionary shift in the global landscape toward the digital economy, which is driving inventive development on a global scale. Simultaneously, China’s digital economy has entered a phase of high-quality development and has become a key driver of the global economic recovery. On the other hand, maintaining sustainability presents difficulties for the traditional growth paradigm. This paper explains the complex relationship between innovation and the digital economy across industries, economies, organizations, and innovation ecosystems by methodically reviewing the body of research on the topic. This report provides important recommendations through a comparative examination of digital economy strategy, current status, and difficulties across leading industrialized nations and China. These include developing all-encompassing plans, establishing strong regulatory frameworks, streamlining management procedures, and encouraging technology advancements and applications to strengthen the groundwork for China’s innovation development powered by the digital economy. Furthermore, this research advances the knowledge economy by aligning with the changing dynamics of digital transformation and innovation-driven growth.","Digital economy
Innovation development
High-quality development
Comparative analysis
China
Global economy",0,,,,"The emergence of e-commerce, digital technology, and the internet have all contributed to a revolutionary shift in the global landscape toward the digital economy, which is driving inventive development on a global scale. Simultaneously, China’s digital economy has entered a phase of high-quality development and has become a key driver of the global economic recovery. On the other hand, maintaining sustainability presents difficulties for the traditional growth paradigm. This paper explains the complex relationship between innovation and the digital economy across industries, economies, organizations, and innovation ecosystems by methodically reviewing the body of research on the topic. This report provides important recommendations through a comparative examination of digital economy strategy, current status, and difficulties across leading industrialized nations and China. These include developing all-encompassing plans, establishing strong regulatory frameworks, streamlining management procedures, and encouraging technology advancements and applications to strengthen the groundwork for China’s innovation development powered by the digital economy. Furthermore, this research advances the knowledge economy by aligning with the changing dynamics of digital transformation and innovation-driven growth.",Low
Amir Bidaki,Automatic,Springer,http://link.springer.com/article/10.1007/s12083-014-0281-3,Active learning for P2P traffic identification,2015,Type,Peer-to-Peer Networking and Applications,Core,"P2P traffic identification methods by using machine learning have been provided in a great number of works, which suffer from a large and representative labeled sample set. To overcome the sample labeling problem, a new P2P traffic identification approach by active learning called P2PTIAL is presented. P2PTIAL is composed of two parts: support vector machine as learner and uncertainty selection based on distance. In order to improve the effectiveness of P2PTIAL, we add filtering policy and balanced policy to P2PTIAL. Firstly, we use support vector data description (SVDD) theory to filter some unlabeled samples, which have little contribution on active learning, and so it can save computation cost and storage space. Secondly, we use the unlabeled sample’s pre-labeled information to develop balanced policy, which can keep balanced learning. Lastly, we support our design with extensive simulation experiments, and our results show P2PTIAL is feasible.","Active learning
Support vector data description
Traffic identification
P2P",0,,,,"P2P traffic identification methods by using machine learning have been provided in a great number of works, which suffer from a large and representative labeled sample set. To overcome the sample labeling problem, a new P2P traffic identification approach by active learning called P2PTIAL is presented. P2PTIAL is composed of two parts: support vector machine as learner and uncertainty selection based on distance. In order to improve the effectiveness of P2PTIAL, we add filtering policy and balanced policy to P2PTIAL. Firstly, we use support vector data description (SVDD) theory to filter some unlabeled samples, which have little contribution on active learning, and so it can save computation cost and storage space. Secondly, we use the unlabeled sample’s pre-labeled information to develop balanced policy, which can keep balanced learning. Lastly, we support our design with extensive simulation experiments, and our results show P2PTIAL is feasible.",Low
Amir Bidaki,Automatic,Springer,http://link.springer.com/article/10.1007/s12065-023-00902-7,Elephant flow detection intelligence for software-defined networks: a survey on current techniques and future direction,2024,Type,Evolutionary Intelligence,Core,"Software-defined networking (SDN) is characterized by the separation of the packet forwarding plane from the network control plane. This separation offers an extensive view of the network’s state, enhancing network resilience and management. Network traffic classification can improve SDN control and resource provisioning, particularly for elephant flows (EFs) detection. Existing techniques for detecting EFs utilize preset thresholds and bandwidth that are inadequate for changing traffic concepts. Moreover, these techniques consume high data plane-controller bandwidth and have a high detection time. This research first describes the related management techniques in SDN. Then according to the detecting location, elephant flow detection approaches are classified into four kinds: host-based, switch-based, controller-based, and hybrid controller-switch-based detection. This research examined four types of detection approaches and concluded that host-based detection primarily relies on the flow statistics threshold. Such approaches frequently gather flow statistics by monitoring the socket buffer or via the hypervisor. In contrast, switch-based detection can leverage both the flow statistics threshold and flow characteristics. Controller-based detection techniques in SDN focus on extracting flow feature statistics at the controller level, aiming to reduce switch overhead while potentially increasing controller loads. Finally, hybrid controller-switch-based detection combines both routing aspects, offering fine-grained flow control. However it faces challenges in maintaining a balance in timeliness, accuracy, and cost. Furthermore, the survey incorporates recent SDN advancements such as machine learning-based methods, programmable switches, and real-world SDN applications in data centers, global content delivery networks, healthcare, and IoT. Finally, the article makes a comprehensive comparison and puts forward several points of future prediction in terms of elephant flow detection, taking into account recent advances in SDN research.",Artificial Intelligence,0,,,,"Software-defined networking (SDN) is characterized by the separation of the packet forwarding plane from the network control plane. This separation offers an extensive view of the network’s state, enhancing network resilience and management. Network traffic classification can improve SDN control and resource provisioning, particularly for elephant flows (EFs) detection. Existing techniques for detecting EFs utilize preset thresholds and bandwidth that are inadequate for changing traffic concepts. Moreover, these techniques consume high data plane-controller bandwidth and have a high detection time. This research first describes the related management techniques in SDN. Then according to the detecting location, elephant flow detection approaches are classified into four kinds: host-based, switch-based, controller-based, and hybrid controller-switch-based detection. This research examined four types of detection approaches and concluded that host-based detection primarily relies on the flow statistics threshold. Such approaches frequently gather flow statistics by monitoring the socket buffer or via the hypervisor. In contrast, switch-based detection can leverage both the flow statistics threshold and flow characteristics. Controller-based detection techniques in SDN focus on extracting flow feature statistics at the controller level, aiming to reduce switch overhead while potentially increasing controller loads. Finally, hybrid controller-switch-based detection combines both routing aspects, offering fine-grained flow control. However it faces challenges in maintaining a balance in timeliness, accuracy, and cost. Furthermore, the survey incorporates recent SDN advancements such as machine learning-based methods, programmable switches, and real-world SDN applications in data centers, global content delivery networks, healthcare, and IoT. Finally, the article makes a comprehensive comparison and puts forward several points of future prediction in terms of elephant flow detection, taking into account recent advances in SDN research.",Low
Amir Bidaki,Automatic,Springer,http://link.springer.com/article/10.1007/s10489-017-0907-2,Evaluation of random forest classifier in security domain,2017,Type,Applied Intelligence,Core,"There is an intrinsic adversarial nature in the security domain such as spam filtering and malware detection systems that attempt to mislead the detection system. This adversarial nature makes security applications different from the classical machine learning problems; for instance, an adversary (attacker) might change the distribution of test data and violate the data stationarity, a common assumption in machine learning techniques. Since machine learning methods are not inherently adversary-aware, a classifier designer should investigate the robustness of a learning system under attack. In this respect, recent studies have modeled the identified attacks against machine learning-based detection systems. Based on this, a classifier designer can evaluate the performance of a learning system leveraging the modeled attacks. Prior research explored a gradient-based approach in order to devise an attack against a classifier with differentiable discriminant function like SVM. However, there are several powerful classifiers with non-differentiable decision boundary such as Random Forest, which are commonly used in different security domain and applications. In this paper, we present a novel approach to model an attack against classifiers with non-differentiable decision boundary. In the experimentation, we first present an example that visually shows the effect of a successful attack on the MNIST handwritten digits classification task. Then we conduct experiments for two well-known applications in the security domain: spam filtering and malware detection in PDF files. The experimental results demonstrate that the proposed attack successfully evades Random Forest classifier and effectively degrades the classifier’s performance.",Artificial Intelligence,0,,,,"There is an intrinsic adversarial nature in the security domain such as spam filtering and malware detection systems that attempt to mislead the detection system. This adversarial nature makes security applications different from the classical machine learning problems; for instance, an adversary (attacker) might change the distribution of test data and violate the data stationarity, a common assumption in machine learning techniques. Since machine learning methods are not inherently adversary-aware, a classifier designer should investigate the robustness of a learning system under attack. In this respect, recent studies have modeled the identified attacks against machine learning-based detection systems. Based on this, a classifier designer can evaluate the performance of a learning system leveraging the modeled attacks. Prior research explored a gradient-based approach in order to devise an attack against a classifier with differentiable discriminant function like SVM. However, there are several powerful classifiers with non-differentiable decision boundary such as Random Forest, which are commonly used in different security domain and applications. In this paper, we present a novel approach to model an attack against classifiers with non-differentiable decision boundary. In the experimentation, we first present an example that visually shows the effect of a successful attack on the MNIST handwritten digits classification task. Then we conduct experiments for two well-known applications in the security domain: spam filtering and malware detection in PDF files. The experimental results demonstrate that the proposed attack successfully evades Random Forest classifier and effectively degrades the classifier’s performance.",Low
Amir Bidaki,Automatic,Springer,http://link.springer.com/article/10.1007/s10489-023-04662-w,Mean based relief: An improved feature selection method based on ReliefF,2023,Type,Applied Intelligence,Core,"Selection of relevant features is vitally important in machine learning tasks involving large datasets with numerous features. It helps in reducing the dimensionality of a dataset and improving model performance. This study introduces a feature selection technique named \(\mu \)-Relief, which is based on ReliefF, one of the most extensively used Relief-based algorithms. \(\mu \)-Relief effectively determines the most relevant feature subset and significantly outperforms the ReliefF algorithm. ReliefF estimates feature quality considering only the nearest neighbors, resulting in low classification accuracy on non-uniformly distributed or noisy datasets. The proposed \(\mu \)-Relief technique considers neighbors with more effective information on the basis of mean distance. It utilizes neighbors far from the mean distance to obtain feature weight estimates, which improves the algorithm’s performance. The algorithm was tested on thirteen real-world datasets and validated on three synthetic datasets. Its effectiveness in selecting relevant features was evaluated by comparing it to other well-known feature selection algorithms, namely Chi-Square, ANOVA, MI, CMIM, MRMR, SURF*, MultiSURF, MultiSURF*, and ReliefF. When evaluated using multiple classifiers trained on the features selected by different feature selection techniques, the metrics of classification accuracy, weighted F1-score, and ROC-AUC, showed that \(\mu \)-Relief effectively determined relevant features and outperformed other techniques.",Artificial Intelligence,0,,,,"Selection of relevant features is vitally important in machine learning tasks involving large datasets with numerous features. It helps in reducing the dimensionality of a dataset and improving model performance. This study introduces a feature selection technique named \(\mu \)-Relief, which is based on ReliefF, one of the most extensively used Relief-based algorithms. \(\mu \)-Relief effectively determines the most relevant feature subset and significantly outperforms the ReliefF algorithm. ReliefF estimates feature quality considering only the nearest neighbors, resulting in low classification accuracy on non-uniformly distributed or noisy datasets. The proposed \(\mu \)-Relief technique considers neighbors with more effective information on the basis of mean distance. It utilizes neighbors far from the mean distance to obtain feature weight estimates, which improves the algorithm’s performance. The algorithm was tested on thirteen real-world datasets and validated on three synthetic datasets. Its effectiveness in selecting relevant features was evaluated by comparing it to other well-known feature selection algorithms, namely Chi-Square, ANOVA, MI, CMIM, MRMR, SURF*, MultiSURF, MultiSURF*, and ReliefF. When evaluated using multiple classifiers trained on the features selected by different feature selection techniques, the metrics of classification accuracy, weighted F1-score, and ROC-AUC, showed that \(\mu \)-Relief effectively determined relevant features and outperformed other techniques.",Low
Amir Bidaki,Automatic,Springer,http://link.springer.com/article/10.1007/s11036-024-02436-3,Fake News Detection in Large-Scale Social Network with Generalized Bayesian Classification,2024,Type,Mobile Networks and Applications,Core,"Fake news in large-scale social networks is relatively rare, resulting in poor detection performance of deep learning method without sufficient samples. Meantime, false information comes from various sources and forms in large-scale social networks, which increases the difficulty of detection by simple Bayesian decision. Therefore, a method for detecting fake news in large-scale social networks based on a generalized Bayesian classifier is proposed. By using web crawlers to collect news in social network from multiple platforms such as entertainment, education, and medical diseases, and employing the HITS (Hyperlink-Induced Topic Search) algorithm to analyze webpage links, the accuracy of webpage target data retrieval is improved. A network data cleaning function is utilized to remove redundant and cluttered data from social network. A multi-modal Transformer model is employed to extract fusion features of text and image from large-scale social network data. By optimizing the Bayesian classifier using a greedy selection algorithm, a generalized Bayesian classifier is obtained. The extracted features of fake news from social networks are used as inputs to the generalized Bayesian classifier to obtain the prior probability of fake news in social networks. Based on this prior probability, evidence factors that meet the conditions of fake news in large-scale social networks are obtained. By evaluating the numerical values of these evidence factors, the classification and detection of fake news in large-scale social networks are achieved. Experimental results show that the maximum KL divergence value of the proposed method is 0.01, and the maximum Gini coefficient value is 0.1, indicating excellent performance in information cleaning and feature extraction. The maximum number of false positive results is only one sample, demonstrating its ability to accurately detect Fake News in social networks.","Generalized Bayesian classifier
Social Network
Large-scale Network
Fake News
Greedy algorithm",0,,,,"Fake news in large-scale social networks is relatively rare, resulting in poor detection performance of deep learning method without sufficient samples. Meantime, false information comes from various sources and forms in large-scale social networks, which increases the difficulty of detection by simple Bayesian decision. Therefore, a method for detecting fake news in large-scale social networks based on a generalized Bayesian classifier is proposed. By using web crawlers to collect news in social network from multiple platforms such as entertainment, education, and medical diseases, and employing the HITS (Hyperlink-Induced Topic Search) algorithm to analyze webpage links, the accuracy of webpage target data retrieval is improved. A network data cleaning function is utilized to remove redundant and cluttered data from social network. A multi-modal Transformer model is employed to extract fusion features of text and image from large-scale social network data. By optimizing the Bayesian classifier using a greedy selection algorithm, a generalized Bayesian classifier is obtained. The extracted features of fake news from social networks are used as inputs to the generalized Bayesian classifier to obtain the prior probability of fake news in social networks. Based on this prior probability, evidence factors that meet the conditions of fake news in large-scale social networks are obtained. By evaluating the numerical values of these evidence factors, the classification and detection of fake news in large-scale social networks are achieved. Experimental results show that the maximum KL divergence value of the proposed method is 0.01, and the maximum Gini coefficient value is 0.1, indicating excellent performance in information cleaning and feature extraction. The maximum number of false positive results is only one sample, demonstrating its ability to accurately detect Fake News in social networks.",Low
Amir Bidaki,Automatic,Springer,http://link.springer.com/article/10.1007/s10586-021-03518-7,LiMPO: lightweight mobility prediction and offloading framework using machine learning for mobile edge computing,2023,Type,Cluster Computing,Core,"Several applications have emerged with the proliferation of mobile devices to provide communication, learning, social networking, entertainment, and community computing services. Such applications include augmented reality, online gaming, and other real-time applications that need higher computational resources. These applications, executing on mobile devices, often need to access external computing resources and offload the application tasks to the cloud or mobile edge computing (MEC) servers. However, delivering task offloading results to the users in the MEC environment is a challenge, certainly when user mobility is high. Sub-optimal server selection at the offloading stage increases latency, energy consumption and deteriorates both quality of experience and quality of service. Existing techniques proposed in the literature handle computation offloading and mobility management separately. Without considering the real-time mobility factors, the solutions produced are sub-optimal. Some solutions exist to manage mobility, but they involve higher time complexity. We consider the user mobility in offloading decisions and present a lightweight mobility prediction and offloading (LiMPO) framework that offloads the compute-intensive tasks to the predicted user location using artificial neural networks with less complexity. In addition, we propose a multi-objective genetic algorithm based server selection technique that jointly optimizes latency and energy consumption while improving the resource utilization of MEC servers. The performance of the proposed framework is compared with two other techniques task-assignment with optimized mobility and dynamic mobility-aware offloading algorithm for edge computing. The simulation results show that LiMPO outperforms the others by latency reduction, energy efficiency, and enhanced resource utilization.","Mobile edge computing
Task offloading
Location prediction
Machine learning",0,,,,"Several applications have emerged with the proliferation of mobile devices to provide communication, learning, social networking, entertainment, and community computing services. Such applications include augmented reality, online gaming, and other real-time applications that need higher computational resources. These applications, executing on mobile devices, often need to access external computing resources and offload the application tasks to the cloud or mobile edge computing (MEC) servers. However, delivering task offloading results to the users in the MEC environment is a challenge, certainly when user mobility is high. Sub-optimal server selection at the offloading stage increases latency, energy consumption and deteriorates both quality of experience and quality of service. Existing techniques proposed in the literature handle computation offloading and mobility management separately. Without considering the real-time mobility factors, the solutions produced are sub-optimal. Some solutions exist to manage mobility, but they involve higher time complexity. We consider the user mobility in offloading decisions and present a lightweight mobility prediction and offloading (LiMPO) framework that offloads the compute-intensive tasks to the predicted user location using artificial neural networks with less complexity. In addition, we propose a multi-objective genetic algorithm based server selection technique that jointly optimizes latency and energy consumption while improving the resource utilization of MEC servers. The performance of the proposed framework is compared with two other techniques task-assignment with optimized mobility and dynamic mobility-aware offloading algorithm for edge computing. The simulation results show that LiMPO outperforms the others by latency reduction, energy efficiency, and enhanced resource utilization.",Low
Amir Bidaki,Automatic,Springer,http://link.springer.com/article/10.1007/s13042-013-0185-1,Computational awareness for smart grid: a review,2014,Type,International Journal of Machine Learning and Cybernetics,Core,"Smart grid has been an active research area in recent years because almost all the technologies required to build smart grid are mature enough. It is expected that not only can smart grid reduce electricity consumption, but it can also provide a more reliable and versatile service than the traditional power grid can. Although the infrastructure of smart grid all over the world is far from complete yet, there is no doubt that our daily life will benefit a lot from smart grid. Hence, many researches are aimed to point out the challenges and needs of future smart grid. The question is, how do we use the massive data captured by smart meters to provide services that are as “smart” as possible instead of just automatically reading information from the meters. This paper begins with a discussion of the smart grid before we move on to the basic computational awareness for smart grid. A brief review of data mining and machine learning technologies for smart grid, which are often used for computational awareness, is then given to further explain their potentials. Finally, challenges, potentials, open issues, and future trends of smart grid are addressed.",Artificial Intelligence,17 Citations,,,,"Smart grid has been an active research area in recent years because almost all the technologies required to build smart grid are mature enough. It is expected that not only can smart grid reduce electricity consumption, but it can also provide a more reliable and versatile service than the traditional power grid can. Although the infrastructure of smart grid all over the world is far from complete yet, there is no doubt that our daily life will benefit a lot from smart grid. Hence, many researches are aimed to point out the challenges and needs of future smart grid. The question is, how do we use the massive data captured by smart meters to provide services that are as “smart” as possible instead of just automatically reading information from the meters. This paper begins with a discussion of the smart grid before we move on to the basic computational awareness for smart grid. A brief review of data mining and machine learning technologies for smart grid, which are often used for computational awareness, is then given to further explain their potentials. Finally, challenges, potentials, open issues, and future trends of smart grid are addressed.",Low
Amir Bidaki,Automatic,Springer,http://link.springer.com/article/10.1186/s13634-023-01080-5,Active eavesdropping detection: a novel physical layer security in wireless IoT,2023,Type,EURASIP Journal on Advances in Signal Processing,Core,"Considering the variety of Internet of Things (IoT) device types and access methods, it remains necessary to address the security challenges we currently encounter. Physical layer security (PLS) can offer streamlined security solutions for the next generation of IoT networks. Presently, we are witnessing the application of intelligent technologies including machine learning (ML) and artificial intelligence (AI) for precise prevention or detection of security breaches. Active eavesdropping detection is a physical layer security-based method that can differentiate wireless signals between wireless devices through feature classification. However, the operation of numerous IoT devices operate in environments characterized by low signal-to-noise ratios (SNR), and active eavesdropping attack detection during communication is rarely studied. We assume that the wireless system comprising an access point (AP), K authorized users and a proactive eavesdropper (E), following the framework of transforming wireless signals at AP into organized datasets that this article proposes a BP neural network model based on deep learning as a classifier to distinguish eavesdropping and non-eavesdropping attack signals. By conducting experiments under SNRs, the numerical results show that the proposed model has stronger robustness and detection accuracy can significantly improve the up to 19.58% compared with the reference approach, which show the superiority of our proposed method.","Deep learning
Internet of things (IoT)
Physical layer security
Active eavesdropping detection
BP neural network",3 Citations,,,,"Considering the variety of Internet of Things (IoT) device types and access methods, it remains necessary to address the security challenges we currently encounter. Physical layer security (PLS) can offer streamlined security solutions for the next generation of IoT networks. Presently, we are witnessing the application of intelligent technologies including machine learning (ML) and artificial intelligence (AI) for precise prevention or detection of security breaches. Active eavesdropping detection is a physical layer security-based method that can differentiate wireless signals between wireless devices through feature classification. However, the operation of numerous IoT devices operate in environments characterized by low signal-to-noise ratios (SNR), and active eavesdropping attack detection during communication is rarely studied. We assume that the wireless system comprising an access point (AP), K authorized users and a proactive eavesdropper (E), following the framework of transforming wireless signals at AP into organized datasets that this article proposes a BP neural network model based on deep learning as a classifier to distinguish eavesdropping and non-eavesdropping attack signals. By conducting experiments under SNRs, the numerical results show that the proposed model has stronger robustness and detection accuracy can significantly improve the up to 19.58% compared with the reference approach, which show the superiority of our proposed method.",Low
Amir Bidaki,Automatic,Springer,http://link.springer.com/article/10.1007/s11227-022-04453-z,Extreme learning machine and bayesian optimization-driven intelligent framework for IoMT cyber-attack detection,2022,Type,The Journal of Supercomputing,Core,"The Internet of Medical Things (IoMT) is a bionetwork of allied medical devices, sensors, wearable biosensor devices, etc. It is gradually reforming the healthcare industry by leveraging its capabilities to improve personalized healthcare services by enabling seamless communication of medical data. IoMT facilitates prompt emergency responses and provides improved quality of medical services with minimum cost. With the advancement of modern technology, progressively ubiquitous medical devices raise critical security and data privacy concerns through resource constraints and open connectivity. Vulnerabilities in IoMT devices allow unauthorized access for potential entry into healthcare and sensitive personal data. In addition, the patient may experience severe physical damage with the attack on IoMT devices. To provide security to IoMT devices and privacy to patient data, we have proposed a novel IoMT framework with the hybridization of Bayesian optimization and extreme learning machine (ELM). The proposed model derives encouraging performance with enhanced accuracy in decision-making process compared to similar state-of-the-art methods.","IoMT
Extreme learning machine
Bayesian optimization
IoT security",0,,,,"The Internet of Medical Things (IoMT) is a bionetwork of allied medical devices, sensors, wearable biosensor devices, etc. It is gradually reforming the healthcare industry by leveraging its capabilities to improve personalized healthcare services by enabling seamless communication of medical data. IoMT facilitates prompt emergency responses and provides improved quality of medical services with minimum cost. With the advancement of modern technology, progressively ubiquitous medical devices raise critical security and data privacy concerns through resource constraints and open connectivity. Vulnerabilities in IoMT devices allow unauthorized access for potential entry into healthcare and sensitive personal data. In addition, the patient may experience severe physical damage with the attack on IoMT devices. To provide security to IoMT devices and privacy to patient data, we have proposed a novel IoMT framework with the hybridization of Bayesian optimization and extreme learning machine (ELM). The proposed model derives encouraging performance with enhanced accuracy in decision-making process compared to similar state-of-the-art methods.",Low
Amir Bidaki,Automatic,Springer,http://link.springer.com/article/10.1007/s12083-020-00993-4,Machine learning models and techniques for VANET based traffic management: Implementation issues and challenges,2021,Type,Peer-to-Peer Networking and Applications,Core,"Low latency in communication among the vehicles and RSUs, smooth traffic flow, and road safety are the major concerns of the Intelligent Transportation Systems. Vehicular Ad hoc Network (VANET) has gained attention from various research communities for such a matters. These systems need constant monitoring for proper functioning, opening the doors to apply Machine Learning algorithms on enormous data generated from different applications in VANET (for example, crowdsourcing, pollution control, environment monitoring, etc.). Machine Learning is an approach where the system automatically learns and improves itself based on previously processed data. These algorithms provide efficient supervised and unsupervised learning of these collected data, which effectively implements VANET’s objective. We highlighted the safety, communication, and traffic-related issues in VANET systems and their implementation in-feasibility and explored how machine learning algorithms can overcome these issues. Finally, we discussed future direction and challenges, along with a case study depicting a VANET based scenario.","Artificial Intelligence
Automotive Engineering",77 Citations,,,,"Low latency in communication among the vehicles and RSUs, smooth traffic flow, and road safety are the major concerns of the Intelligent Transportation Systems. Vehicular Ad hoc Network (VANET) has gained attention from various research communities for such a matters. These systems need constant monitoring for proper functioning, opening the doors to apply Machine Learning algorithms on enormous data generated from different applications in VANET (for example, crowdsourcing, pollution control, environment monitoring, etc.). Machine Learning is an approach where the system automatically learns and improves itself based on previously processed data. These algorithms provide efficient supervised and unsupervised learning of these collected data, which effectively implements VANET’s objective. We highlighted the safety, communication, and traffic-related issues in VANET systems and their implementation in-feasibility and explored how machine learning algorithms can overcome these issues. Finally, we discussed future direction and challenges, along with a case study depicting a VANET based scenario.",Low
Amir Bidaki,Automatic,Springer,http://link.springer.com/article/10.1007/s11042-018-6185-0,A fast convex hull algorithm inspired by human visual perception,2018,Type,Multimedia Tools and Applications,Core,"This paper proposes a convex hull algorithm for high dimensional point set, which is faster than the well-known Quickhull algorithm in many cases. The main idea of the proposed algorithm is to exclude inner points by early detection of global topological properties. The algorithm firstly computes an initial convex hull of \(2*d + 2^{d}\) extreme points. Then, it discards all the inner points which are inside the inscribed ball of the initial convex hull. The other inner points are processed recursively according to the relationships of points and facets. Maximum inscribed circle affine transformations are also designed to accelerate the computation of the convex hull. Experimental results show that the proposed algorithm achieves a significant saving of computation time in comparison with the Quickhull algorithm in 3, 4 and 5 dimensional space. The space efficiency of the proposed algorithm is also demonstrated by experimental results.",,0,,,,"This paper proposes a convex hull algorithm for high dimensional point set, which is faster than the well-known Quickhull algorithm in many cases. The main idea of the proposed algorithm is to exclude inner points by early detection of global topological properties. The algorithm firstly computes an initial convex hull of \(2*d + 2^{d}\) extreme points. Then, it discards all the inner points which are inside the inscribed ball of the initial convex hull. The other inner points are processed recursively according to the relationships of points and facets. Maximum inscribed circle affine transformations are also designed to accelerate the computation of the convex hull. Experimental results show that the proposed algorithm achieves a significant saving of computation time in comparison with the Quickhull algorithm in 3, 4 and 5 dimensional space. The space efficiency of the proposed algorithm is also demonstrated by experimental results.",Low
Amir Bidaki,Automatic,Springer,http://link.springer.com/article/10.1038/s41598-024-82585-3,A secure and efficient blockchain enabled federated Q-learning model for vehicular Ad-hoc networks,2024,Type,Scientific Reports,Core,"Vehicular Ad-hoc Networks (VANETs) are growing into more desirable targets for malicious individuals due to the quick rise in the number of automated vehicles around the roadside. Secure data transfer is necessary for VANETs to preserve the integrity of the entire network. Federated learning (FL) is often suggested as a safe technique for exchanging data among VANETs, however, its capacity to protect private information is constrained. This research proposes an extra level of security to Federated Q-learning by merging Blockchain technology with VANETs. Initially, traffic data is encrypted utilizing the Extended Elliptic Curve Cryptography (EX-ECC) technique to enhance the security of data. Then, the Federated Q-learning model trains the data and ensures higher privacy protection. Moreover, interplanetary file system (IPFS) technology allows Blockchain storage to improve the security of VANETs information. Additionally, the validation process of the proposed Blockchain framework is performed by utilizing a Delegated Practical Byzantine Fault Tolerance (DPBFT) based consensus algorithm. The proposed approach to federated Q-learning offered by Blockchain technology has the potential to develop VANET safety and performance. Comprehensive simulation tests are performed with several assessment criteria considered for number of vehicles 100, Throughput (102465.8 KB/s), Communication overhead (360.57 Mb), Average Latency (864.425 ms), Communication Time (19.51 s), Encryption time (0.98 ms), Decryption time (1.97 ms), Consensus delay (50 ms) and Validation delay (1.68 ms), respectively. As a result, the proposed approach performs significantly better than the existing approaches.",,0,,,,"Vehicular Ad-hoc Networks (VANETs) are growing into more desirable targets for malicious individuals due to the quick rise in the number of automated vehicles around the roadside. Secure data transfer is necessary for VANETs to preserve the integrity of the entire network. Federated learning (FL) is often suggested as a safe technique for exchanging data among VANETs, however, its capacity to protect private information is constrained. This research proposes an extra level of security to Federated Q-learning by merging Blockchain technology with VANETs. Initially, traffic data is encrypted utilizing the Extended Elliptic Curve Cryptography (EX-ECC) technique to enhance the security of data. Then, the Federated Q-learning model trains the data and ensures higher privacy protection. Moreover, interplanetary file system (IPFS) technology allows Blockchain storage to improve the security of VANETs information. Additionally, the validation process of the proposed Blockchain framework is performed by utilizing a Delegated Practical Byzantine Fault Tolerance (DPBFT) based consensus algorithm. The proposed approach to federated Q-learning offered by Blockchain technology has the potential to develop VANET safety and performance. Comprehensive simulation tests are performed with several assessment criteria considered for number of vehicles 100, Throughput (102465.8 KB/s), Communication overhead (360.57 Mb), Average Latency (864.425 ms), Communication Time (19.51 s), Encryption time (0.98 ms), Decryption time (1.97 ms), Consensus delay (50 ms) and Validation delay (1.68 ms), respectively. As a result, the proposed approach performs significantly better than the existing approaches.",Low
Amir Bidaki,Automatic,Springer,http://link.springer.com/article/10.1007/s10922-023-09785-6,DT-ARO: Decision Tree-Based Artificial Rabbits Optimization to Mitigate IoT Botnet Exploitation,2023,Type,Journal of Network and Systems Management,Core,"The rapid growth of Artificial Intelligence (AI) algorithms has created the opportunity to solve complex problems such as Internet of Things (IoT) botnet attacks. The severity of IoT botnet attacks is a critical challenge for improving the smart IoT environment. Therefore, there is an urgent need to design and implement an efficient detection model to deal with various IoT bot attacks and simultaneously handle issues related to the massive feature space. This paper introduces a wrapper feature selection technique by adapting the Artificial Rabbit Optimization (ARO) algorithm and the Decision Tree (DT) algorithm to detect various types of IoT botnet attacks. During the design of the suggested DT-ARO model, the N-BaIoT datasets were used as a testbed environment. The feature space optimization step was carried out using the ARO algorithm to select only the high-priority features for detecting the IoT botnet attacks. The binary vector technique was used to distinguish the optimal features. The detection engine was performed using the DT algorithm. The conducted experiments have demonstrated the ability of the suggested DT-ARO model to detect various types of IoT botnet attacks, where the accuracy rate was 99.89%. Meanwhile, effectively reducing the feature’s space. In addition, the accomplished results were compared with the latest typical approaches. The DT-ARO model was found to be competitive with these methods and even outperformed them in reducing the feature space.",,0,,,,"The rapid growth of Artificial Intelligence (AI) algorithms has created the opportunity to solve complex problems such as Internet of Things (IoT) botnet attacks. The severity of IoT botnet attacks is a critical challenge for improving the smart IoT environment. Therefore, there is an urgent need to design and implement an efficient detection model to deal with various IoT bot attacks and simultaneously handle issues related to the massive feature space. This paper introduces a wrapper feature selection technique by adapting the Artificial Rabbit Optimization (ARO) algorithm and the Decision Tree (DT) algorithm to detect various types of IoT botnet attacks. During the design of the suggested DT-ARO model, the N-BaIoT datasets were used as a testbed environment. The feature space optimization step was carried out using the ARO algorithm to select only the high-priority features for detecting the IoT botnet attacks. The binary vector technique was used to distinguish the optimal features. The detection engine was performed using the DT algorithm. The conducted experiments have demonstrated the ability of the suggested DT-ARO model to detect various types of IoT botnet attacks, where the accuracy rate was 99.89%. Meanwhile, effectively reducing the feature’s space. In addition, the accomplished results were compared with the latest typical approaches. The DT-ARO model was found to be competitive with these methods and even outperformed them in reducing the feature space.",Low
Amir Bidaki,Automatic,Springer,http://link.springer.com/article/10.1007/s00521-020-04760-7,Critical infrastructure protection based on memory-augmented meta-learning framework,2020,Type,Neural Computing and Applications,Core,"Critical infrastructures are related to systems which are essential for sustaining the important functions of a society. Their potential failures can cause serious problems not only to the population and economy but also to national security as well. The importance of these infrastructures calls for measures toward their security and protection. The aim was the reduction of risk related to natural disasters, terrorist acts, and cyberthreats. Traditional security systems, even those that employ intelligent algorithms, fail to prevent advanced zero-day attacks as they require constant training. This research proposes a novel meta-learning architecture that considers the neural turing machines as the approach upon which the model is founded. The introduced model allows for the memorization of useful data from past processes, by integrating external storage memory. Moreover, it facilitates the rapid integration of new information without the need for retraining. In particular, the proposed novel architecture is called memory-augmented neural network (M-ANN) whose core is a sophisticated, very fast, and highly efficient extreme learning machine. The M-ANN is assisted by a series of original modifications, related to fine-tuning of training, to memory retrieval mechanisms, to addressing techniques, and to ways of attention-weight allocation to memory vectors. The efficiency of the proposed system has been successfully tested using an extremely complex scenario for the protection of critical infrastructures. According to the testing scenario, memory could quickly encode and record information about new types of attacks, while any stored representation from previous experience was easily and consistently accessible, to maximize the detection efficiency.",Artificial Intelligence,4 Citations,,,,"Critical infrastructures are related to systems which are essential for sustaining the important functions of a society. Their potential failures can cause serious problems not only to the population and economy but also to national security as well. The importance of these infrastructures calls for measures toward their security and protection. The aim was the reduction of risk related to natural disasters, terrorist acts, and cyberthreats. Traditional security systems, even those that employ intelligent algorithms, fail to prevent advanced zero-day attacks as they require constant training. This research proposes a novel meta-learning architecture that considers the neural turing machines as the approach upon which the model is founded. The introduced model allows for the memorization of useful data from past processes, by integrating external storage memory. Moreover, it facilitates the rapid integration of new information without the need for retraining. In particular, the proposed novel architecture is called memory-augmented neural network (M-ANN) whose core is a sophisticated, very fast, and highly efficient extreme learning machine. The M-ANN is assisted by a series of original modifications, related to fine-tuning of training, to memory retrieval mechanisms, to addressing techniques, and to ways of attention-weight allocation to memory vectors. The efficiency of the proposed system has been successfully tested using an extremely complex scenario for the protection of critical infrastructures. According to the testing scenario, memory could quickly encode and record information about new types of attacks, while any stored representation from previous experience was easily and consistently accessible, to maximize the detection efficiency.",High
Amir Bidaki,Automatic,Springer,http://link.springer.com/article/10.1007/s10489-022-04101-2,Multi-view multi-manifold learning with local and global structure preservation,2023,Type,Applied Intelligence,Core,"Most existing multi-view learning methods adopt a single geometrical model to describe multi-class and heterogeneous data on the original feature space without considering the manifold structure contained in the dataset. This may lose some information when detecting nonlinear forms in real world datasets. Moreover, traditional kernel-based multi-view learning methods pay insufficient attention to preserving the global or local structure of training samples. To address these two problems, this paper proposes a novel method called multi-view multi-manifold learning with local and global structure preservation (MML-LGSP). First, in the feature spaces obtained by multiple empirical kernel mapping, the MML-LGSP uses nonlinear feature extraction to maintain the intrinsic low-dimensional embedding of the original data from multiple views. Second, the discriminant projection matrix is calculated with the between-class graph representing multi-manifold information and the within-class graph representing sub-manifold information. The MML-LGSP method combines multi-manifold learning and multi-view learning based on multiple empirical kernel learning into one unified learning framework. It utilizes the local and global geometrical information around the data point of each view and shows better classification performance. Extensive experiment results of various real-world multi-view datasets demonstrate the superiority our method over the state-of-the-art methods.",Artificial Intelligence,0,,,,"Most existing multi-view learning methods adopt a single geometrical model to describe multi-class and heterogeneous data on the original feature space without considering the manifold structure contained in the dataset. This may lose some information when detecting nonlinear forms in real world datasets. Moreover, traditional kernel-based multi-view learning methods pay insufficient attention to preserving the global or local structure of training samples. To address these two problems, this paper proposes a novel method called multi-view multi-manifold learning with local and global structure preservation (MML-LGSP). First, in the feature spaces obtained by multiple empirical kernel mapping, the MML-LGSP uses nonlinear feature extraction to maintain the intrinsic low-dimensional embedding of the original data from multiple views. Second, the discriminant projection matrix is calculated with the between-class graph representing multi-manifold information and the within-class graph representing sub-manifold information. The MML-LGSP method combines multi-manifold learning and multi-view learning based on multiple empirical kernel learning into one unified learning framework. It utilizes the local and global geometrical information around the data point of each view and shows better classification performance. Extensive experiment results of various real-world multi-view datasets demonstrate the superiority our method over the state-of-the-art methods.",Low
Amir Bidaki,Automatic,Springer,http://link.springer.com/article/10.1007/s11704-021-0412-y,Efficient protocols for heavy hitter identification with local differential privacy,2022,Type,Frontiers of Computer Science,Core,"Local differential privacy (LDP), which is a technique that employs unbiased statistical estimations instead of real data, is usually adopted in data collection, as it can protect every user’s privacy and prevent the leakage of sensitive information. The segment pairs method (SPM), multiple-channel method (MCM) and prefix extending method (PEM) are three known LDP protocols for heavy hitter identification as well as the frequency oracle (FO) problem with large domains. However, the low scalability of these three LDP algorithms often limits their application. Specifically, communication and computation strongly affect their efficiency. Moreover, excessive grouping or sharing of privacy budgets makes the results inaccurate. To address the above-mentioned problems, this study proposes independent channel (IC) and mixed independent channel (MIC), which are efficient LDP protocols for FO with a large domains. We design a flexible method for splitting a large domain to reduce the number of sub-domains. Further, we employ the false positive rate with interaction to obtain an accurate estimation. Numerical experiments demonstrate that IC outperforms all the existing solutions under the same privacy guarantee while MIC performs well under a small privacy budget with the lowest communication cost.","local differential privacy
frequency oracle
heavy hitter",0,,,,"Local differential privacy (LDP), which is a technique that employs unbiased statistical estimations instead of real data, is usually adopted in data collection, as it can protect every user’s privacy and prevent the leakage of sensitive information. The segment pairs method (SPM), multiple-channel method (MCM) and prefix extending method (PEM) are three known LDP protocols for heavy hitter identification as well as the frequency oracle (FO) problem with large domains. However, the low scalability of these three LDP algorithms often limits their application. Specifically, communication and computation strongly affect their efficiency. Moreover, excessive grouping or sharing of privacy budgets makes the results inaccurate. To address the above-mentioned problems, this study proposes independent channel (IC) and mixed independent channel (MIC), which are efficient LDP protocols for FO with a large domains. We design a flexible method for splitting a large domain to reduce the number of sub-domains. Further, we employ the false positive rate with interaction to obtain an accurate estimation. Numerical experiments demonstrate that IC outperforms all the existing solutions under the same privacy guarantee while MIC performs well under a small privacy budget with the lowest communication cost.",Low
Amir Bidaki,Automatic,Springer,http://link.springer.com/article/10.1007/s40860-022-00176-3,Making distributed edge machine learning for resource-constrained communities and environments smarter: contexts and challenges,2023,Type,Journal of Reliable Intelligent Environments,Core,"The maturity of machine learning (ML) development and the decreasing deployment cost of capable edge devices have proliferated the development and deployment of edge ML solutions for critical IoT-based business applications. The combination of edge computing and ML not only addresses the development cost barrier, but also solves the obstacles due to the lack of powerful cloud data centers. However, not only the edge ML research and development is still at an early stage and requires substantial skills normally missed in resource-constrained communities, but also various infrastructure constraints w.r.t. network reliability and computing power, and business contexts from the resource-constrained environments require different considerations to make edge ML applications context aware through smart and intelligent runtime strategies. In this paper, we analyze representative real-world business scenarios for edge ML solutions and their contexts in resource-constrained communities and environments. We identify and map the key distinguished contexts of distributed edge ML and discuss the impacts of these contexts on data and software components and deployment models. Finally, we present key research areas, how we should approach them, and possible tooling for making edge machine learning solutions smarter in resource-constrained communities and environments.",Artificial Intelligence,0,,,,"The maturity of machine learning (ML) development and the decreasing deployment cost of capable edge devices have proliferated the development and deployment of edge ML solutions for critical IoT-based business applications. The combination of edge computing and ML not only addresses the development cost barrier, but also solves the obstacles due to the lack of powerful cloud data centers. However, not only the edge ML research and development is still at an early stage and requires substantial skills normally missed in resource-constrained communities, but also various infrastructure constraints w.r.t. network reliability and computing power, and business contexts from the resource-constrained environments require different considerations to make edge ML applications context aware through smart and intelligent runtime strategies. In this paper, we analyze representative real-world business scenarios for edge ML solutions and their contexts in resource-constrained communities and environments. We identify and map the key distinguished contexts of distributed edge ML and discuss the impacts of these contexts on data and software components and deployment models. Finally, we present key research areas, how we should approach them, and possible tooling for making edge machine learning solutions smarter in resource-constrained communities and environments.",Low
Amir Bidaki,Automatic,Springer,http://link.springer.com/article/10.1007/s11227-024-06503-0,Enhancing beyond 5G connectivity and security: optimizing user-to-multiple AP associations with hybrid deep learning and innovative optimization techniques,2024,Type,The Journal of Supercomputing,Core,"The investigation of sophisticated methods to maximize user-to-multiple access point (AP) associations has been spurred by the unwavering need for fast, dependable connectivity in Beyond 5G (B5G) networks. This paper proposed a novel approach for selecting optimal AP by combining state-of-the-art deep learning (DL) architectures such as Alex Net, ResNet50, and Darknet53 with an innovative hybrid optimization algorithm. The adaptive feature of the FOX-inspired optimization algorithm is combined with the efficiency of the standard Gazelle optimization algorithm in this study. Combining these two optimization strategies guarantees a balanced trade-off between exploration and exploitation, leading to selecting APs that optimize network performance while adjusting to changing environmental conditions. This strategy not only improves connectivity but also advances wireless network evolution, establishing the way for effective and flexible B5G communication systems. The optimization of user-to-multiple AP associations in B5G networks presents a comprehensive challenge that this research attempts to handle through the smooth integration of cutting-edge optimization methods with DL approaches.","5G connectivity
User-to-multiple access point
Deep learning
Security
DarkNet 53
Gazelle optimization algorithm",0,,,,"The investigation of sophisticated methods to maximize user-to-multiple access point (AP) associations has been spurred by the unwavering need for fast, dependable connectivity in Beyond 5G (B5G) networks. This paper proposed a novel approach for selecting optimal AP by combining state-of-the-art deep learning (DL) architectures such as Alex Net, ResNet50, and Darknet53 with an innovative hybrid optimization algorithm. The adaptive feature of the FOX-inspired optimization algorithm is combined with the efficiency of the standard Gazelle optimization algorithm in this study. Combining these two optimization strategies guarantees a balanced trade-off between exploration and exploitation, leading to selecting APs that optimize network performance while adjusting to changing environmental conditions. This strategy not only improves connectivity but also advances wireless network evolution, establishing the way for effective and flexible B5G communication systems. The optimization of user-to-multiple AP associations in B5G networks presents a comprehensive challenge that this research attempts to handle through the smooth integration of cutting-edge optimization methods with DL approaches.",Low
Amir Bidaki,Automatic,Springer,http://link.springer.com/article/10.1007/s10207-024-00945-6,Detecting unknown vulnerabilities in smart contracts with the CNN-BiLSTM model,2024,Type,International Journal of Information Security,Core,"Smart contracts, fundamental to blockchain technology, are extensively utilized in diverse fields such as finance, supply chain management, and beyond. Nevertheless, once deployed, their capability to handle significant transactions and their unchangeable nature bring about substantial risks, potentially leading to severe security breaches and financial losses. While existing research has made progress in detecting unknown vulnerabilities, identifying novel, unforeseen security issues remains significant. We aim to contribute to this ongoing effort by introducing an innovative approach that capitalizes on the similarities between known and unknown vulnerabilities. We propose a CNN-BiLSTM model meticulously designed to identify features of known vulnerabilities and employ them to detect potential unknown vulnerabilities. We intricately gather opcode sequences generated during smart contract execution using Geth instrumentation and meticulously analyze them. We conduct rigorous experiments to validate the model’s effectiveness in detecting potential unknown vulnerabilities. Our approach represents a step forward in blockchain security by providing proactive measures to strengthen smart contract security against emerging threats.","Blockchain
Smart contracts
Unknown vulnerabilities
Opcode sequences
CNN-BiLSTM",0,,,,"Smart contracts, fundamental to blockchain technology, are extensively utilized in diverse fields such as finance, supply chain management, and beyond. Nevertheless, once deployed, their capability to handle significant transactions and their unchangeable nature bring about substantial risks, potentially leading to severe security breaches and financial losses. While existing research has made progress in detecting unknown vulnerabilities, identifying novel, unforeseen security issues remains significant. We aim to contribute to this ongoing effort by introducing an innovative approach that capitalizes on the similarities between known and unknown vulnerabilities. We propose a CNN-BiLSTM model meticulously designed to identify features of known vulnerabilities and employ them to detect potential unknown vulnerabilities. We intricately gather opcode sequences generated during smart contract execution using Geth instrumentation and meticulously analyze them. We conduct rigorous experiments to validate the model’s effectiveness in detecting potential unknown vulnerabilities. Our approach represents a step forward in blockchain security by providing proactive measures to strengthen smart contract security against emerging threats.",Low
Amir Bidaki,Automatic,Springer,http://link.springer.com/article/10.2991/ijndc.2013.1.3.2,BSP-Based Support Vector Regression Machine Parallel Framework,2013,Type,International Journal of Networked and Distributed Computing,Core,"In this paper, we investigate the distributed parallel Support Vector Machine training strategy, and then propose a BSP-Based Support Vector Regression Machine Parallel Framework which can implement the most of distributed Support Vector Regression Machine algorithms. The major difference in these algorithms is the network topology among distributed nodes. Therefore, we adopt the Bulk Synchronous Parallel model to solve the strongly connected graph problem in exchanging support vectors among distributed nodes. In addition, we introduce the dynamic algorithms which can change the strongly connected graph among SVR distributed nodes in every BSP’s super-step. The performance of this framework has been analyzed and evaluated with KDD99 data and four DPSVR algorithms on the high-performance computer. The results prove that the framework can implement the most of distributed SVR algorithms and keep the performance of original algorithms.","parallel computing
bulk synchronous parallel
support vector regression machine (SVR)
regression prediction",0,,,,"In this paper, we investigate the distributed parallel Support Vector Machine training strategy, and then propose a BSP-Based Support Vector Regression Machine Parallel Framework which can implement the most of distributed Support Vector Regression Machine algorithms. The major difference in these algorithms is the network topology among distributed nodes. Therefore, we adopt the Bulk Synchronous Parallel model to solve the strongly connected graph problem in exchanging support vectors among distributed nodes. In addition, we introduce the dynamic algorithms which can change the strongly connected graph among SVR distributed nodes in every BSP’s super-step. The performance of this framework has been analyzed and evaluated with KDD99 data and four DPSVR algorithms on the high-performance computer. The results prove that the framework can implement the most of distributed SVR algorithms and keep the performance of original algorithms.",Low
Amir Bidaki,Automatic,Springer,http://link.springer.com/article/10.1007/s11390-023-3073-5,"Ubiquitous WiFi and Acoustic Sensing: Principles, Technologies, and Applications",2023,Type,Journal of Computer Science and Technology,Core,"With the increasing pervasiveness of mobile devices such as smartphones, smart TVs, and wearables, smart sensing, transforming the physical world into digital information based on various sensing medias, has drawn researchers’ great attention. Among different sensing medias, WiFi and acoustic signals stand out due to their ubiquity and zero hardware cost. Based on different basic principles, researchers have proposed different technologies for sensing applications with WiFi and acoustic signals covering human activity recognition, motion tracking, indoor localization, health monitoring, and the like. To enable readers to get a comprehensive understanding of ubiquitous wireless sensing, we conduct a survey of existing work to introduce their underlying principles, proposed technologies, and practical applications. Besides we also discuss some open issues of this research area. Our survey reals that as a promising research direction, WiFi and acoustic sensing technologies can bring about fancy applications, but still have limitations in hardware restriction, robustness, and applicability.",Artificial Intelligence,0,,,,"With the increasing pervasiveness of mobile devices such as smartphones, smart TVs, and wearables, smart sensing, transforming the physical world into digital information based on various sensing medias, has drawn researchers’ great attention. Among different sensing medias, WiFi and acoustic signals stand out due to their ubiquity and zero hardware cost. Based on different basic principles, researchers have proposed different technologies for sensing applications with WiFi and acoustic signals covering human activity recognition, motion tracking, indoor localization, health monitoring, and the like. To enable readers to get a comprehensive understanding of ubiquitous wireless sensing, we conduct a survey of existing work to introduce their underlying principles, proposed technologies, and practical applications. Besides we also discuss some open issues of this research area. Our survey reals that as a promising research direction, WiFi and acoustic sensing technologies can bring about fancy applications, but still have limitations in hardware restriction, robustness, and applicability.",Low
Amir Bidaki,Automatic,Springer,http://link.springer.com/article/10.1007/s11277-020-07446-4,"Internet of Things (IoT), Applications and Challenges: A Comprehensive Review",2020,Type,Wireless Personal Communications,Core,"During recent years, one of the most familiar names scaling new heights and creating a benchmark in the world is the Internet of Things (IoT). It is indeed the future of communication that has transformed things (objects) of the real-world into smart objects. The functional aspect of IoT is to unite every object of the world under one common infrastructure; in such a manner that humans not only have the ability to control those objects; but to provide regular and timely updates on the current status. IoT concepts were proposed a couple of years ago and it may not be incorrect to quote that this term has become a benchmark for establishing communication among objects. In context to the present standings of IoT, a comprehensive review of literature has been undertaken on various aspects of IoT, i.e., technologies, applications, challenges, etc. This paper evaluates various contributions of researchers in different areas of applications. These papers were investigated on various parameters identified in each application domain. Furthermore, existing challenges in these areas are highlighted. Future research directions in the field of IoT have also been highlighted in the study to equip novel researchers in this area to assess the current standings of IoT and to improve upon them with innovative ideas.","Internet of Things (IoT)
Wireless sensor networks (WSN)
Radio-frequency identification (RFID)
Near-field communication (NFC)
Internet of Energy (IoE)
Global Positioning System (GPS)
Representational State Transfer (REST)
Information and Communication Technology (ICT)
Service Oriented Architecture (SOA)",0,,,,"During recent years, one of the most familiar names scaling new heights and creating a benchmark in the world is the Internet of Things (IoT). It is indeed the future of communication that has transformed things (objects) of the real-world into smart objects. The functional aspect of IoT is to unite every object of the world under one common infrastructure; in such a manner that humans not only have the ability to control those objects; but to provide regular and timely updates on the current status. IoT concepts were proposed a couple of years ago and it may not be incorrect to quote that this term has become a benchmark for establishing communication among objects. In context to the present standings of IoT, a comprehensive review of literature has been undertaken on various aspects of IoT, i.e., technologies, applications, challenges, etc. This paper evaluates various contributions of researchers in different areas of applications. These papers were investigated on various parameters identified in each application domain. Furthermore, existing challenges in these areas are highlighted. Future research directions in the field of IoT have also been highlighted in the study to equip novel researchers in this area to assess the current standings of IoT and to improve upon them with innovative ideas.",Low
Amir Bidaki,Automatic,Springer,http://link.springer.com/article/10.1186/s41239-024-00474-1,An empirical analysis of EFL teachers’ digital literacy in Chinese higher education institutions,2024,Type,International Journal of Educational Technology in Higher Education,Core,"In 2022, China’s Educational Ministry, for the first time, released an industry standard for teachers’ digital literacy. This standard provides a holistic framework for teachers’ digital literacy (TDL) in five dimensions. Since few studies have investigated EFL teachers’ digital literacy, the relationships among the five dimensions proposed for EFL teachers’ digital literacy remain unknown. Therefore, this research applied a quantitative method using a five-point Likert questionnaire designed based on TDL. Ninety-two EFL teachers from two universities and one higher vocational college in China participated in this questionnaire research. Partial least squares structural equation modeling was used to assess the relationships between these five dimensions. The relationships between the five TDL dimensions were highlighted by the findings, which supported the proposed model. In addition, implications for enhancing EFL teachers’ future digital literacy have been provided to facilitate and favor EFL teachers’ digital literacy development.",Digital Education and Educational Technology,1 Citation,,,,"In 2022, China’s Educational Ministry, for the first time, released an industry standard for teachers’ digital literacy. This standard provides a holistic framework for teachers’ digital literacy (TDL) in five dimensions. Since few studies have investigated EFL teachers’ digital literacy, the relationships among the five dimensions proposed for EFL teachers’ digital literacy remain unknown. Therefore, this research applied a quantitative method using a five-point Likert questionnaire designed based on TDL. Ninety-two EFL teachers from two universities and one higher vocational college in China participated in this questionnaire research. Partial least squares structural equation modeling was used to assess the relationships between these five dimensions. The relationships between the five TDL dimensions were highlighted by the findings, which supported the proposed model. In addition, implications for enhancing EFL teachers’ future digital literacy have been provided to facilitate and favor EFL teachers’ digital literacy development.",Low
Amir Bidaki,Automatic,Springer,http://link.springer.com/article/10.1007/s13218-022-00763-9,"Digital Forensics AI: Evaluating, Standardizing and Optimizing Digital Evidence Mining Techniques",2022,Type,KI - Künstliche Intelligenz,Core,"The impact of AI on numerous sectors of our society and its successes over the years indicate that it can assist in resolving a variety of complex digital forensics investigative problems. Forensics analysis can make use of machine learning models’ pattern detection and recognition capabilities to uncover hidden evidence in digital artifacts that would have been missed if conducted manually. Numerous works have proposed ways for applying AI to digital forensics; nevertheless, scepticism regarding the opacity of AI has impeded the domain’s adequate formalization and standardization. We present three critical instruments necessary for the development of sound machine-driven digital forensics methodologies in this paper. We cover various methods for evaluating, standardizing, and optimizing techniques applicable to artificial intelligence models used in digital forensics. Additionally, we describe several applications of these instruments in digital forensics, emphasizing their strengths and weaknesses that may be critical to the methods’ admissibility in a judicial process.",Artificial Intelligence,8 Citations,,,,"The impact of AI on numerous sectors of our society and its successes over the years indicate that it can assist in resolving a variety of complex digital forensics investigative problems. Forensics analysis can make use of machine learning models’ pattern detection and recognition capabilities to uncover hidden evidence in digital artifacts that would have been missed if conducted manually. Numerous works have proposed ways for applying AI to digital forensics; nevertheless, scepticism regarding the opacity of AI has impeded the domain’s adequate formalization and standardization. We present three critical instruments necessary for the development of sound machine-driven digital forensics methodologies in this paper. We cover various methods for evaluating, standardizing, and optimizing techniques applicable to artificial intelligence models used in digital forensics. Additionally, we describe several applications of these instruments in digital forensics, emphasizing their strengths and weaknesses that may be critical to the methods’ admissibility in a judicial process.",Low
Amir Bidaki,Automatic,Springer,http://link.springer.com/article/10.1186/s42400-024-00243-7,GLDOC: detection of implicitly malicious MS-Office documents using graph convolutional networks,2024,Type,Cybersecurity,Core,"Nowadays, the malicious MS-Office document has already become one of the most effective attacking vectors in APT attacks. Though many protection mechanisms are provided, they have been proved easy to bypass, and the existed detection methods show poor performance when facing malicious documents with unknown vulnerabilities or with few malicious behaviors. In this paper, we first introduce the definition of im-documents, to describe those vulnerable documents which show implicitly malicious behaviors and escape most of public antivirus engines. Then we present GLDOC—a GCN based framework that is aimed at effectively detecting im-documents with dynamic analysis, and improving the possible blind spots of past detection methods. Besides the system call which is the only focus in most researches, we capture all dynamic behaviors in sandbox, take the process tree into consideration and reconstruct both of them into graphs. Using each line to learn each graph, GLDOC trains a 2-channel network as well as a classifier to formulate the malicious document detection problem into a graph learning and classification problem. Experiments show that GLDOC has a comprehensive balance of accuracy rate and false alarm rate − 95.33% and 4.33% respectively, outperforming other detection methods. When further testing in a simulated 5-day attacking scenario, our proposed framework still maintains a stable and high detection accuracy on the unknown vulnerabilities.","Im-document
APT attack
GCN
Dynamic analysis
Malicious document detection",0,,,,"Nowadays, the malicious MS-Office document has already become one of the most effective attacking vectors in APT attacks. Though many protection mechanisms are provided, they have been proved easy to bypass, and the existed detection methods show poor performance when facing malicious documents with unknown vulnerabilities or with few malicious behaviors. In this paper, we first introduce the definition of im-documents, to describe those vulnerable documents which show implicitly malicious behaviors and escape most of public antivirus engines. Then we present GLDOC—a GCN based framework that is aimed at effectively detecting im-documents with dynamic analysis, and improving the possible blind spots of past detection methods. Besides the system call which is the only focus in most researches, we capture all dynamic behaviors in sandbox, take the process tree into consideration and reconstruct both of them into graphs. Using each line to learn each graph, GLDOC trains a 2-channel network as well as a classifier to formulate the malicious document detection problem into a graph learning and classification problem. Experiments show that GLDOC has a comprehensive balance of accuracy rate and false alarm rate − 95.33% and 4.33% respectively, outperforming other detection methods. When further testing in a simulated 5-day attacking scenario, our proposed framework still maintains a stable and high detection accuracy on the unknown vulnerabilities.",Low
Amir Bidaki,Automatic,Springer,http://link.springer.com/article/10.1007/s43926-024-00058-5,"Current trends of blockchain technology: architecture, applications, challenges, and opportunities",2024,Type,Discover Internet of Things,Core,"Blockchain technology has gained attention in recent times owing to its ability to revolutionize traditional trade through its distributed ledger attribute. The prompt advancement of blockchain demands new systematic studies to investigate and analyze the existing knowledge in this domain. In the current work, the present standing and emerging trends of blockchain have been analyzed to direct both new and experienced researchers in establishing a baseline for future research projects. Likewise, the research advancement of consensus protocol was reviewed with a particular emphasis on their security perspective. Accordingly, the attributes, appropriate scenarios, and probable weaknesses of different consensus protocols and their future trends were reviewed. This helps in scrutinizing how blockchain technology can be applied to a variety of emerging fields, including economics, healthcare, information systems, wireless networks, and smart grids. Additionally, the current evaluation provides a throughout discussion of blockchain applications in various fields. Finally, the paper offers a brief insight into limitations and prospective future development in this domain. Overall, the aim is to aid newbies in investigating and scheming new solutions while considering the present demands and issues.","Blockchain
Consensus protocol
Decentralized ledger
Merkle tree
Security",0,,,,"Blockchain technology has gained attention in recent times owing to its ability to revolutionize traditional trade through its distributed ledger attribute. The prompt advancement of blockchain demands new systematic studies to investigate and analyze the existing knowledge in this domain. In the current work, the present standing and emerging trends of blockchain have been analyzed to direct both new and experienced researchers in establishing a baseline for future research projects. Likewise, the research advancement of consensus protocol was reviewed with a particular emphasis on their security perspective. Accordingly, the attributes, appropriate scenarios, and probable weaknesses of different consensus protocols and their future trends were reviewed. This helps in scrutinizing how blockchain technology can be applied to a variety of emerging fields, including economics, healthcare, information systems, wireless networks, and smart grids. Additionally, the current evaluation provides a throughout discussion of blockchain applications in various fields. Finally, the paper offers a brief insight into limitations and prospective future development in this domain. Overall, the aim is to aid newbies in investigating and scheming new solutions while considering the present demands and issues.",Low
Amir Bidaki,Automatic,Springer,http://link.springer.com/article/10.1007/s10115-024-02201-8,Community detection in social networks using machine learning: a systematic mapping study,2024,Type,Knowledge and Information Systems,Core,"One of the important issues in social networks is the social communities which are formed by interactions between its members. Three types of community including overlapping, non-overlapping, and hidden are detected by different approaches. Regarding the importance of community detection in social networks, this paper provides a systematic mapping of machine learning-based community detection approaches. The study aimed to show the type of communities in social networks along with the algorithms of machine learning that have been used for community detection. After carrying out the steps of mapping and removing useless references, 246 papers were selected to answer the questions of this research. The results of the research indicated that unsupervised machine learning-based algorithms with 41.46% (such as k means) are the most used categories to detect communities in social networks due to their low processing overheads. On the other hand, there has been a significant increase in the use of deep learning since 2020 which has sufficient performance for community detection in large-volume data. With regard to the ability of NMI to measure the correlation or similarity between communities, with 53.25%, it is the most frequently used metric to evaluate the performance of community identifications. Furthermore, considering availability, low in size, and lack of multiple edge and loops, dataset Zachary’s Karate Club with 26.42% is the most used dataset for community detection research in social networks.","Community detection
Social network
Systematic mapping study
Machine learning",0,,,,"One of the important issues in social networks is the social communities which are formed by interactions between its members. Three types of community including overlapping, non-overlapping, and hidden are detected by different approaches. Regarding the importance of community detection in social networks, this paper provides a systematic mapping of machine learning-based community detection approaches. The study aimed to show the type of communities in social networks along with the algorithms of machine learning that have been used for community detection. After carrying out the steps of mapping and removing useless references, 246 papers were selected to answer the questions of this research. The results of the research indicated that unsupervised machine learning-based algorithms with 41.46% (such as k means) are the most used categories to detect communities in social networks due to their low processing overheads. On the other hand, there has been a significant increase in the use of deep learning since 2020 which has sufficient performance for community detection in large-volume data. With regard to the ability of NMI to measure the correlation or similarity between communities, with 53.25%, it is the most frequently used metric to evaluate the performance of community identifications. Furthermore, considering availability, low in size, and lack of multiple edge and loops, dataset Zachary’s Karate Club with 26.42% is the most used dataset for community detection research in social networks.",Low
Amir Bidaki,Automatic,Springer,http://link.springer.com/article/10.1007/s10878-023-01065-y,Robust optimization for minimizing energy consumption of multicast transmissions in coded wireless packet networks under distance uncertainty,2023,Type,Journal of Combinatorial Optimization,Core,"Multicast transmissions in coded wireless packet networks can be affected by uncertain factors such as the distance between nodes. We develop a robust optimization method to minimize the energy consumption of such multicasts. We therefore consider the distances to belong to closed convex uncertainty sets. As solution, we select the optimum in the worst case over these uncertainty sets. We prove that the complexity of obtaining this robust solution is similar to that of determining a solution of the problem without uncertainty. Numerical results show that the proposed solution significantly reduces the energy consumption of a multicast connection and that it can be obtained quickly enough for practical applications. Compared with the optimal solution of the deterministic problem, the robust results only exhibit a small performance loss, even if the size of the uncertainty set is notably large.","Network communication
Coded networks
Wireless networks
Uncertainty
Robust optimization",0,,,,"Multicast transmissions in coded wireless packet networks can be affected by uncertain factors such as the distance between nodes. We develop a robust optimization method to minimize the energy consumption of such multicasts. We therefore consider the distances to belong to closed convex uncertainty sets. As solution, we select the optimum in the worst case over these uncertainty sets. We prove that the complexity of obtaining this robust solution is similar to that of determining a solution of the problem without uncertainty. Numerical results show that the proposed solution significantly reduces the energy consumption of a multicast connection and that it can be obtained quickly enough for practical applications. Compared with the optimal solution of the deterministic problem, the robust results only exhibit a small performance loss, even if the size of the uncertainty set is notably large.",Low
Amir Bidaki,Automatic,Springer,http://link.springer.com/article/10.1007/s10586-017-0830-7,An immune-inspired political boycotts action prediction paradigm,2017,Type,Cluster Computing,Core,"The political boycotts action has been highly valued and studied by social scientists. So the research on political boycotts prediction has important theoretical research value and significance. In order to explore an efficacious method of political boycotts action prediction problem, and enhance the prediction accuracy, a novel immune-inspired political boycotts action prediction paradigm (referred as Aipo) is proposed. Aipo first initialize the pools of antibody cell and memory cells; then through the training of each antigen, antibody cells will evolve; after antibody cells’ convergence, the best antibody cell will be selected to update the pool of memory cell; finally the political boycott prediction is accomplished by through memory cells with KNN classification of the test data. The proposed algorithm has the characteristics of non-linearity, and the characteristics of biological immune system, such as clonal selection, immune network and immune memory. The proposed paradigm is evaluated on World Values Survey datasets, and the experimental results show that Aipo is effective and efficient.",Artificial Intelligence,1 Citation,,,,"The political boycotts action has been highly valued and studied by social scientists. So the research on political boycotts prediction has important theoretical research value and significance. In order to explore an efficacious method of political boycotts action prediction problem, and enhance the prediction accuracy, a novel immune-inspired political boycotts action prediction paradigm (referred as Aipo) is proposed. Aipo first initialize the pools of antibody cell and memory cells; then through the training of each antigen, antibody cells will evolve; after antibody cells’ convergence, the best antibody cell will be selected to update the pool of memory cell; finally the political boycott prediction is accomplished by through memory cells with KNN classification of the test data. The proposed algorithm has the characteristics of non-linearity, and the characteristics of biological immune system, such as clonal selection, immune network and immune memory. The proposed paradigm is evaluated on World Values Survey datasets, and the experimental results show that Aipo is effective and efficient.",Low
Amir Bidaki,Automatic,Springer,http://link.springer.com/article/10.1007/s10115-019-01429-z,A survey of recent methods on deriving topics from Twitter: algorithm to evaluation,2020,Type,Knowledge and Information Systems,Core,"In recent years, studies related to topic derivation in Twitter have gained a lot of interest from businesses and academics. The interconnection between users and information has made social media, especially Twitter, an ultimate platform for propagation of information about events in real time. Many applications require topic derivation from this social media platform. These include, for example, disaster management, outbreak detection, situation awareness, surveillance, and market analysis. Deriving topics from Twitter is challenging due to the short content of the individual posts. The environment itself is also highly dynamic. This paper presents a review of recent methods proposed to derive topics from social media platform from algorithms to evaluations. With regard to algorithms, we classify them based on the features they exploit, such as content, social interactions, and temporal aspects. In terms of evaluations, we discuss the datasets and metrics generally used to evaluate the methods. Finally, we highlight the gaps in the research this far and the problems that remain to be addressed.","Topic derivation
Twitter analysis
Algorithms
Evaluations",0,,,,"In recent years, studies related to topic derivation in Twitter have gained a lot of interest from businesses and academics. The interconnection between users and information has made social media, especially Twitter, an ultimate platform for propagation of information about events in real time. Many applications require topic derivation from this social media platform. These include, for example, disaster management, outbreak detection, situation awareness, surveillance, and market analysis. Deriving topics from Twitter is challenging due to the short content of the individual posts. The environment itself is also highly dynamic. This paper presents a review of recent methods proposed to derive topics from social media platform from algorithms to evaluations. With regard to algorithms, we classify them based on the features they exploit, such as content, social interactions, and temporal aspects. In terms of evaluations, we discuss the datasets and metrics generally used to evaluate the methods. Finally, we highlight the gaps in the research this far and the problems that remain to be addressed.",Low
Amir Bidaki,Automatic,Springer,http://link.springer.com/article/10.1007/s42045-019-00021-x,Research on sensitive content detection in social networks,2019,Type,CCF Transactions on Networking,Core,"Social networks have become the most effective information communication platform, which is deeply loved by netizens. However, the wide application of social networks also provides a cyberspace for the spread of sensitive content. Aiming at the detection difficulty caused by a large number of deformed and disguised sensitive words, we first identified sensitive words and sensitive deformation words. Secondly, considering the semantic relationship between the deformed words and the original words, we proposed the sensitive words fingerprint convergence method to associate the deformed words of sensitive words with the original words. Finally, for the text containing sensitive words, we established a convolutional neural network model based on multi-task learning, which combines sensitivity and emotional polarity to detect text content. The experimental results show that the sensitive content detection method proposed in this paper has a good effect.","Social networks
Multitask learning
Convolutional neural network
Sensitive content detection",0,,,,"Social networks have become the most effective information communication platform, which is deeply loved by netizens. However, the wide application of social networks also provides a cyberspace for the spread of sensitive content. Aiming at the detection difficulty caused by a large number of deformed and disguised sensitive words, we first identified sensitive words and sensitive deformation words. Secondly, considering the semantic relationship between the deformed words and the original words, we proposed the sensitive words fingerprint convergence method to associate the deformed words of sensitive words with the original words. Finally, for the text containing sensitive words, we established a convolutional neural network model based on multi-task learning, which combines sensitivity and emotional polarity to detect text content. The experimental results show that the sensitive content detection method proposed in this paper has a good effect.",Low
Amir Bidaki,Automatic,Springer,http://link.springer.com/article/10.1007/s10664-023-10346-3,"AIBugHunter: A Practical tool for predicting, classifying and repairing software vulnerabilities",2023,Type,Empirical Software Engineering,Core,"Many Machine Learning(ML)-based approaches have been proposed to automatically detect, localize, and repair software vulnerabilities. While ML-based methods are more effective than program analysis-based vulnerability analysis tools, few have been integrated into modern Integrated Development Environments (IDEs), hindering practical adoption. To bridge this critical gap, we propose in this article AIBUGHUNTER, a novel Machine Learning-based software vulnerability analysis tool for C/C++ languages that is integrated into the Visual Studio Code (VS Code) IDE. AIBUGHUNTER helps software developers to achieve real-time vulnerability detection, explanation, and repairs during programming. In particular, AIBUGHUNTER scans through developers’ source code to (1) locate vulnerabilities, (2) identify vulnerability types, (3) estimate vulnerability severity, and (4) suggest vulnerability repairs. We integrate our previous works (i.e., LineVul and VulRepair) to achieve vulnerability localization and repairs. In this article, we propose a novel multi-objective optimization (MOO)-based vulnerability classification approach and a transformer-based estimation approach to help AIBUGHUNTER accurately identify vulnerability types and estimate severity. Our empirical experiments on a large dataset consisting of 188K+ C/C++ functions confirm that our proposed approaches are more accurate than other state-of-the-art baseline methods for vulnerability classification and estimation. Furthermore, we conduct qualitative evaluations including a survey study and a user study to obtain software practitioners’ perceptions of our AIBUGHUNTER tool and assess the impact that AIBUGHUNTER may have on developers’ productivity in security aspects. Our survey study shows that our AIBUGHUNTER is perceived as useful where 90% of the participants consider adopting our AIBUGHUNTER during their software development. Last but not least, our user study shows that our AIBUGHUNTER can enhance developers’ productivity in combating cybersecurity issues during software development. AIBUGHUNTER is now publicly available in the Visual Studio Code marketplace.","Vulnerability prediction
Vulnerability localization
Vulnerability classification
Vulnerability repair",12 Citations,,,,"Many Machine Learning(ML)-based approaches have been proposed to automatically detect, localize, and repair software vulnerabilities. While ML-based methods are more effective than program analysis-based vulnerability analysis tools, few have been integrated into modern Integrated Development Environments (IDEs), hindering practical adoption. To bridge this critical gap, we propose in this article AIBUGHUNTER, a novel Machine Learning-based software vulnerability analysis tool for C/C++ languages that is integrated into the Visual Studio Code (VS Code) IDE. AIBUGHUNTER helps software developers to achieve real-time vulnerability detection, explanation, and repairs during programming. In particular, AIBUGHUNTER scans through developers’ source code to (1) locate vulnerabilities, (2) identify vulnerability types, (3) estimate vulnerability severity, and (4) suggest vulnerability repairs. We integrate our previous works (i.e., LineVul and VulRepair) to achieve vulnerability localization and repairs. In this article, we propose a novel multi-objective optimization (MOO)-based vulnerability classification approach and a transformer-based estimation approach to help AIBUGHUNTER accurately identify vulnerability types and estimate severity. Our empirical experiments on a large dataset consisting of 188K+ C/C++ functions confirm that our proposed approaches are more accurate than other state-of-the-art baseline methods for vulnerability classification and estimation. Furthermore, we conduct qualitative evaluations including a survey study and a user study to obtain software practitioners’ perceptions of our AIBUGHUNTER tool and assess the impact that AIBUGHUNTER may have on developers’ productivity in security aspects. Our survey study shows that our AIBUGHUNTER is perceived as useful where 90% of the participants consider adopting our AIBUGHUNTER during their software development. Last but not least, our user study shows that our AIBUGHUNTER can enhance developers’ productivity in combating cybersecurity issues during software development. AIBUGHUNTER is now publicly available in the Visual Studio Code marketplace.",Low
Amir Bidaki,Automatic,Google Scholar,https://ieeexplore.ieee.org/abstract/document/9668482/,Continual learning for anomaly based network intrusion detection,2022,Type,,Core,"To defend computing systems against ever-growing cyber attacks, Anomaly-based Network Intrusion Detection Systems (A-NIDS) have to evolve continuously. This requirement renders classical machine learning algorithms ineffective since they do not handle sequentially evolving tasks gracefully. Specifically, neural networks (NNs) are prone to catastrophic forgetting (CF) when trained on sequential data. Recent advances in addressing this drawback of NNs have resulted in a paradigm called Continual Learning (CL) which mitigates CF by introducing suitable constraints during the sequential training of these NNs. CL has been shown to be very effective in improving the performance of NNs on computer vision tasks. However, its application to the design of A-NIDS has not been explored. In this work, we evaluate the suitability of CL to address the challenges posed in A-NIDS design. Unlike computer vision datasets, network datasets suffer from the Class Imbalance (CI) problem, which makes the direct application of CL algorithms challenging. To evaluate the suitability of CL algorithms on network datasets, we study the impact of class imbalance on task ordering and its effect on the design of CL- based A-NIDS in the Class Incremental (CIL) and Domain Incremental (DIL) learning settings. Towards this end, we apply two popular CL algorithms viz. Elastic Weight Consolidation (EWC) and Gradient Episodic Memory (GEM) on two datasets viz., CICIDS and KDD Cup'99, and evaluate their performance. We found that CI affects task order sensitivity to a greater extent in the CIL setting when compared to the DIL setting. The performance of DIL setting can be further enhanced by incorporating experience forgetting aware memory population techniques, and we recommend this as a practical approach to building CL-based A-NIDS.
ieeexplore.ieee.org",,Cited by 33,,,,"To defend computing systems against ever-growing cyber attacks, Anomaly-based Network Intrusion Detection Systems (A-NIDS) have to evolve continuously. This requirement renders classical machine learning algorithms ineffective since they do not handle sequentially evolving tasks gracefully. Specifically, neural networks (NNs) are prone to catastrophic forgetting (CF) when trained on sequential data. Recent advances in addressing this drawback of NNs have resulted in a paradigm called Continual Learning (CL) which mitigates CF by introducing suitable constraints during the sequential training of these NNs. CL has been shown to be very effective in improving the performance of NNs on computer vision tasks. However, its application to the design of A-NIDS has not been explored. In this work, we evaluate the suitability of CL to address the challenges posed in A-NIDS design. Unlike computer vision datasets, network datasets suffer from the Class Imbalance (CI) problem, which makes the direct application of CL algorithms challenging. To evaluate the suitability of CL algorithms on network datasets, we study the impact of class imbalance on task ordering and its effect on the design of CL- based A-NIDS in the Class Incremental (CIL) and Domain Incremental (DIL) learning settings. Towards this end, we apply two popular CL algorithms viz. Elastic Weight Consolidation (EWC) and Gradient Episodic Memory (GEM) on two datasets viz., CICIDS and KDD Cup'99, and evaluate their performance. We found that CI affects task order sensitivity to a greater extent in the CIL setting when compared to the DIL setting. The performance of DIL setting can be further enhanced by incorporating experience forgetting aware memory population techniques, and we recommend this as a practical approach to building CL-based A-NIDS.
ieeexplore.ieee.org",High
Amir Bidaki,Automatic,Google Scholar,https://ieeexplore.ieee.org/abstract/document/9614112/,T-DFNN: an incremental learning algorithm for intrusion detection systems,2022,Type,,Core,"Machine learning has recently become a popular algorithm in building reliable intrusion detection systems (IDSs). However, most of the models are static and trained using datasets containing all targeted intrusions. If new intrusions emerge, these trained models must be retrained using old and new datasets to classify all intrusions accurately. In real-world situations, new threats continuously appear. Therefore, machine learning algorithms used for IDSs should have the ability to learn incrementally when these new intrusions emerge. To solve this issue, we propose T-DFNN. T-DFNN is an algorithm capable of learning new intrusions incrementally as they emerge. A T-DFNN model is composed of multiple deep feedforward neural network (DFNN) models connected in a tree-like structure. We examined our proposed algorithm using CICIDS2017, an open and widely used network intrusion dataset covering benign traffic and the most common network intrusions. The experimental results showed that the T-DFNN algorithm can incrementally learn new intrusions and reduce the catastrophic forgetting effect. The macro average of the F1-score of the T-DFNN model was over 0.85 for every retraining process. In addition, our proposed T-DFNN model has some advantages in several aspects compared to other models. Compared to the DFNN and Hoeffding tree models trained with a dataset containing only the latest targeted intrusions, our proposed T-DFNN model has higher F1-scores. Moreover, our proposed T-DFNN model has significantly shorter training times than a DFNN model trained using a dataset containing all targeted intrusions. Even though several factors can affect the duration of the training process, the T-DFNN algorithm shows promising results in solving the problem of ever-evolving network intrusion variants.",,Cited by 32,,,,"Machine learning has recently become a popular algorithm in building reliable intrusion detection systems (IDSs). However, most of the models are static and trained using datasets containing all targeted intrusions. If new intrusions emerge, these trained models must be retrained using old and new datasets to classify all intrusions accurately. In real-world situations, new threats continuously appear. Therefore, machine learning algorithms used for IDSs should have the ability to learn incrementally when these new intrusions emerge. To solve this issue, we propose T-DFNN. T-DFNN is an algorithm capable of learning new intrusions incrementally as they emerge. A T-DFNN model is composed of multiple deep feedforward neural network (DFNN) models connected in a tree-like structure. We examined our proposed algorithm using CICIDS2017, an open and widely used network intrusion dataset covering benign traffic and the most common network intrusions. The experimental results showed that the T-DFNN algorithm can incrementally learn new intrusions and reduce the catastrophic forgetting effect. The macro average of the F1-score of the T-DFNN model was over 0.85 for every retraining process. In addition, our proposed T-DFNN model has some advantages in several aspects compared to other models. Compared to the DFNN and Hoeffding tree models trained with a dataset containing only the latest targeted intrusions, our proposed T-DFNN model has higher F1-scores. Moreover, our proposed T-DFNN model has significantly shorter training times than a DFNN model trained using a dataset containing all targeted intrusions. Even though several factors can affect the duration of the training process, the T-DFNN algorithm shows promising results in solving the problem of ever-evolving network intrusion variants.",Medium
Amir Bidaki,Automatic,Google Scholar,https://www.sciencedirect.com/science/article/pii/S0952197624013010,… cybersecurity frontiers: A comprehensive survey on concept drift and feature dynamics aware machine and deep learning in intrusion detection systems,2021,Type,,Core,"Intrusion Detection Systems (IDS) have become pivotal in safeguarding information systems against evolving threats. Concurrently, Concept Drift presents a significant challenge in machine learning, affecting the adaptability and accuracy of predictive models in dynamic environments. Understanding the synergy between IDS and Concept Drift is crucial for developing robust security systems. The motivation behind this survey is driven by the emerging complexities in cyber threats and the dynamic nature of data streams, which necessitate advanced IDS capable of adapting to Concept and Feature Drift. Our analysis reveals a glaring omission in the existing literature—the integration of Concept Drift and Feature Drift within IDS. Most studies have focused on Concept Drift in a general context or on IDS but have yet to comprehensively consider the implications of data dynamics. This oversight has led to a fragmented understanding and suboptimal approaches to tackling modern cyber threats. To address this, we propose a comprehensive review that delves into the role of machine learning in IDS, explicitly focusing on Concept and Feature Drift. We have proposed a framework that includes all the necessary components for a drift-aware IDS. The framework incorporates dynamic feature selection, adaptive learning algorithms, and continuous monitoring techniques to handle Concept Drift and Feature Drift effectively. The survey highlights state-of-the-art methodologies and current challenges in integrating these concepts. The methodology involves an exhaustive analysis of published works from 2019 to 2024, comparing and contrasting various models and approaches. This includes a detailed examination of Concept Drift-aware IDS methods, dynamic feature selection techniques, and the impact of high dimensionality in IDS. These quantitative improvements underscore the necessity for developing adaptive and resilient IDS. The survey uncovers under-represented areas in current research, paving the way for future investigations. By highlighting these gaps and providing comparative data, the survey sets a clear direction for upcoming research efforts to foster the development of more dynamic and adaptable IDS solutions. The quantitative experimental evaluation of the proposed framework is planned to be conducted in a future article, where we will assess its effectiveness and performance in real-world scenarios.
Elsevier",,Cited by 5,,,,"Intrusion Detection Systems (IDS) have become pivotal in safeguarding information systems against evolving threats. Concurrently, Concept Drift presents a significant challenge in machine learning, affecting the adaptability and accuracy of predictive models in dynamic environments. Understanding the synergy between IDS and Concept Drift is crucial for developing robust security systems. The motivation behind this survey is driven by the emerging complexities in cyber threats and the dynamic nature of data streams, which necessitate advanced IDS capable of adapting to Concept and Feature Drift. Our analysis reveals a glaring omission in the existing literature—the integration of Concept Drift and Feature Drift within IDS. Most studies have focused on Concept Drift in a general context or on IDS but have yet to comprehensively consider the implications of data dynamics. This oversight has led to a fragmented understanding and suboptimal approaches to tackling modern cyber threats. To address this, we propose a comprehensive review that delves into the role of machine learning in IDS, explicitly focusing on Concept and Feature Drift. We have proposed a framework that includes all the necessary components for a drift-aware IDS. The framework incorporates dynamic feature selection, adaptive learning algorithms, and continuous monitoring techniques to handle Concept Drift and Feature Drift effectively. The survey highlights state-of-the-art methodologies and current challenges in integrating these concepts. The methodology involves an exhaustive analysis of published works from 2019 to 2024, comparing and contrasting various models and approaches. This includes a detailed examination of Concept Drift-aware IDS methods, dynamic feature selection techniques, and the impact of high dimensionality in IDS. These quantitative improvements underscore the necessity for developing adaptive and resilient IDS. The survey uncovers under-represented areas in current research, paving the way for future investigations. By highlighting these gaps and providing comparative data, the survey sets a clear direction for upcoming research efforts to foster the development of more dynamic and adaptable IDS solutions. The quantitative experimental evaluation of the proposed framework is planned to be conducted in a future article, where we will assess its effectiveness and performance in real-world scenarios.
Elsevier",Medium
Amir Bidaki,Automatic,Google Scholar,https://ieeexplore.ieee.org/abstract/document/8763842/,A novel online incremental learning intrusion prevention system,2024,Type,,Core,"Attack vectors are continuously evolving in order to evade Intrusion Detection systems. Internet of Things (IoT) environments, while beneficial for the IT ecosystem, suffer from inherent hardware limitations, which restrict their ability to implement comprehensive security measures and increase their exposure to vulnerability attacks. This paper proposes a novel Network Intrusion Prevention System that utilises a Self-Organizing Incremental Neural Network along with a Support Vector Machine. Due to its structure, the proposed system provides a security solution that does not rely on signatures or rules and is capable to mitigate known and unknown attacks in real-time with high accuracy. Based on our experimental results with the NSL KDD dataset, the proposed framework can achieve on-line updated incremental learning, making it suitable for efficient and scalable industrial applications.
ieeexplore.ieee.org",,Cited by 71,,,,"Attack vectors are continuously evolving in order to evade Intrusion Detection systems. Internet of Things (IoT) environments, while beneficial for the IT ecosystem, suffer from inherent hardware limitations, which restrict their ability to implement comprehensive security measures and increase their exposure to vulnerability attacks. This paper proposes a novel Network Intrusion Prevention System that utilises a Self-Organizing Incremental Neural Network along with a Support Vector Machine. Due to its structure, the proposed system provides a security solution that does not rely on signatures or rules and is capable to mitigate known and unknown attacks in real-time with high accuracy. Based on our experimental results with the NSL KDD dataset, the proposed framework can achieve on-line updated incremental learning, making it suitable for efficient and scalable industrial applications.
ieeexplore.ieee.org",Medium
Amir Bidaki,Automatic,Google Scholar,https://dl.acm.org/doi/abs/10.1145/3689935.3690396,Adaptive Network Intrusion Detection Systems Against Performance Degradation via Model Agnostic Meta-Learning,2019,Type,,Core,"Network Intrusion Detection Systems (NIDS) are essential for identifying and mitigating cyber threats in dynamic network environments. However, maintaining high performance over time is challenging due to factors such as initial model limitations, data poisoning attacks, and the influx of low-quality data. Continual learning offers a potential solution, but the risk of performance degradation remains significant. This work proposes a novel approach to enhance the robustness and adaptability of NIDS through the integration of Model Agnostic Meta-Learning (MAML) and Open-Set Recognition (OSR). OSR allows the system to identify and handle previously unseen attack patterns, while MAML facilitates rapid model adaptation to new tasks with minimal additional data. By detecting performance degradation and employing MAML for model repair, our approach aims to maintain and improve NIDS performance over time. Our empirical feasibility evaluations demonstrate the effectiveness of our method in addressing the challenges of continual learning, providing a resilient and adaptive solution for cybersecurity applications.
ACM Digital Library",,Cited by 1,,,,"Network Intrusion Detection Systems (NIDS) are essential for identifying and mitigating cyber threats in dynamic network environments. However, maintaining high performance over time is challenging due to factors such as initial model limitations, data poisoning attacks, and the influx of low-quality data. Continual learning offers a potential solution, but the risk of performance degradation remains significant. This work proposes a novel approach to enhance the robustness and adaptability of NIDS through the integration of Model Agnostic Meta-Learning (MAML) and Open-Set Recognition (OSR). OSR allows the system to identify and handle previously unseen attack patterns, while MAML facilitates rapid model adaptation to new tasks with minimal additional data. By detecting performance degradation and employing MAML for model repair, our approach aims to maintain and improve NIDS performance over time. Our empirical feasibility evaluations demonstrate the effectiveness of our method in addressing the challenges of continual learning, providing a resilient and adaptive solution for cybersecurity applications.
ACM Digital Library",High
Amir Bidaki,Automatic,Google Scholar,https://ieeexplore.ieee.org/abstract/document/10143925/,Federated continuous learning based on stacked broad learning system assisted by digital twin networks: An incremental learning approach for intrusion detection in …,2019,Type,,Core,"The edge of the Internet of Things (IoT), which consists of unmanned aerial vehicles (UAVs), is vulnerable to network intrusion because software and wireless connections are used extensively in the IoT. Designing an efficient intrusion detection system (IDS) model is imperative. However, when creating IDS models with distributed data collected by UAVs, it is necessary to take precautions to protect the data’s security and privacy. Furthermore, most of the IDS models are focused on one-time learning but not on continuous learning. To this end, we propose a federated continuous learning framework with a stacked broad learning system (FCL-SBLS) based on the digital twin network (DTN), which can learn and train the IDS model on new data quickly and continuously. In order to improve the efficiency and quality of the IDS model when training and aggregation, we employ an asynchronous federated learning (FL) architecture, and a deep deterministic policy gradient (DDPG)-based UAV selection scheme assisted by DTN is proposed to help the global IDS model aggregation. The presented algorithm is validated using the CIC-IDS2017 data set, and the simulation results reveal that our algorithm achieves higher efficiency and accuracy than the existing FL scheme.
ieeexplore.ieee.org",,Cited by 25,,,,"The edge of the Internet of Things (IoT), which consists of unmanned aerial vehicles (UAVs), is vulnerable to network intrusion because software and wireless connections are used extensively in the IoT. Designing an efficient intrusion detection system (IDS) model is imperative. However, when creating IDS models with distributed data collected by UAVs, it is necessary to take precautions to protect the data’s security and privacy. Furthermore, most of the IDS models are focused on one-time learning but not on continuous learning. To this end, we propose a federated continuous learning framework with a stacked broad learning system (FCL-SBLS) based on the digital twin network (DTN), which can learn and train the IDS model on new data quickly and continuously. In order to improve the efficiency and quality of the IDS model when training and aggregation, we employ an asynchronous federated learning (FL) architecture, and a deep deterministic policy gradient (DDPG)-based UAV selection scheme assisted by DTN is proposed to help the global IDS model aggregation. The presented algorithm is validated using the CIC-IDS2017 data set, and the simulation results reveal that our algorithm achieves higher efficiency and accuracy than the existing FL scheme.
ieeexplore.ieee.org",High
Amir Bidaki,Automatic,Google Scholar,https://www.mdpi.com/2076-3417/11/4/1674,Intelligent cyber attack detection and classification for network-based intrusion detection systems,2023,Type,,Core,"With the latest advances in information and communication technologies, greater amounts of sensitive user and corporate information are shared continuously across the network, making it susceptible to an attack that can compromise data confidentiality, integrity, and availability. Intrusion Detection Systems (IDS) are important security mechanisms that can perform the timely detection of malicious events through the inspection of network traffic or host-based logs. Many machine learning techniques have proven to be successful at conducting anomaly detection throughout the years, but only a few considered the sequential nature of data. This work proposes a sequential approach and evaluates the performance of a Random Forest (RF), a Multi-Layer Perceptron (MLP), and a Long-Short Term Memory (LSTM) on the CIDDS-001 dataset. The resulting performance measures of this particular approach are compared with the ones obtained from a more traditional one, which only considers individual flow information, in order to determine which methodology best suits the concerned scenario. The experimental outcomes suggest that anomaly detection can be better addressed from a sequential perspective. The LSTM is a highly reliable model for acquiring sequential patterns in network traffic data, achieving an accuracy of 99.94% and an f1-score of 91.66%.
MDPI",,Cited by 112,,,,"With the latest advances in information and communication technologies, greater amounts of sensitive user and corporate information are shared continuously across the network, making it susceptible to an attack that can compromise data confidentiality, integrity, and availability. Intrusion Detection Systems (IDS) are important security mechanisms that can perform the timely detection of malicious events through the inspection of network traffic or host-based logs. Many machine learning techniques have proven to be successful at conducting anomaly detection throughout the years, but only a few considered the sequential nature of data. This work proposes a sequential approach and evaluates the performance of a Random Forest (RF), a Multi-Layer Perceptron (MLP), and a Long-Short Term Memory (LSTM) on the CIDDS-001 dataset. The resulting performance measures of this particular approach are compared with the ones obtained from a more traditional one, which only considers individual flow information, in order to determine which methodology best suits the concerned scenario. The experimental outcomes suggest that anomaly detection can be better addressed from a sequential perspective. The LSTM is a highly reliable model for acquiring sequential patterns in network traffic data, achieving an accuracy of 99.94% and an f1-score of 91.66%.
MDPI",Medium
Amir Bidaki,Automatic,Google Scholar,https://ieeexplore.ieee.org/abstract/document/7454287/,Real-time anomaly-based distributed intrusion detection systems for advanced Metering Infrastructure utilizing stream data mining,2018,Type,,Core,"The advanced Metering Infrastructure (AMI) is one of the core components of smart grids' architecture. As AMI components are connected through mesh networks in a distributed mechanism, new vulnerabilities will be exploited by grid's attackers who intentionally interfere with network's communication system and steal customer data. As a result, identifying distributed security solutions to maintain the confidentiality, integrity, and availability of AMI devices' traffic is an essential requirement that needs to be taken into account. This paper proposes a real-time distributed intrusion detection system (DIDS) for the AMI infrastructure that utilizes stream data mining techniques and a multi-layer implementation approach. Using unsupervised online clustering techniques, the anomaly-based DIDS monitors the data flow in the AMI and distinguish if there are anomalous traffics. By comparing between online and offline clustering techniques, the experimental results showed that online clustering “Mini-Batch K-means” were successfully able to suit the architecture requirements by giving high detection rate and low false positive rates.
ieeexplore.ieee.org",,Cited by 51,,,,"The advanced Metering Infrastructure (AMI) is one of the core components of smart grids' architecture. As AMI components are connected through mesh networks in a distributed mechanism, new vulnerabilities will be exploited by grid's attackers who intentionally interfere with network's communication system and steal customer data. As a result, identifying distributed security solutions to maintain the confidentiality, integrity, and availability of AMI devices' traffic is an essential requirement that needs to be taken into account. This paper proposes a real-time distributed intrusion detection system (DIDS) for the AMI infrastructure that utilizes stream data mining techniques and a multi-layer implementation approach. Using unsupervised online clustering techniques, the anomaly-based DIDS monitors the data flow in the AMI and distinguish if there are anomalous traffics. By comparing between online and offline clustering techniques, the experimental results showed that online clustering “Mini-Batch K-means” were successfully able to suit the architecture requirements by giving high detection rate and low false positive rates.
ieeexplore.ieee.org",Medium
Amir Bidaki,Automatic,Google Scholar,https://www.sciencedirect.com/science/article/pii/S0167404824002876,Ais-nids: An intelligent and self-sustaining network intrusion detection system,2021,Type,,Core,"The ever-evolving landscape of network security is continually molded by the dynamic evolution of attack vectors and the relentless emergence of new, highly sophisticated attacks. Attackers consistently employ increasingly advanced techniques, rendering their actions elusive and formidable. In response to this ever-growing threat, the demand for intelligent and autonomous security systems has reached paramount importance. In this paper, we introduce AIS-NIDS (An Intelligent and Self-Sustaining Network Intrusion Detection System), an innovative network intrusion detection system (NIDS) that delves into the realm of packet-level analysis. By doing so, AIS-NIDS is capable of identifying threats with intricate payload-level details, a level of granularity that traditional NIDS relying solely on flow-level data may overlook. The defining feature of AIS-NIDS is its dual functionality, driven by autonomous and intelligent learning. It not only autonomously distinguishes between benign and unknown attacks using machine learning models but also conducts incremental learning, adapting to new attack classes. In essence, AIS-NIDS bridges the gap between traditional NIDS and the next generation of intelligent systems, endowing the system with the capacity for independent decision-making and real-time adaptability in the face of evolving threats. Our extensive experiments stand as a testament to AIS-NIDS’ ability to efficiently manage and identify new attack classes, thus establishing it as a valuable asset in the reinforcement of network infrastructures. Through our experimentation, we have demonstrated the practical efficacy of the proposed approach by simulating a real-world scenario in which certain attack classes are unknown. AIS-NIDS not only effectively identified these unknown threats but also autonomously learned to recognize them as it encountered them, enhancing the system’s capabilities for future encounters with these threats.
Elsevier",,Cited by 5,,,,"The ever-evolving landscape of network security is continually molded by the dynamic evolution of attack vectors and the relentless emergence of new, highly sophisticated attacks. Attackers consistently employ increasingly advanced techniques, rendering their actions elusive and formidable. In response to this ever-growing threat, the demand for intelligent and autonomous security systems has reached paramount importance. In this paper, we introduce AIS-NIDS (An Intelligent and Self-Sustaining Network Intrusion Detection System), an innovative network intrusion detection system (NIDS) that delves into the realm of packet-level analysis. By doing so, AIS-NIDS is capable of identifying threats with intricate payload-level details, a level of granularity that traditional NIDS relying solely on flow-level data may overlook. The defining feature of AIS-NIDS is its dual functionality, driven by autonomous and intelligent learning. It not only autonomously distinguishes between benign and unknown attacks using machine learning models but also conducts incremental learning, adapting to new attack classes. In essence, AIS-NIDS bridges the gap between traditional NIDS and the next generation of intelligent systems, endowing the system with the capacity for independent decision-making and real-time adaptability in the face of evolving threats. Our extensive experiments stand as a testament to AIS-NIDS’ ability to efficiently manage and identify new attack classes, thus establishing it as a valuable asset in the reinforcement of network infrastructures. Through our experimentation, we have demonstrated the practical efficacy of the proposed approach by simulating a real-world scenario in which certain attack classes are unknown. AIS-NIDS not only effectively identified these unknown threats but also autonomously learned to recognize them as it encountered them, enhancing the system’s capabilities for future encounters with these threats.
Elsevier",Medium
Amir Bidaki,Automatic,Google Scholar,https://ieeexplore.ieee.org/abstract/document/5640501/,Energy efficient sensor network security using Stream cipher mode of operation,2015,Type,,Core,"Sometimes the information, which is transferred during the communication in between sensor node, is very much confidential which is needed to be secure. Since sensor nodes are resource constrained and run on battery, energy consumption should be low to make it operate for many days. For securing the information various symmetric key encryption algorithms like DES, BLOWFISH, RC4 etc., are used. Our aim in this paper is to apply security in sensor network in such a way that it provide confidentiality with authentication using Stream cipher modes of operation, so that the energy consumption will minimizes at the sensor node and the life time of sensor node will increase.
ieeexplore.ieee.org",,Cited by 10,,,,"Sometimes the information, which is transferred during the communication in between sensor node, is very much confidential which is needed to be secure. Since sensor nodes are resource constrained and run on battery, energy consumption should be low to make it operate for many days. For securing the information various symmetric key encryption algorithms like DES, BLOWFISH, RC4 etc., are used. Our aim in this paper is to apply security in sensor network in such a way that it provide confidentiality with authentication using Stream cipher modes of operation, so that the energy consumption will minimizes at the sensor node and the life time of sensor node will increase.
ieeexplore.ieee.org",Low
Amir Bidaki,Automatic,Google Scholar,https://ieeexplore.ieee.org/abstract/document/5437730/,Design and analysis of stream cipher for network security,2024,Type,,Core,"This paper mainly analysis and describe the design issue of stream ciphers in Network security as the streams are widely used to protecting the privacy of digital information. A variety of attacks against stream cipher exist;(algebraic and so on). These attacks have been very successful against a variety of stream ciphers. So in this paper efforts have been done to design and analyze stream ciphers. The main contribution is to design new stream ciphers through analysis of the algebraic immunity of Boolean functions and S-Boxes. In this paper, the cryptographic properties of non-linear transformation have been used for designing of stream ciphers Many LFSR (Linear feedback Shift Register) based stream ciphers use non-linear Boolean function to destroy the linearity of the LFSR(s) output. Many of these designs have been broken by algebraic attacks. Here we analyze a popular and cryptographically significant class of non-linear Boolean functions for their resistance to algebraic attacks.
ieeexplore.ieee.org",,Cited by 33,,,,"This paper mainly analysis and describe the design issue of stream ciphers in Network security as the streams are widely used to protecting the privacy of digital information. A variety of attacks against stream cipher exist;(algebraic and so on). These attacks have been very successful against a variety of stream ciphers. So in this paper efforts have been done to design and analyze stream ciphers. The main contribution is to design new stream ciphers through analysis of the algebraic immunity of Boolean functions and S-Boxes. In this paper, the cryptographic properties of non-linear transformation have been used for designing of stream ciphers Many LFSR (Linear feedback Shift Register) based stream ciphers use non-linear Boolean function to destroy the linearity of the LFSR(s) output. Many of these designs have been broken by algebraic attacks. Here we analyze a popular and cryptographically significant class of non-linear Boolean functions for their resistance to algebraic attacks.
ieeexplore.ieee.org",Low
Amir Bidaki,Automatic,Google Scholar,https://www.mdpi.com/2078-2489/11/6/315,Ensemble-based online machine learning algorithms for network intrusion detection systems using streaming data,2010,Type,,Core,"As new cyberattacks are launched against systems and networks on a daily basis, the ability for network intrusion detection systems to operate efficiently in the big data era has become critically important, particularly as more low-power Internet-of-Things (IoT) devices enter the market. This has motivated research in applying machine learning algorithms that can operate on streams of data, trained online or “live” on only a small amount of data kept in memory at a time, as opposed to the more classical approaches that are trained solely offline on all of the data at once. In this context, one important concept from machine learning for improving detection performance is the idea of “ensembles”, where a collection of machine learning algorithms are combined to compensate for their individual limitations and produce an overall superior algorithm. Unfortunately, existing research lacks proper performance comparison between homogeneous and heterogeneous online ensembles. Hence, this paper investigates several homogeneous and heterogeneous ensembles, proposes three novel online heterogeneous ensembles for intrusion detection, and compares their performance accuracy, run-time complexity, and response to concept drifts. Out of the proposed novel online ensembles, the heterogeneous ensemble consisting of an adaptive random forest of Hoeffding Trees combined with a Hoeffding Adaptive Tree performed the best, by dealing with concept drift in the most effective way. While this scheme is less accurate than a larger size adaptive random forest, it offered a marginally better run-time, which is beneficial for online training.
MDPI",,Cited by 39,,,,"As new cyberattacks are launched against systems and networks on a daily basis, the ability for network intrusion detection systems to operate efficiently in the big data era has become critically important, particularly as more low-power Internet-of-Things (IoT) devices enter the market. This has motivated research in applying machine learning algorithms that can operate on streams of data, trained online or “live” on only a small amount of data kept in memory at a time, as opposed to the more classical approaches that are trained solely offline on all of the data at once. In this context, one important concept from machine learning for improving detection performance is the idea of “ensembles”, where a collection of machine learning algorithms are combined to compensate for their individual limitations and produce an overall superior algorithm. Unfortunately, existing research lacks proper performance comparison between homogeneous and heterogeneous online ensembles. Hence, this paper investigates several homogeneous and heterogeneous ensembles, proposes three novel online heterogeneous ensembles for intrusion detection, and compares their performance accuracy, run-time complexity, and response to concept drifts. Out of the proposed novel online ensembles, the heterogeneous ensemble consisting of an adaptive random forest of Hoeffding Trees combined with a Hoeffding Adaptive Tree performed the best, by dealing with concept drift in the most effective way. While this scheme is less accurate than a larger size adaptive random forest, it offered a marginally better run-time, which is beneficial for online training.
MDPI",Medium
Amir Bidaki,Automatic,Google Scholar,https://ieeexplore.ieee.org/abstract/document/9862833/,AB-HT: An ensemble incremental learning algorithm for network intrusion detection systems,2010,Type,,Core,"Most machine learning models used in network intrusion detection system (IDS) studies are batch models which require all targeted intrusions to be present in the training data. This approach is slow because computer networks produce massive amounts of data. Furthermore, new network intrusion variants continuously emerge. Retraining the model using these extensive and evolving data takes time and resources. This study proposes AB-HT: an ensemble incremental learning algorithm for IDSs. AB-HT utilizes incremental Adaptive Boosting (AdaBoost) and Hoeffding Tree algorithms. AB-HT model could detect new intrusions without retraining the model using old training data. Thus, it could reduce the computational resources needed to retrain the model while maintaining the model’s performance. We compared it to an AdaBoost-Decision Tree model, a batch learning model, to analyze the effectiveness of the incremental learning approach. Then we compared it to other incremental learning models, the Hoeffding Tree (HT) and Hoeffding Anytime Tree (HATT) models. The experimental results showed that the proposed incremental model had shorter training times than the AdaBoost-Decision Tree model in the long run. Also, on average, the AB-HT model has 18% higher F1-score values than the HT and HATT models. These advantages show that the AB-HT algorithm has promising potential to be used in the IDS field.
ieeexplore.ieee.org",,Cited by 7,,,,"Most machine learning models used in network intrusion detection system (IDS) studies are batch models which require all targeted intrusions to be present in the training data. This approach is slow because computer networks produce massive amounts of data. Furthermore, new network intrusion variants continuously emerge. Retraining the model using these extensive and evolving data takes time and resources. This study proposes AB-HT: an ensemble incremental learning algorithm for IDSs. AB-HT utilizes incremental Adaptive Boosting (AdaBoost) and Hoeffding Tree algorithms. AB-HT model could detect new intrusions without retraining the model using old training data. Thus, it could reduce the computational resources needed to retrain the model while maintaining the model’s performance. We compared it to an AdaBoost-Decision Tree model, a batch learning model, to analyze the effectiveness of the incremental learning approach. Then we compared it to other incremental learning models, the Hoeffding Tree (HT) and Hoeffding Anytime Tree (HATT) models. The experimental results showed that the proposed incremental model had shorter training times than the AdaBoost-Decision Tree model in the long run. Also, on average, the AB-HT model has 18% higher F1-score values than the HT and HATT models. These advantages show that the AB-HT algorithm has promising potential to be used in the IDS field.
ieeexplore.ieee.org",Medium
Amir Bidaki,Automatic,Google Scholar,https://www.sciencedirect.com/science/article/pii/S1389128618306881,The hybrid technique for DDoS detection with supervised learning algorithms,2020,Type,,Core,"Distributed denial of service (DDoS) is still one of the main threats of the online services. Attackers are able to run DDoS with simple steps and high efficiency in order to prevent or slow down users' access to services. In this paper, we propose a novel hybrid framework based on data stream approach for detecting DDoS attack with incremental learning. We use a technique which divides the computational load between client and proxy sides based on their resource to organize the task with high speed. Client side contains three steps, first is the data collecting of the client system, second is the feature extraction based on forward feature selection for each algorithm, and the divergence test. Consequently, if divergence got bigger than a threshold, the attack is detected otherwise data processed to the proxy side. We use the naïve Bayes, random forest, decision tree, multilayer perceptron (MLP), and k-nearest neighbors (K-NN) on the proxy side to make better results. Different attacks have their specific behavior, and because of different selected features for each algorithm, the appropriate performance for detecting attacks and more ability to distinguish new attack types is achieved. The results show that the random forest produces better results among other mentioned algorithms.
Elsevier",,Cited by 146,,,,"Distributed denial of service (DDoS) is still one of the main threats of the online services. Attackers are able to run DDoS with simple steps and high efficiency in order to prevent or slow down users' access to services. In this paper, we propose a novel hybrid framework based on data stream approach for detecting DDoS attack with incremental learning. We use a technique which divides the computational load between client and proxy sides based on their resource to organize the task with high speed. Client side contains three steps, first is the data collecting of the client system, second is the feature extraction based on forward feature selection for each algorithm, and the divergence test. Consequently, if divergence got bigger than a threshold, the attack is detected otherwise data processed to the proxy side. We use the naïve Bayes, random forest, decision tree, multilayer perceptron (MLP), and k-nearest neighbors (K-NN) on the proxy side to make better results. Different attacks have their specific behavior, and because of different selected features for each algorithm, the appropriate performance for detecting attacks and more ability to distinguish new attack types is achieved. The results show that the random forest produces better results among other mentioned algorithms.
Elsevier",Medium
Amir Bidaki,Automatic,Google Scholar,https://pdfs.semanticscholar.org/2a91/5eaf1ad1103df1f336117ff78f333904ef7d.pdf,Big Data in intrusion detection systems and intrusion prevention systems,2022,Type,,Core,"This paper introduces network attacks, intrusion detection systems, intrusion prevention systems, and intrusion detection methods including signature-based detection and anomaly-based detection. Intrusion detection/prevention system (ID/PS) methods are compared. Some data mining and machine learning methods and their applications in intrusion detection are introduced. Big data in intrusion detection systems and Big Data analytics for huge volume of data, heterogeneous features, and real-time stream processing are presented. Challenges of intrusion detection systems and challenges posed by stream processing of big data in the systems are also discussed.
pdfs.semanticscholar.org",,Cited by 32,,,,"This paper introduces network attacks, intrusion detection systems, intrusion prevention systems, and intrusion detection methods including signature-based detection and anomaly-based detection. Intrusion detection/prevention system (ID/PS) methods are compared. Some data mining and machine learning methods and their applications in intrusion detection are introduced. Big data in intrusion detection systems and Big Data analytics for huge volume of data, heterogeneous features, and real-time stream processing are presented. Challenges of intrusion detection systems and challenges posed by stream processing of big data in the systems are also discussed.
pdfs.semanticscholar.org",Low
Amir Bidaki,Automatic,Google Scholar,https://dl.acm.org/doi/abs/10.1145/940071.940084,Designing and implementing a family of intrusion detection systems,2019,Type,,Core,"Intrusion detection systems are distributed applications that analyze the events in a networked system to identify malicious behavior. The analysis is performed using a number of attack models (or signatures) that are matched against a specific event stream. Intrusion detection systems may operate in heterogeneous environments, analyzing different types of event streams. Currently, intrusion detection systems and the corresponding attack modeling languages are developed following an ad hoc approach to match the characteristics of specific target environments. As the number of systems that have to be protected increases, this approach results in increased development effort. To overcome this limitation, we developed a framework, called STAT, that supports the development of new intrusion detection functionality in a modular fashion. The STAT framework can be extended following a well-defined process to implement intrusion detection systems tailored to specific environments, platforms, and event streams. The STAT framework is novel in the fact that the extension process also includes the extension of the attack modeling language. The resulting intrusion detection systems represent a software family whose members share common attack modeling features and the ability to reconfigure their behavior dynamically.",,Cited by 118,,,,"Intrusion detection systems are distributed applications that analyze the events in a networked system to identify malicious behavior. The analysis is performed using a number of attack models (or signatures) that are matched against a specific event stream. Intrusion detection systems may operate in heterogeneous environments, analyzing different types of event streams. Currently, intrusion detection systems and the corresponding attack modeling languages are developed following an ad hoc approach to match the characteristics of specific target environments. As the number of systems that have to be protected increases, this approach results in increased development effort. To overcome this limitation, we developed a framework, called STAT, that supports the development of new intrusion detection functionality in a modular fashion. The STAT framework can be extended following a well-defined process to implement intrusion detection systems tailored to specific environments, platforms, and event streams. The STAT framework is novel in the fact that the extension process also includes the extension of the attack modeling language. The resulting intrusion detection systems represent a software family whose members share common attack modeling features and the ability to reconfigure their behavior dynamically.",Low
Amir Bidaki,Automatic,Google Scholar,https://ieeexplore.ieee.org/abstract/document/1254342/,An experience developing an IDS stimulator for the black-box testing of network intrusion detection systems,2017,Type,,Core,"Signature-based intrusion detection systems use a set of attack descriptions to analyze event streams, looking for evidence of malicious behavior. If the signatures are expressed in a well-defined language, it is possible to analyze the attack signatures and automatically generate events or series of events that conform to the attack descriptions. This approach has been used in tools whose goal is to force intrusion detection systems to generate a large number of detection alerts. The resulting ""alert storm"" is used to desensitize intrusion detection system administrators and hide attacks in the event stream. We apply a similar technique to perform testing of intrusion detection systems. Signatures from one intrusion detection system are used as input to an event stream generator that produces randomized synthetic events that match the input signatures. The resulting event stream is then fed to a number of different intrusion detection systems and the results are analyzed. This paper presents the general testing approach and describes the first prototype of a tool, called Mucus, that automatically generates network traffic using the signatures of the Snort network-based intrusion detection system. The paper describes preliminary cross-testing experiments with both an open-source and a commercial tool and reports the results. An evasion attack that was discovered as a result of analyzing the test results is also presented.
ieeexplore.ieee.org",,Cited by 135,,,,"Signature-based intrusion detection systems use a set of attack descriptions to analyze event streams, looking for evidence of malicious behavior. If the signatures are expressed in a well-defined language, it is possible to analyze the attack signatures and automatically generate events or series of events that conform to the attack descriptions. This approach has been used in tools whose goal is to force intrusion detection systems to generate a large number of detection alerts. The resulting ""alert storm"" is used to desensitize intrusion detection system administrators and hide attacks in the event stream. We apply a similar technique to perform testing of intrusion detection systems. Signatures from one intrusion detection system are used as input to an event stream generator that produces randomized synthetic events that match the input signatures. The resulting event stream is then fed to a number of different intrusion detection systems and the results are analyzed. This paper presents the general testing approach and describes the first prototype of a tool, called Mucus, that automatically generates network traffic using the signatures of the Snort network-based intrusion detection system. The paper describes preliminary cross-testing experiments with both an open-source and a commercial tool and reports the results. An evasion attack that was discovered as a result of analyzing the test results is also presented.
ieeexplore.ieee.org",Low
Amir Bidaki,Automatic,Google Scholar,https://onlinelibrary.wiley.com/doi/abs/10.1155/2017/5202836,Multiple‐Features‐Based Semisupervised Clustering DDoS Detection Method,2003,Type,,Core,"DDoS attack stream from different agent host converged at victim host will become very large, which will lead to system halt or network congestion. Therefore, it is necessary to propose an effective method to detect the DDoS attack behavior from the massive data stream. In order to solve the problem that large numbers of labeled data are not provided in supervised learning method, and the relatively low detection accuracy and convergence speed of unsupervised k‐means algorithm, this paper presents a semisupervised clustering detection method using multiple features. In this detection method, we firstly select three features according to the characteristics of DDoS attacks to form detection feature vector. Then, Multiple‐Features‐Based Constrained‐K‐Means (MF‐CKM) algorithm is proposed based on semisupervised clustering. Finally, using MIT Laboratory Scenario (DDoS) 1.0 data set, we verify that the proposed method can improve the convergence speed and accuracy of the algorithm under the condition of using a small amount of labeled data sets.
Wiley Online Library",,Cited by 28,,,,"DDoS attack stream from different agent host converged at victim host will become very large, which will lead to system halt or network congestion. Therefore, it is necessary to propose an effective method to detect the DDoS attack behavior from the massive data stream. In order to solve the problem that large numbers of labeled data are not provided in supervised learning method, and the relatively low detection accuracy and convergence speed of unsupervised k‐means algorithm, this paper presents a semisupervised clustering detection method using multiple features. In this detection method, we firstly select three features according to the characteristics of DDoS attacks to form detection feature vector. Then, Multiple‐Features‐Based Constrained‐K‐Means (MF‐CKM) algorithm is proposed based on semisupervised clustering. Finally, using MIT Laboratory Scenario (DDoS) 1.0 data set, we verify that the proposed method can improve the convergence speed and accuracy of the algorithm under the condition of using a small amount of labeled data sets.
Wiley Online Library",Low
Amir Bidaki,Automatic,Google Scholar,https://ieeexplore.ieee.org/abstract/document/8071525/,"Network security and anomaly detection with Big-DAMA, a big data analytics framework",2003,Type,,Core,"The complexity of the Internet and the volume of network traffic have dramatically increased in the last few years, making it more challenging to design scalable Network Traffic Monitoring and Analysis (NTMA) systems. Critical NTMA applications such as the detection of network attacks and anomalies require fast mechanisms for on-line analysis of thousands of events per second, as well as efficient techniques for off-line analysis of massive historical data. The high-dimensionality of network data provided by current network monitoring systems opens the door to the massive application of machine learning approaches to improve the detection and classification of network attacks and anomalies, but this higher dimensionality comes with an extra data processing overhead. In this paper we present Big-DAMA, a big data analytics framework (BDAF) for NTMA applications. Big-DAMA is a flexible BDAF, capable to analyze and store big amounts of both structured and unstructured heterogeneous data sources, with both stream and batch processing capabilities. Big-DAMA uses off-the-shelf big data storage and processing engines to offer both stream data processing and batch processing capabilities, decomposing separate engines for stream, batch and query, following a Data Stream Warehouse (DSW) paradigm. Big-DAMA implements several algorithms for anomaly detection and network security using supervised and unsupervised machine learning (ML) models, using off-the-shelf ML libraries. We apply Big-DAMA to the detection of different types of network attacks and anomalies, benchmarking multiple supervised ML models. Evaluations are conducted on top of real network measurements collected at the WIDE backbone network, using the well-known MAWILab dataset for attacks labeling. Big-DAMA can speed up computations by a factor of 10 when compared to a standard Apache Spark cluster, and can be easily deployed in cloud environments, using hardware virtualization technology.
ieeexplore.ieee.org",,Cited by 60,,,,"The complexity of the Internet and the volume of network traffic have dramatically increased in the last few years, making it more challenging to design scalable Network Traffic Monitoring and Analysis (NTMA) systems. Critical NTMA applications such as the detection of network attacks and anomalies require fast mechanisms for on-line analysis of thousands of events per second, as well as efficient techniques for off-line analysis of massive historical data. The high-dimensionality of network data provided by current network monitoring systems opens the door to the massive application of machine learning approaches to improve the detection and classification of network attacks and anomalies, but this higher dimensionality comes with an extra data processing overhead. In this paper we present Big-DAMA, a big data analytics framework (BDAF) for NTMA applications. Big-DAMA is a flexible BDAF, capable to analyze and store big amounts of both structured and unstructured heterogeneous data sources, with both stream and batch processing capabilities. Big-DAMA uses off-the-shelf big data storage and processing engines to offer both stream data processing and batch processing capabilities, decomposing separate engines for stream, batch and query, following a Data Stream Warehouse (DSW) paradigm. Big-DAMA implements several algorithms for anomaly detection and network security using supervised and unsupervised machine learning (ML) models, using off-the-shelf ML libraries. We apply Big-DAMA to the detection of different types of network attacks and anomalies, benchmarking multiple supervised ML models. Evaluations are conducted on top of real network measurements collected at the WIDE backbone network, using the well-known MAWILab dataset for attacks labeling. Big-DAMA can speed up computations by a factor of 10 when compared to a standard Apache Spark cluster, and can be easily deployed in cloud environments, using hardware virtualization technology.
ieeexplore.ieee.org",Low
Amir Bidaki,Automatic,Google Scholar,https://ieeexplore.ieee.org/abstract/document/682263/,Neural networks applied in intrusion detection systems,2017,Type,,Core,"Information is one of the most valuable possessions today. As the Internet expands both in number of hosts connected and number of services provided, security has become a key issue for the technology developers. This work presents a prototype of an intrusion detection system for TCP/IP networks. The system works by capturing packets and using a neural network to identify an intrusive behavior within the analyzed data stream. The identification is based on previous well know intrusion profiles. The system is adaptive, since new profiles can be added to the data base and the neural network retrained to consider them. We present the proposed model, the results achieved and the analysis of an implemented prototype.",,Cited by 132,,,,"Information is one of the most valuable possessions today. As the Internet expands both in number of hosts connected and number of services provided, security has become a key issue for the technology developers. This work presents a prototype of an intrusion detection system for TCP/IP networks. The system works by capturing packets and using a neural network to identify an intrusive behavior within the analyzed data stream. The identification is based on previous well know intrusion profiles. The system is adaptive, since new profiles can be added to the data base and the neural network retrained to consider them. We present the proposed model, the results achieved and the analysis of an implemented prototype.",Medium
Amir Bidaki,Automatic,Google Scholar,https://link.springer.com/article/10.1007/s12243-020-00754-7,Blockchain and artificial intelligence for network security,2017,Type,,Core,"Blockchain is a disruptive technology that enables the development of reliable applications, without the need for trust between network peers. Blockchain technology creates global and immutable repositories that guarantee non-repudiation and accountability of stored information. Besides, the explosion in the generation and availability of data on computer networks raises the challenge of processing and managing large amounts of data with ever lower latencies. As a consequence, artificial intelligence and machine learning techniques experience significant improvements and emerge as enabling technologies for the next-generation networks. This special edition is dedicated to these new technologies that shape the world to have more reliable computer networks while enabling new distributed and knowledge-driven security applications and services. This issue’s papers cover a wide range of topics, such as new cryptographic models applied to healthcare, intelligent threat-detection systems, and new consensus mechanisms for the blockchain. After a thorough reviewing process, in which at least two experts have evaluated every paper, six papers have been accepted for this special issue. Reviewers’ comments were helpful and productive first to select the most meaningful contributions as well as to improve the content, quality, and presentation of the accepted papers. Hereafter, we provide a summary of each paper in this special issue. The first one from Marcela Tuler de Oliveira and her colleagues is entitled “A Break-Glass Protocol based on Ciphertext-Policy Attribute-Based Encryption to Access Medical Records in the Cloud.” Although the electronic medical record (EMR) has to keep the patient’s privacy, in the paper, the authors argue that the patient’s EMR should be promptly available to healthcare professionals in the case of an emergency. Thus, the authors propose an attribute-based encryption protocol to provide access to the patient’s encrypted EMR during acute stroke emergency treatment. The proposal assures authorization for accessing patient’s data only for the emergency period. The authors prove that the proposed protocol is resilient to a comprehensive set of security threats, and they also expose the usability of the protocol in a real-world scenario.
The second paper from Hélio do Nascimento Cunha Neto and his colleagues is proposing the “MineCap: Super Incremental Learning for Detecting and Blocking Cryptocurrency Mining on Software-Defined Networking.” The authors claim that unallowed cryptocurrency mining is a current threat in corporative networks. Hence, the authors model the discovery of cryptocurrency mining as a classification problem on a machine learning approach. Moreover, Cunha Neto et al. propose a new incremental learning scheme, called super incremental learning, in which offline candidate classifiers feed a super classification model. The supermodel performs incremental learning on the samples’ pertinence probability for each candidate model and each target class. The authors show growth in the supermodel accuracy as the proposal learns with new arriving data. The following paper from Fabio Cesar Schuartz and his colleagues is entitled “Improving Threat Detection in Networks Using Deep Learning.” In this paper, the authors address the threat detection in big streaming data traffics. The authors propose a deep learning method as a data preprocessing layer to reduce the data dimensionality before the data analysis in the stream processing layer. The proposal applies a deep learning neural network, which automatically extracts the feature from an original feature dataset. The proposal reduces …
Springer",,Cited by 23,,,,"Blockchain is a disruptive technology that enables the development of reliable applications, without the need for trust between network peers. Blockchain technology creates global and immutable repositories that guarantee non-repudiation and accountability of stored information. Besides, the explosion in the generation and availability of data on computer networks raises the challenge of processing and managing large amounts of data with ever lower latencies. As a consequence, artificial intelligence and machine learning techniques experience significant improvements and emerge as enabling technologies for the next-generation networks. This special edition is dedicated to these new technologies that shape the world to have more reliable computer networks while enabling new distributed and knowledge-driven security applications and services. This issue’s papers cover a wide range of topics, such as new cryptographic models applied to healthcare, intelligent threat-detection systems, and new consensus mechanisms for the blockchain. After a thorough reviewing process, in which at least two experts have evaluated every paper, six papers have been accepted for this special issue. Reviewers’ comments were helpful and productive first to select the most meaningful contributions as well as to improve the content, quality, and presentation of the accepted papers. Hereafter, we provide a summary of each paper in this special issue. The first one from Marcela Tuler de Oliveira and her colleagues is entitled “A Break-Glass Protocol based on Ciphertext-Policy Attribute-Based Encryption to Access Medical Records in the Cloud.” Although the electronic medical record (EMR) has to keep the patient’s privacy, in the paper, the authors argue that the patient’s EMR should be promptly available to healthcare professionals in the case of an emergency. Thus, the authors propose an attribute-based encryption protocol to provide access to the patient’s encrypted EMR during acute stroke emergency treatment. The proposal assures authorization for accessing patient’s data only for the emergency period. The authors prove that the proposed protocol is resilient to a comprehensive set of security threats, and they also expose the usability of the protocol in a real-world scenario.
The second paper from Hélio do Nascimento Cunha Neto and his colleagues is proposing the “MineCap: Super Incremental Learning for Detecting and Blocking Cryptocurrency Mining on Software-Defined Networking.” The authors claim that unallowed cryptocurrency mining is a current threat in corporative networks. Hence, the authors model the discovery of cryptocurrency mining as a classification problem on a machine learning approach. Moreover, Cunha Neto et al. propose a new incremental learning scheme, called super incremental learning, in which offline candidate classifiers feed a super classification model. The supermodel performs incremental learning on the samples’ pertinence probability for each candidate model and each target class. The authors show growth in the supermodel accuracy as the proposal learns with new arriving data. The following paper from Fabio Cesar Schuartz and his colleagues is entitled “Improving Threat Detection in Networks Using Deep Learning.” In this paper, the authors address the threat detection in big streaming data traffics. The authors propose a deep learning method as a data preprocessing layer to reduce the data dimensionality before the data analysis in the stream processing layer. The proposal applies a deep learning neural network, which automatically extracts the feature from an original feature dataset. The proposal reduces …
Springer",Medium
Amir Bidaki,Automatic,Google Scholar,https://ieeexplore.ieee.org/abstract/document/7981527/,Toward stream-based IP flow analysis,1998,Type,,Core,"Analyzing IP flows is an essential part of traffic measurement for cyber security. Based on information from IP flows, it is possible to discover the majority of concurrent cyber threats in highspeed, large-scale networks. Some major prevailing challenges for IP flow analysis include, but are not limited to, analysis over a large volume of IP flows, scalability issues, and detecting cyber threats in real time. In this article, we discuss the transformation of present IP flow analysis into a stream-based approach to face current challenges in IP flow analysis. We examine the possible positive and negative impacts of the transformation and present examples of real-world applications, along with our recommendations. Our ongoing results show that stream-based IP flow analysis successfully meets the above-mentioned challenges and is suitable for achieving real-time network security analysis and situational awareness.
ieeexplore.ieee.org",,Cited by 46,,,,"Analyzing IP flows is an essential part of traffic measurement for cyber security. Based on information from IP flows, it is possible to discover the majority of concurrent cyber threats in highspeed, large-scale networks. Some major prevailing challenges for IP flow analysis include, but are not limited to, analysis over a large volume of IP flows, scalability issues, and detecting cyber threats in real time. In this article, we discuss the transformation of present IP flow analysis into a stream-based approach to face current challenges in IP flow analysis. We examine the possible positive and negative impacts of the transformation and present examples of real-world applications, along with our recommendations. Our ongoing results show that stream-based IP flow analysis successfully meets the above-mentioned challenges and is suitable for achieving real-time network security analysis and situational awareness.
ieeexplore.ieee.org",Medium
Amir Bidaki,Automatic,Google Scholar,https://ieeexplore.ieee.org/abstract/document/8735821/,Intrusion detection systems: A cross-domain overview,2020,Type,,Core,"Nowadays, network technologies are essential for transferring and storing various information of users, companies, and industries. However, the growth of the information transfer rate expands the attack surface, offering a rich environment to intruders. Intrusion detection systems (IDSs) are widespread systems able to passively or actively control intrusive activities in a defined host and network perimeter. Recently, different IDSs have been proposed by integrating various detection techniques, generic or adapted to a specific domain and to the nature of attacks operating on. The cybersecurity landscape deals with tremendous diverse event streams that exponentially increase the attack vectors. Event stream processing (ESP) methods appear to be solutions that leverage event streams to provide actionable insights and faster detection. In this paper, we briefly describe domains (as well as their vulnerabilities) on which recent papers were-based. We also survey standards for vulnerability assessment and attack classification. Afterwards, we carry out a classification of IDSs, evaluation metrics, and datasets. Next, we provide the technical details and an evaluation of the most recent work on IDS techniques and ESP approaches covering different dimensions (axes): domains, architectures, and local communication technologies. Finally, we discuss challenges and strategies to improve IDS in terms of accuracy, performance, and robustness.
ieeexplore.ieee.org",,Cited by 149,,,,"Nowadays, network technologies are essential for transferring and storing various information of users, companies, and industries. However, the growth of the information transfer rate expands the attack surface, offering a rich environment to intruders. Intrusion detection systems (IDSs) are widespread systems able to passively or actively control intrusive activities in a defined host and network perimeter. Recently, different IDSs have been proposed by integrating various detection techniques, generic or adapted to a specific domain and to the nature of attacks operating on. The cybersecurity landscape deals with tremendous diverse event streams that exponentially increase the attack vectors. Event stream processing (ESP) methods appear to be solutions that leverage event streams to provide actionable insights and faster detection. In this paper, we briefly describe domains (as well as their vulnerabilities) on which recent papers were-based. We also survey standards for vulnerability assessment and attack classification. Afterwards, we carry out a classification of IDSs, evaluation metrics, and datasets. Next, we provide the technical details and an evaluation of the most recent work on IDS techniques and ESP approaches covering different dimensions (axes): domains, architectures, and local communication technologies. Finally, we discuss challenges and strategies to improve IDS in terms of accuracy, performance, and robustness.
ieeexplore.ieee.org",Low
Amir Bidaki,Automatic,Google Scholar,http://cseweb.ucsd.edu/classes/fa01/cse221/projects/group10.pdf,A survey of intrusion detection systems,2017,Type,,Core,"There should be no question that one of the  most pervasive technology trends in modern computing is an increasing reliance on network connectivity and inter-host communication. Along  with the tremendous opportunities for information and resource sharing that this entails comes  a heightened need for information security, as  computing resources are both more vulnerable  and more heavily depended upon than before.  One subset of information security research  that has been the subject of much attention in recent years is that of intrusion detection systems.  The National Institute of Standards and Technology classifies intrusion detection as “the process of monitoring the events occurring in a computer system or network and analyzing them for  signs of intrusions, defined as attempts to compromise the confidentiality, integrity, availability,  or to bypass the security mechanisms of a computer or network.”1 This definition captures the  essence of intrusion detection but fails to address  the methods by which Intrusion Detection Systems (IDS’s) automate this process. The concepts of false positive and false negative are essential to this classification process. False positives  are those sequences of innocuous events that an  IDS erroneously classifies as intrusive, while false  negatives refer to intrusion attempts that an IDS  fails to report: the reduction of both false positives and false negatives is a critical objective in  intrusion detection.  Modern IDS’s are extremely diverse in the  techniques they employ to gather and analyze  data. Most rely, however, on a common architecture for their structure: a detection module  gathers data that may contain evidence of intrusions, an analysis engine processes this data to  identify intrusive activity, and a response component reports intrusions. As the response mechanisms tend to be dictated by site-specific policy  rather than science, this paper will not discuss  this feature of IDS’s in much detail. Section 2  discusses the primary sources for intrusion detection while Section 3 examines current approaches  to the problem of intrusion data analysis. Section  4 describes a number of issues that represent the  current state of research in intrusion detection;  Section 5 is devoted to addressing the state of  the art in evaluating these systems. Opportunities for future research are presented in Section 6  and concluding remarks are located in Section 7.",,Cited by 81,,,,"There should be no question that one of the  most pervasive technology trends in modern computing is an increasing reliance on network connectivity and inter-host communication. Along  with the tremendous opportunities for information and resource sharing that this entails comes  a heightened need for information security, as  computing resources are both more vulnerable  and more heavily depended upon than before.  One subset of information security research  that has been the subject of much attention in recent years is that of intrusion detection systems.  The National Institute of Standards and Technology classifies intrusion detection as “the process of monitoring the events occurring in a computer system or network and analyzing them for  signs of intrusions, defined as attempts to compromise the confidentiality, integrity, availability,  or to bypass the security mechanisms of a computer or network.”1 This definition captures the  essence of intrusion detection but fails to address  the methods by which Intrusion Detection Systems (IDS’s) automate this process. The concepts of false positive and false negative are essential to this classification process. False positives  are those sequences of innocuous events that an  IDS erroneously classifies as intrusive, while false  negatives refer to intrusion attempts that an IDS  fails to report: the reduction of both false positives and false negatives is a critical objective in  intrusion detection.  Modern IDS’s are extremely diverse in the  techniques they employ to gather and analyze  data. Most rely, however, on a common architecture for their structure: a detection module  gathers data that may contain evidence of intrusions, an analysis engine processes this data to  identify intrusive activity, and a response component reports intrusions. As the response mechanisms tend to be dictated by site-specific policy  rather than science, this paper will not discuss  this feature of IDS’s in much detail. Section 2  discusses the primary sources for intrusion detection while Section 3 examines current approaches  to the problem of intrusion data analysis. Section  4 describes a number of issues that represent the  current state of research in intrusion detection;  Section 5 is devoted to addressing the state of  the art in evaluating these systems. Opportunities for future research are presented in Section 6  and concluding remarks are located in Section 7.",Low
Amir Bidaki,Automatic,Google Scholar,https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=558a077ff4ad18be043f9cfe5c6607980803ae3c,Next generation intrusion detection: Autonomous reinforcement learning of network attacks,2019,Type,,Core,"The timely and accurate detection of computer and network system intrusions has always been an elusive goal for system administrators and information security researchers. Existing intrusion detection approaches require either manual coding of new attacks in expert systems or the complete retraining of a neural network to improve analysis or learn new attacks. This paper presents a new approach to applying adaptive neural networks to intrusion detection that is capable of autonomously learning new attacks rapidly through the use of a modified reinforcement learning method that uses feedback from the protected system. The approach has been demonstrated to be extremely effective in learning new attacks, detecting previously learned attacks in a network data stream, and in autonomously improving its analysis over time using feedback from the protected system.
Citeseer",,Cited by 121,,,,"The timely and accurate detection of computer and network system intrusions has always been an elusive goal for system administrators and information security researchers. Existing intrusion detection approaches require either manual coding of new attacks in expert systems or the complete retraining of a neural network to improve analysis or learn new attacks. This paper presents a new approach to applying adaptive neural networks to intrusion detection that is capable of autonomously learning new attacks rapidly through the use of a modified reinforcement learning method that uses feedback from the protected system. The approach has been demonstrated to be extremely effective in learning new attacks, detecting previously learned attacks in a network data stream, and in autonomously improving its analysis over time using feedback from the protected system.
Citeseer",High
Amir Bidaki,Automatic,Google Scholar,https://ieeexplore.ieee.org/abstract/document/10621237/,Advancing cybersecurity with ai: A multimodal fusion approach for intrusion detection systems,2002,Type,,Core,"This paper presents a novel AI-enabled Intrusion Detection System (IDS) that enhances cybersecurity by integrating multimodal data analysis and AI fusion techniques. Through analysis of network traffic data from four test environments, it illustratesthe benefits of a more robust detection strategy utilizing multiple modalities of the network traffic. The modalities extracted are designed to be protocol-agnostic, enabling their application across various network protocols, thereby broadening the system’s applicability and effectiveness. Fusion of these models’ outputs results in a robust, adaptable solution capable of real-time threat detection with improved accuracy across different network protocols.
ieeexplore.ieee.org",,Cited by 3,,,,"This paper presents a novel AI-enabled Intrusion Detection System (IDS) that enhances cybersecurity by integrating multimodal data analysis and AI fusion techniques. Through analysis of network traffic data from four test environments, it illustratesthe benefits of a more robust detection strategy utilizing multiple modalities of the network traffic. The modalities extracted are designed to be protocol-agnostic, enabling their application across various network protocols, thereby broadening the system’s applicability and effectiveness. Fusion of these models’ outputs results in a robust, adaptable solution capable of real-time threat detection with improved accuracy across different network protocols.
ieeexplore.ieee.org",Medium
Amir Bidaki,Automatic,Google Scholar,https://ieeexplore.ieee.org/abstract/document/5949392/,Fuzzy logic based anomaly detection for embedded network security cyber sensor,2000,Type,,Core,"Resiliency and security in critical infrastructure control systems in the modern world of cyber terrorism constitute a relevant concern. Developing a network security system specifically tailored to the requirements of such critical assets is of a primary importance. This paper proposes a novel learning algorithm for anomaly based network security cyber sensor together with its hardware implementation. The presented learning algorithm constructs a fuzzy logic rule base modeling the normal network behavior. Individual fuzzy rules are extracted directly from the stream of incoming packets using an online clustering algorithm. This learning algorithm was specifically developed to comply with the constrained computational requirements of low-cost embedded network security cyber sensors. The performance of the system was evaluated on a set of network data recorded from an experimental test-bed mimicking the environment of a critical infrastructure control system.
ieeexplore.ieee.org",,Cited by 92,,,,"Resiliency and security in critical infrastructure control systems in the modern world of cyber terrorism constitute a relevant concern. Developing a network security system specifically tailored to the requirements of such critical assets is of a primary importance. This paper proposes a novel learning algorithm for anomaly based network security cyber sensor together with its hardware implementation. The presented learning algorithm constructs a fuzzy logic rule base modeling the normal network behavior. Individual fuzzy rules are extracted directly from the stream of incoming packets using an online clustering algorithm. This learning algorithm was specifically developed to comply with the constrained computational requirements of low-cost embedded network security cyber sensors. The performance of the system was evaluated on a set of network data recorded from an experimental test-bed mimicking the environment of a critical infrastructure control system.
ieeexplore.ieee.org",Medium
Amir Bidaki,Automatic,Google Scholar,https://dl.acm.org/doi/abs/10.1145/1402958.1402980,Enriching network security analysis with time travel,2024,Type,,Core,"In many situations it can be enormously helpful to archive the raw contents of a network traffic stream to disk, to enable later inspection of activity that becomes interesting only in retrospect. We present a Time Machine (TM) for network traffic that provides such a capability. The TM leverages the heavy-tailed nature of network flows to capture nearly all of the likely-interesting traffic while storing only a small fraction of the total volume. An initial proof-of-principle prototype established the forensic value of such an approach, contributing to the investigation of numerous attacks at a site with thousands of users. Based on these experiences, a rearchitected implementation of the system provides flexible, highperformance traffic stream capture, indexing and retrieval, including an interface between the TM and a real-time network intrusion detection system (NIDS). The NIDS controls the TM by dynamically adjusting recording parameters, instructing it to permanently store suspicious activity for offline forensics, and fetching traffic from the past for retrospective analysis. We present a detailed performance evaluation of both stand-alone and joint setups, and report on experiences with running the system live in high-volume environments.
ACM Digital Library",,Cited by 134,,,,"In many situations it can be enormously helpful to archive the raw contents of a network traffic stream to disk, to enable later inspection of activity that becomes interesting only in retrospect. We present a Time Machine (TM) for network traffic that provides such a capability. The TM leverages the heavy-tailed nature of network flows to capture nearly all of the likely-interesting traffic while storing only a small fraction of the total volume. An initial proof-of-principle prototype established the forensic value of such an approach, contributing to the investigation of numerous attacks at a site with thousands of users. Based on these experiences, a rearchitected implementation of the system provides flexible, highperformance traffic stream capture, indexing and retrieval, including an interface between the TM and a real-time network intrusion detection system (NIDS). The NIDS controls the TM by dynamically adjusting recording parameters, instructing it to permanently store suspicious activity for offline forensics, and fetching traffic from the past for retrospective analysis. We present a detailed performance evaluation of both stand-alone and joint setups, and report on experiences with running the system live in high-volume environments.
ACM Digital Library",Low
Amir Bidaki,Automatic,Google Scholar,http://ftp.unpad.ac.id/orari/library/library-ref-eng/ref-eng-2/network/network-security/Benchmarking-IDS-NFR.pdf,Experiences benchmarking intrusion detection systems,2011,Type,,Core,"Intrusion Detection Systems (hereafter abbreviated as “IDS”) are a topic that has  recently garnered much interest in the computer security community. As interest has grown, the  topic of testing and benchmarking IDS has also received a great deal of attention. It has not,  however, received a great deal of thought, since an embarrassingly large number of IDS  “benchmarks” have proven to be so fundamentally flawed that they actually provide misleading  information rather than useful results. In this paper, we discuss the topic of IDS benchmarking,  and present a few examples of poor benchmarks and how they can be fixed. We also present  some guidelines on how to design and test IDS effectively",,Cited by 98,,,,"Intrusion Detection Systems (hereafter abbreviated as “IDS”) are a topic that has  recently garnered much interest in the computer security community. As interest has grown, the  topic of testing and benchmarking IDS has also received a great deal of attention. It has not,  however, received a great deal of thought, since an embarrassingly large number of IDS  “benchmarks” have proven to be so fundamentally flawed that they actually provide misleading  information rather than useful results. In this paper, we discuss the topic of IDS benchmarking,  and present a few examples of poor benchmarks and how they can be fixed. We also present  some guidelines on how to design and test IDS effectively",Low
Amir Bidaki,Automatic,Google Scholar,https://ieeexplore.ieee.org/abstract/document/6475958/,Strifa: Stride finite automata for high-speed regular expression matching in network intrusion detection systems,2008,Type,,Core,"Deep packet inspection has become a key component in network intrusion detection systems (NIDSes), where every packet in the incoming data stream needs to be compared with patterns in an attack database, byte-by-byte, using either string matching or regular expression matching. Regular expression matching, despite its flexibility and efficiency in attack identification, brings significantly high computation and storage complexities to NIDSes, making line-rate packet processing a challenging task. In this paper, we present stride finite automata (StriFA), a novel finite automata family, to accelerate both string matching and regular expression matching. Different from conventional finite automata, which scan the entire traffic stream to locate malicious information, a StriFA only needs to scan a partial traffic stream to find suspicious information. The presented StriFA technique has been implemented in software and evaluated based on different traces. The simulation results show that the StriFA acceleration scheme offers an increased speed over traditional nondeterministic finite automaton",,Cited by 18,,,,"Deep packet inspection has become a key component in network intrusion detection systems (NIDSes), where every packet in the incoming data stream needs to be compared with patterns in an attack database, byte-by-byte, using either string matching or regular expression matching. Regular expression matching, despite its flexibility and efficiency in attack identification, brings significantly high computation and storage complexities to NIDSes, making line-rate packet processing a challenging task. In this paper, we present stride finite automata (StriFA), a novel finite automata family, to accelerate both string matching and regular expression matching. Different from conventional finite automata, which scan the entire traffic stream to locate malicious information, a StriFA only needs to scan a partial traffic stream to find suspicious information. The presented StriFA technique has been implemented in software and evaluated based on different traces. The simulation results show that the StriFA acceleration scheme offers an increased speed over traditional nondeterministic finite automaton",Low
Amir Bidaki,Automatic,Google Scholar,https://www.mdpi.com/2076-3417/9/20/4396,Machine learning and deep learning methods for intrusion detection systems: A survey,2013,Type,,Core,"Networks play important roles in modern life, and cyber security has become a vital research area. An intrusion detection system (IDS) which is an important cyber security technique, monitors the state of software and hardware running in the network. Despite decades of development, existing IDSs still face challenges in improving the detection accuracy, reducing the false alarm rate and detecting unknown attacks. To solve the above problems, many researchers have focused on developing IDSs that capitalize on machine learning methods. Machine learning methods can automatically discover the essential differences between normal data and abnormal data with high accuracy. In addition, machine learning methods have strong generalizability, so they are also able to detect unknown attacks. Deep learning is a branch of machine learning, whose performance is remarkable and has become a research hotspot. This survey proposes a taxonomy of IDS that takes data objects as the main dimension to classify and summarize machine learning-based and deep learning-based IDS literature. We believe that this type of taxonomy framework is fit for cyber security researchers. The survey first clarifies the concept and taxonomy of IDSs. Then, the machine learning algorithms frequently used in IDSs, metrics, and benchmark datasets are introduced. Next, combined with the representative literature, we take the proposed taxonomic system as a baseline and explain how to solve key IDS issues with machine learning and deep learning techniques. Finally, challenges and future developments are discussed by reviewing recent representative studies.
MDPI",,Cited by 1011,,,,"Networks play important roles in modern life, and cyber security has become a vital research area. An intrusion detection system (IDS) which is an important cyber security technique, monitors the state of software and hardware running in the network. Despite decades of development, existing IDSs still face challenges in improving the detection accuracy, reducing the false alarm rate and detecting unknown attacks. To solve the above problems, many researchers have focused on developing IDSs that capitalize on machine learning methods. Machine learning methods can automatically discover the essential differences between normal data and abnormal data with high accuracy. In addition, machine learning methods have strong generalizability, so they are also able to detect unknown attacks. Deep learning is a branch of machine learning, whose performance is remarkable and has become a research hotspot. This survey proposes a taxonomy of IDS that takes data objects as the main dimension to classify and summarize machine learning-based and deep learning-based IDS literature. We believe that this type of taxonomy framework is fit for cyber security researchers. The survey first clarifies the concept and taxonomy of IDSs. Then, the machine learning algorithms frequently used in IDSs, metrics, and benchmark datasets are introduced. Next, combined with the representative literature, we take the proposed taxonomic system as a baseline and explain how to solve key IDS issues with machine learning and deep learning techniques. Finally, challenges and future developments are discussed by reviewing recent representative studies.
MDPI",Low
Amir Bidaki,Automatic,Google Scholar,https://ieeexplore.ieee.org/abstract/document/4482891/,A gpu-based multiple-pattern matching algorithm for network intrusion detection systems,2020,Type,,Core,"By the development of network applications, network security issues are getting more and more important. This paper proposes a multiple-pattern matching algorithm for the network intrusion detection systems based on the GPU (Graphics Processing Units). The highly parallelism of the GPU computation power is used to inspect the packet content in parallel. The performance of the proposed approach is analyzed through evaluations such as using various texture formats and different implementations. Experimental results indicate that the performance of the proposed approach is twice of that of the modified Wu-Manber algorithm used in Snort. The proposed approach makes a commodity and cheap GPU card as a high performance pattern matching co-processor.
ieeexplore.ieee.org",,Cited by 98,,,,"By the development of network applications, network security issues are getting more and more important. This paper proposes a multiple-pattern matching algorithm for the network intrusion detection systems based on the GPU (Graphics Processing Units). The highly parallelism of the GPU computation power is used to inspect the packet content in parallel. The performance of the proposed approach is analyzed through evaluations such as using various texture formats and different implementations. Experimental results indicate that the performance of the proposed approach is twice of that of the modified Wu-Manber algorithm used in Snort. The proposed approach makes a commodity and cheap GPU card as a high performance pattern matching co-processor.
ieeexplore.ieee.org",Low
Amir Bidaki,Automatic,Google Scholar,https://link.springer.com/article/10.1007/s12083-017-0630-0,Survey on SDN based network intrusion detection system using machine learning approaches,2019,Type,,Core,"Software Defined Networking Technology (SDN) provides a prospect to effectively detect and monitor network security problems ascribing to the emergence of the programmable features. Recently, Machine Learning (ML) approaches have been implemented in the SDN-based Network Intrusion Detection Systems (NIDS) to protect computer networks and to overcome network security issues. A stream of advanced machine learning approaches – the deep learning technology (DL) commences to emerge in the SDN context. In this survey, we reviewed various recent works on machine learning (ML) methods that leverage SDN to implement NIDS. More specifically, we evaluated the techniques of deep learning in developing SDN-based NIDS. In the meantime, in this survey, we covered tools that can be used to develop NIDS models in SDN environment. This survey is concluded with a discussion of ongoing challenges in implementing NIDS using ML/DL and future works.",,Cited by 636,,,,"Software Defined Networking Technology (SDN) provides a prospect to effectively detect and monitor network security problems ascribing to the emergence of the programmable features. Recently, Machine Learning (ML) approaches have been implemented in the SDN-based Network Intrusion Detection Systems (NIDS) to protect computer networks and to overcome network security issues. A stream of advanced machine learning approaches – the deep learning technology (DL) commences to emerge in the SDN context. In this survey, we reviewed various recent works on machine learning (ML) methods that leverage SDN to implement NIDS. More specifically, we evaluated the techniques of deep learning in developing SDN-based NIDS. In the meantime, in this survey, we covered tools that can be used to develop NIDS models in SDN environment. This survey is concluded with a discussion of ongoing challenges in implementing NIDS using ML/DL and future works.",Low
Amir Bidaki,Automatic,Google Scholar,https://dl.acm.org/doi/abs/10.1145/1787275.1787296,Efficient pattern matching on GPUs for intrusion detection systems,2008,Type,,Core,"In this paper we present an efficient implementation of the Aho-Corasick pattern matching algorithm on Graphics Processing Units (GPU), showing how we redesigned the algorithm and the data structures to fit on the architecture and comparing it with an equivalent implementation on the CPU. We show that with a synthetic dataset, our implementation obtains a speedup up to 6.67 with respect to the CPU solution.",,Cited by 58,,,,"In this paper we present an efficient implementation of the Aho-Corasick pattern matching algorithm on Graphics Processing Units (GPU), showing how we redesigned the algorithm and the data structures to fit on the architecture and comparing it with an equivalent implementation on the CPU. We show that with a synthetic dataset, our implementation obtains a speedup up to 6.67 with respect to the CPU solution.",Low
Amir Bidaki,Automatic,Google Scholar,https://ieeexplore.ieee.org/abstract/document/5716474/,A system approach to network modeling for DDoS detection using a Naive Bayesian classifier,2019,Type,,Core,"Denial of Service(DoS) attacks pose a big threat to any electronic society. DoS and DDoS attacks are catastrophic particularly when applied to highly sensitive targets like Critical Information Infrastructure. While research literature has focussed on using various fundamental classifier models for detecting attacks, the common trend observed in literature is to classify DoS attacks into the broad class of intrusions, which makes proposed solutions to this class of attacks unrealistic in practical terms. In this work, the approach to a carefully engineered, practically realised system to detect DoS attacks using a Naìve Bayesian(NB) classifier is described. The work includes network modeling for two protocols - TCP and UDP.
ieeexplore.ieee.org",,Cited by 87,,,,"Denial of Service(DoS) attacks pose a big threat to any electronic society. DoS and DDoS attacks are catastrophic particularly when applied to highly sensitive targets like Critical Information Infrastructure. While research literature has focussed on using various fundamental classifier models for detecting attacks, the common trend observed in literature is to classify DoS attacks into the broad class of intrusions, which makes proposed solutions to this class of attacks unrealistic in practical terms. In this work, the approach to a carefully engineered, practically realised system to detect DoS attacks using a Naìve Bayesian(NB) classifier is described. The work includes network modeling for two protocols - TCP and UDP.
ieeexplore.ieee.org",Low
Amir Bidaki,Automatic,Google Scholar,https://dl.acm.org/doi/abs/10.1145/974044.974078,Generating realistic workloads for network intrusion detection systems,2010,Type,,Core,"While the use of network intrusion detection systems (nIDS) is becoming pervasive, evaluating nIDS performance has been found to be challenging. The goal of this study is to determine how to generate realistic workloads for nIDS performance evaluation. We develop a workload model that appears to provide reasonably accurate estimates compared to real workloads. The model attempts to emulate a traffic mix of different applications, reflecting characteristics of each application and the way these interact with the system. We have implemented this model as part of a traffic generator that can be extended and tuned to reflect the needs of different scenarios. We also present an approach to measuring the capacity of a nIDS that does not require the setup of a full network testbed.
ACM Digital Library",,Cited by 272,,,,"While the use of network intrusion detection systems (nIDS) is becoming pervasive, evaluating nIDS performance has been found to be challenging. The goal of this study is to determine how to generate realistic workloads for nIDS performance evaluation. We develop a workload model that appears to provide reasonably accurate estimates compared to real workloads. The model attempts to emulate a traffic mix of different applications, reflecting characteristics of each application and the way these interact with the system. We have implemented this model as part of a traffic generator that can be extended and tuned to reflect the needs of different scenarios. We also present an approach to measuring the capacity of a nIDS that does not require the setup of a full network testbed.
ACM Digital Library",Low
Amir Bidaki,Automatic,Google Scholar,https://www.sciencedirect.com/science/article/pii/S1389128699001139,Intrusion detection systems as evidence,2011,Type,,Core,"Although the main aim of Intrusion Detection Systems (IDSs) is to detect intrusions to prompt evasive measures, a further aim can be to supply evidence in criminal and civil legal proceedings. However the features that make an ID product good at providing early warning may render it less useful as an evidence-acquisition tool. An explanation is provided of admissibility and weight, the two determinants in the legal acceptability of evidence. The problems the courts have in dealing with novel scientific evidence and the differences between `scientific' and `legal' proof are discussed. Criteria for the evaluation of IDSs as sources of legal evidence are proposed, including preservation of evidence, continuity of evidence and transparency of forensic method. It is suggested that the key to successful prosecution of complex intrusions is the finding of multiple independent streams of evidence which corroborate one another. The USAF Rome Labs intrusion of early 1994 is used as a case-study to show how defence experts and lawyers can undermine investigators’ evidence.
Elsevier",,Cited by 171,,,,"Although the main aim of Intrusion Detection Systems (IDSs) is to detect intrusions to prompt evasive measures, a further aim can be to supply evidence in criminal and civil legal proceedings. However the features that make an ID product good at providing early warning may render it less useful as an evidence-acquisition tool. An explanation is provided of admissibility and weight, the two determinants in the legal acceptability of evidence. The problems the courts have in dealing with novel scientific evidence and the differences between `scientific' and `legal' proof are discussed. Criteria for the evaluation of IDSs as sources of legal evidence are proposed, including preservation of evidence, continuity of evidence and transparency of forensic method. It is suggested that the key to successful prosecution of complex intrusions is the finding of multiple independent streams of evidence which corroborate one another. The USAF Rome Labs intrusion of early 1994 is used as a case-study to show how defence experts and lawyers can undermine investigators’ evidence.
Elsevier",Low
Amir Bidaki,Automatic,Google Scholar,https://link.springer.com/chapter/10.1007/978-3-319-46376-6_22,Dimensionality reduction for intrusion detection systems in multi-data streams—A review and proposal of unsupervised feature selection scheme,2004,Type,,Core,"An Intrusion Detection System (IDS) is a security mechanism that is intended to dynamically inspect traffic in order to detect any suspicious behaviour or launched attacks. However, it is a challenging task to apply IDS for large and high dimensional data streams. Data streams have characteristics that are quite distinct from those of statistical databases, which greatly impact on the performance of the anomaly-based ID algorithms used in the detection process. These characteristics include, but are not limited to, the processing of large data as they arrive (real-time), the dynamic nature of data streams, the curse of dimensionality, limited memory capacity and high complexity. Therefore, the main challenge in this area of research is to design efficient data-driven ID systems that are capable of efficiently dealing with data streams by considering these specific traffic characteristics. This chapter provides an overview of some of the relevant work carried out in three major fields related to the topic, namely feature selections (FS), intrusion detection systems (IDS) and anomaly detection in multi data streams. This overview is intended to provide the reader with a better understanding of the major recent works in the area. By critically investigating and combining those three fields, researchers and practitioners will be better able to develop efficient and robust IDS for data streams. At the end of this chapter, we provide two basic models: an Unsupervised Feature Selection to Improve Detection Accuracy for Anomaly Detection (UFSAD) and its extension (UFSAD-MS) for multi streams, that could reduce the volume and the dimensionality of the big data resulting from the streams. The reduction is based on the selection of only the relevant features and removing irrelevant and redundant ones. The last section of the chapter provides an example of the developed UFSAD model, followed by some experimental results. UFSAD-MS is provided as a conceptual model as it is in the implementation phase.Springer",,Cited by 35,,,,"An Intrusion Detection System (IDS) is a security mechanism that is intended to dynamically inspect traffic in order to detect any suspicious behaviour or launched attacks. However, it is a challenging task to apply IDS for large and high dimensional data streams. Data streams have characteristics that are quite distinct from those of statistical databases, which greatly impact on the performance of the anomaly-based ID algorithms used in the detection process. These characteristics include, but are not limited to, the processing of large data as they arrive (real-time), the dynamic nature of data streams, the curse of dimensionality, limited memory capacity and high complexity. Therefore, the main challenge in this area of research is to design efficient data-driven ID systems that are capable of efficiently dealing with data streams by considering these specific traffic characteristics. This chapter provides an overview of some of the relevant work carried out in three major fields related to the topic, namely feature selections (FS), intrusion detection systems (IDS) and anomaly detection in multi data streams. This overview is intended to provide the reader with a better understanding of the major recent works in the area. By critically investigating and combining those three fields, researchers and practitioners will be better able to develop efficient and robust IDS for data streams. At the end of this chapter, we provide two basic models: an Unsupervised Feature Selection to Improve Detection Accuracy for Anomaly Detection (UFSAD) and its extension (UFSAD-MS) for multi streams, that could reduce the volume and the dimensionality of the big data resulting from the streams. The reduction is based on the selection of only the relevant features and removing irrelevant and redundant ones. The last section of the chapter provides an example of the developed UFSAD model, followed by some experimental results. UFSAD-MS is provided as a conceptual model as it is in the implementation phase.Springer",Medium
Amir Bidaki,Automatic,Google Scholar,https://ieeexplore.ieee.org/abstract/document/8424629/,Machine learning ddos detection for consumer internet of things devices,1999,Type,,Core,"An increasing number of Internet of Things (IoT) devices are connecting to the Internet, yet many of these devices are fundamentally insecure, exposing the Internet to a variety of attacks. Botnets such as Mirai have used insecure consumer IoT devices to conduct distributed denial of service (DDoS) attacks on critical Internet infrastructure. This motivates the development of new techniques to automatically detect consumer IoT attack traffic. In this paper, we demonstrate that using IoT-specific network behaviors (e.g., limited number of endpoints and regular time intervals between packets) to inform feature selection can result in high accuracy DDoS detection in IoT network traffic with a variety of machine learning algorithms, including neural networks. These results indicate that home gateway routers or other network middleboxes could automatically detect local IoT device sources of DDoS attacks using low-cost machine learning algorithms and traffic data that is flow-based and protocol-agnostic.
ieeexplore.ieee.org",,Cited by 958,,,,"An increasing number of Internet of Things (IoT) devices are connecting to the Internet, yet many of these devices are fundamentally insecure, exposing the Internet to a variety of attacks. Botnets such as Mirai have used insecure consumer IoT devices to conduct distributed denial of service (DDoS) attacks on critical Internet infrastructure. This motivates the development of new techniques to automatically detect consumer IoT attack traffic. In this paper, we demonstrate that using IoT-specific network behaviors (e.g., limited number of endpoints and regular time intervals between packets) to inform feature selection can result in high accuracy DDoS detection in IoT network traffic with a variety of machine learning algorithms, including neural networks. These results indicate that home gateway routers or other network middleboxes could automatically detect local IoT device sources of DDoS attacks using low-cost machine learning algorithms and traffic data that is flow-based and protocol-agnostic.
ieeexplore.ieee.org",Low
Amir Bidaki,Automatic,Google Scholar,https://tsapps.nist.gov/publication/get_pdf.cfm?pub_id=50745,An overview of issues in testing intrusion detection systems,2017,Type,,Core,"While intrusion detection systems are becoming ubiquitous defenses in today's networks, currently we have no comprehensive and scientifically rigorous methodology to test the effectiveness of these systems. This paper explores the types of performance measurements that are desired and that have been used in the past. We review many past evaluations that have been designed to assess these metrics. We also discuss the hurdles that have blocked successful measurements in this area and present suggestions for research directed toward improving our measurement capabilities. ",,Cited by 238,,,,"While intrusion detection systems are becoming ubiquitous defenses in today's networks, currently we have no comprehensive and scientifically rigorous methodology to test the effectiveness of these systems. This paper explores the types of performance measurements that are desired and that have been used in the past. We review many past evaluations that have been designed to assess these metrics. We also discuss the hurdles that have blocked successful measurements in this area and present suggestions for research directed toward improving our measurement capabilities. ",Low