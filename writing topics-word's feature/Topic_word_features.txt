sample selection : purity aware sampling , Quantized exemplars , sample efficiency issue , sampled without replacement , samples of data stream are seen only once , skip more training samples , squeeze out more valuable information and effectively , Use the knowledge of task boundaries to select the representative examples , compute the probability of cleanliness per sample , control the class distribution in memory well , adaptation with dynamic data sampling , candidates selecting , clustering based exemplar selection algorithm , controls what resolution the sample is stored , custom replay sampling , data sampling method used , drawing examples from the memory , editing the drawn examples , Exemplar selection , exemplar selection algorithm , gradient-based sample selection strategy , greedy selection , Instance stream rather than task stream , integrates dynamic sampling and pixel relabeling , keep a small part of samples from the data stream in memory , more informative samples , optimally sample training data from generated data , to address dilemma that selects the examples with low diversity , Diversity-based update mechanism of the replay buffer , maximizing the diversity of samples , trade-off in quality of information versus the amount of samples stored , adding qualified images into the buffer from streaming data , apture a representative sample from each class in memory , automatically sampling , based on rare semantic class sampling and image diversity , buffer deletion (and insertion) agnostic to the level of the sample , assessing image quality , Online Sample Selection , random sampling , raw data are retrieved randomly , Reservoir sampling , reservoir sampling , Sample Selection as Constraint Reduction , samples selection , sampling from memory , scores memory data samples , scores memory data samples , Select exampler , select the same number of samples , selection of stored samples , selective sampling , selective sampling strategy , reservoir sampling strategy , just needs the gradient of a random subset of memory examples , MULTI-LEVEL RESERVOIR SAMPLING , selecting suitable images from the memory to replay , selecting suitable images from the buffer to replay during training , set the target class distribution in memory , balancing the class distribution in memory , Class-balanced Exemplar Selection , confidence-based coreset selection , detect the training stage and judge the balance of the coresets , guarantees the same number of training samples for each class in each training iteration , maintaining class balance in the buffer , make the buffer more balanced , Exemplar Selection From Clusters 

Optimization techniques : optimal learning , Optimization Objective Function , optimization problem , optimization process to train the calibrated model , optimize the loss on the current example(s) without increasing the losses on the previously learned examples , solid angle minimization problem , solve convex constraint optimization problems , stabilize our training process , Studied the relationship between the solid angle and the surrogate function , the inner loss function achieves better performance , The training process is controlled , The training process of the parameter generator , to address the underfitting-overfitting dilemma , training procedure specialized , tune the hyperparameters , update importance weights , Weighted Random Replay , PREVENTING THE INNER OPTIMIZATION BIAS , reduce the variance of SGD , Adam optimizer , Adjust the coefficient , adjust the hyperparameters , avoid over-fitting and vanishing gradients , backpropagation to update the network weights , backward optimization path , build efficient regression models on this search space , coefficient learning , Compensate the gradient imbalance (GI) , Conditional Computation , converges faster , Efficient Computation , feasible parameter space during the optimization trajectory , feature optimiser , find the high-performing parameter configuration , find the high-performing parameter configuration in this mixed search space , generates the configuration to evaluate in the mixed search space , hyper-parameter selection , identify optimal configurations for each case , Integer Quadratic Programming , is trained to maximize , is trained to minimize , Learning rate Scheduling , minimizing the distance in parameter spac , Mixed-Integer Optimization Framework , new combination of three loss function , obtaining the optimal solution , avoids the high-loss ridge of past tasks , Efficient Online Hyperparameter Optimization , a first-order method , dose not require computing the Hessian matrix , linear time complexity , avoid computing second-order derivative which is computationally inhibitable , bi-level optimization problem , BILEVEL LEARNING , calculate the training gradients , gradient based optimization , Gradient-Based 

knowledge retention : preserve knowledge at different feature levels , Protects the learned knowledge , retain information from previous samples and tasks , retain the information among tasks , retain the information among tasks , retain useful learned knowledge , retention of old knowledge , strengthens supervision to consolidate previous knowledge , Summarize the knowledge , the ability to retain previous knowledge , the objective for optimizing information retention , update the network parameters with respect to the new task while preserving the knowledge of the previous tasks , use more past knowledge to prevent interference , accumulate knowledge over time , allowing knowledge retention , comprehensive knowledge , consolidating the predictions of previous samples at each learning step , encourages remembering all historical knowledge , faces difficulty in accumulating the knowledge over time , knowledge retention , Accumulate the knowledge of previous classes , generating indistinguishable features , obtain new knowledge from incoming tasks , collecting distillation data , combine generic experience replay with additional constraint , distilling the training characteristics , enable the use of knowledge distillation , knowledge distillation , imitate the predictions of the past model , knowledge distillation on augmented exemplars , learn the predictions of the previous model�s intermediate representations. , Past-assisted Summarizing , preserving old information , preserving previous information without example buffers , retain instance memory , retain task memory , creating a buffer to store data with independent and identical distributions , maintains a purified replay buffer , improve information retention , keep important information , memorize the facts and experiences ,

Streaming data learning : online continual learne , online continual learning setting , online learning , online learning rate algorithms , online manner , online scenario , online training regime , only observing each sample once , Rehearsal to mitigate forgetting in online case , stability-plasticity trade-off online , true continuous inference , full exploration of the data stream , handle streaming data , online updates , rapid online learning , any-time inference , be updated in realtime , continues to reveal new data , data arrives in small batches , data arrives one-by-one , detects novel activities from streaming sensor data , effective online learning regime , focus on training at the mini-batch level , handle data streams , learn from a continuous stream , minibatch of training , online , online continual classification of images , online continual generative modeling , online continual visual learning , samples of data stream are seen only once , Online Adaptation , (online) incremental learning , rapid online updates , rehearse for retaining and calibrating learned and online information , Rehearsal to mitigate forgetting in online case 

classifier : schedule-robust classifier , single-head classifier in the domain-incremental learning , task-label inference for hybrid task-free classifiers , train task-specific classification models , training an intermediate classifier , update the classifier , auxiliary classification loss , balance the classifier based on degree of bias , classification , classification settings , kNN-based , auxiliary classifier , classification , classification loss to update the model , classifier , classifier cannot leverage the distribution shift , decoupling the issue of incremental classification from the one of learning an overall transferable representation , directly classifying fails to fully explore data samples of novel classes. , image classification , incremental classification , Keys and classifiers , Learn a discriminative classifier for each new task , maintaining strong classification performance , maintaining the classification characteristics of the replayed features , maintaining the classification characteristics of the replayed features , multi-head classifiers , classify the generated features , food image classification , generate the class prediction scores , Generates a more discriminative classifier , intermediary classification 

parameter updating : alternates between standard-process and intra-process updates , compute the parameter importance , Rapidly adapt to the new task , update the parameter generator , update the parameters , update the weights , update the weights decoders only , updating the network weights , dynamically adjust the hyperparameters , iteratively update the model parameters. , multiple parameter updates , adapt the network during inference time , adjust the frequency dynamically , Depth Adaptatio , DNNs can be updated , dynamic threshold , Efficient Online Hyperparameter Optimization , evaluation process iteratively , fast adaptation in a parallel way , intervene in the update of network parameters in the current task , learned the distribution to generate model parameters , learns to scale the weights , update the classifier , update the network parameters with respect to the new task while preserving the knowledge of the previous tasks , adapt to new knowledge , incremental model updates , rapid online updates , optimizethe model�s confidence threshold value , parameter updating , careful update scheme allows for learning incrementally 

task-agnostic : online task-agnostic continual learning , online task-free continual learning , Task agnostic , Task Agnostic Setting , task-agnostic evaluation , task-agnostic inference , Task-Agnosticism , task-free , Task-free setting , without assumptions about labels without the need for task labels , without the notion of clearly delineated tasks setting , task-free , blurry task boundaries , can work with or without task information at test time , do not require knowledge of the current task , Does not require knowing the total number of data , does not require knowledge of the current task , task-label inference for hybrid task-free classifiers , task-agnostic calibrator , learner does not have access to  the task identifier t at inference time , no assumption on task boundaries , no task boundary , no task or class boundaries , not define clear task boundaries , not rely on the oracle of task boundary , task-free online class incremental setup 

feature representation : only the most salient features are included in the representation , provide a generalized feature network for future inputs , task-invariant features enabled , the generic feature bank , feature transformation strategy , induce common features , classify the generated features , converting the common features into the task-specific features , embeds the class-specific features , feature representation layer , feature transformation , feeding the combined features , leverages multi-level representations produced , store feature embedding , generate image features , Extract feature embeddings of new data , extract generalised useful features , extract the features , extracts the task-invariant features enabled , feature extraction , generate task-invariant features , Generates a more discriminative classifier , multi-layer feature extractor , pretrained feature extractor , feature extraction layers , feature extractor 

 mitigateing Catastrophic forgetting : overcame forgetting , overcome forgetting , Reduce forgetting even with noisy labels , reducing forgetting issues , robust against the catastrophic forgetting problem , without catastrophic forgetting , mitigate forgettin , alleviate catastrophic forgetting , alleviate forgetting , alleviate forgetting , alleviate the forgetting , alleviating forgetting , avoid CF , avoid forgetting , avoiding overwriting the previous information , Avoiding the need for removing data from the buffer , Catastrophic forgetting , catastrophic forgetting is significantly mitigated , catastrophic forgetting suppressed , effect reducing forgetting , Ensures no interference with tasks that have been learned , explicitly reducing forgetting , mitigating catastrophic forgetting , never forgets , Rehearsal to mitigate forgetting in online case 

Memory update and retrieval : store feature embedding , store the classes informations , store the corresponding logits and feature and previous raw samples , stored in a low-dimensional vector , update a replay memory , update memory , Update the replay buffer , updating of the memory , MemoryRetrieva , MemoryUpdate , adopted for MemoryUpdate operation , apture a representative sample from each class in memory , CL memory retrieval , Construct an update batch , Incoming data add to storage , memory retrieval , memory update , MemoryRetrieval , MemoryUpdate , multiple updates , Multiple Updates for Incoming Samples , needs to update the memory , store an independent and identically distributed(iid) , store the most representative data , adding qualified images into the buffer from streaming data 

memory efficiency : reduces the memory usage , avoiding an increase of memory consumption over time , buffer can store more data for the same fixed amount of memory , does not need to store additional samples , Does not require additional memory to store the output logits , improved storage of previous data , low storage requirements , maintaining quality and maximizing memory usage , memory efficient , memory usage , Memory-efficient , needs no extra memory , avoiding overwriting the previous information , Avoiding the need for removing data from the buffer , efficient use of limited model capacity , optimize storage and sampling , second best algorithm across all memory configurations , control the network�s memory footprint , less memory requirements , allows use of more capacity for harder data, and fewer for easier , better on small memory size , consumes few memory requirements , effect freeing memory , efficiently train the model with a limited memory 

Task Incremental Learning : ONLINE TASK-INCREMENTAL CONTINUAL LEARNING , preserving the ability to solve old tasks , promote consistency with past tasks , protects the learned knowledge for the previous tasks , single task , single-incremental-task (SIT) , starting learning the new task on the current batch , suitable for long task sequences , Task-incremental (task-IL) Learning , task-incremental scenario , Task-incremental setting , training the sequential tasks , task-incremental learning problems , discrimination for each learned task , is suitable for long task sequences , learning new tasks , multi-incremental-task (MIT) , multi-task (MT) , number of different tasks , obtain new knowledge from incoming tasks , Use the knowledge of task boundaries to select the representative examples , suitable for long task sequences achieving strong performance with minimal or no additional cost , Ensures no interference with tasks that have been learned 

Imbalanced data handling : preventing imbalance , rebalance contrastive learning between new and past classes , imbalanced data streams , Gradient Imbalance due to CL Imbalance , Gradient Imbalance due to Data Imbalance , mitigate the influence of the resulting imbalance later , rehearsal imbalance , Class Balancing Strategies , Classes are imbalanced , cop with class-imbalance simultaneously , imbalanced , task boundaries are blurry or data are imbalanced , set the target class distribution in memory , balancing the class distribution in memory , Class-balanced Exemplar Selection , control the class distribution in memory , detect the training stage and judge the balance of the coresets , guarantees the same number of training samples for each class in each training iteration , maintaining class balance in the buffer , make the buffer more balanced , apture a representative sample from each class in memory , Exemplar Selection From Clusters , confidence-based coreset selection 

image handling : pixel-wise semantic labeling of images , selecting suitable images from the memory to replay , selecting suitable images from the buffer to replay during training , synthetic image , generate image features , adding qualified images into the buffer from streaming data , Balances the computational cost of the whole OCL training phase and the degradation of the images in memory , calculate weights of the images in the buffer , online continual classification of images , online continual visual learning , weights are automatically assigned to each image in the memory , without requiring to store all the augmented images , fully utilize the image annotations , assessing image quality , based on rare semantic class sampling and image diversity , food image classification , image classification , make the representations of dual view image pairs consistent , maximizes image diversity , constrain the relationship between input and synthetic images 

computational efficiency : suitable for long task sequences achieving strong performance with minimal or no additional cost , to advance compute efficient continual learning , achieving strong performance with minimal or no additional cost , alleviate the computational cost , avoid extra computational requirements , compute constrained , considering the computational cost and delay , do not use additional parameters or fix parameters , faster and lightweight continual learning , improve computational efficiency , Improve the performance without requiring additional storage , lightweight , lightweight network architecture , maintaining low computational overhead , minimal-prerequisite scenario , Balances the computational cost of the whole OCL training phase and the degradation of the images in memory , needs no extra memory , Bounded Resources , lightweight network architecture 

Class Incremental Learning : ONLINE CLASS-INCREMENTAL CONTINUAL LEARNING , online class-incremental CL setting , online class-incremental continual learning , online class-incremental setting , task-free online class incremental setup , assigns a semantic label to the class-agnostic instance , class-incremental learning , class-incremental setting , accommodate online new classes with minimal interference with learned knowledge , Accumulate the knowledge of previous classes , class incremental learning setting , class-agnostic , class-incremental learning (class-IL) , class-incremental learning tasks , maintain recognition ability for the old classes , online class incremental continual learning , decoupling the issue of incremental classification from the one of learning an overall transferable representation , incremental classification 

sample compression : Online Continual Compression , quick integration of information , sample compression , store and compress incoming memories , Summarize Stream Data , Summarizing model training , yielding high compression , better compression , compress dimensional vectors , compressed feature replay at intermediate layers , high compression ratio , stronger compression module based , Allows to compress multiple examples into a single one , compressed feature replay , high compression rates , learn to compress and store , maintain both summarized and original samples. , Compress the memory and achieve a better use of its limited size 

experience replay : Replay based , replay-based , replaying the edited examples , retrieve the interfered samples from the previous history for the function being learned , save the past data , self-supervised replay technique , complement to experience replay , consider limitations on the size of the replay buffer , efficiently re-use the training data , experience replay , jointly train all layers after the replay layer , multiple replay iterations , selecting suitable images from the buffer to replay during training , update a replay memory , Diversity-based update mechanism of the replay buffer , maintains a purified replay buffer , combine generic experience replay with additional constraint , Update the replay buffer 

Memory Limitation : under restricted memory size , storage constraints , trade-off in quality of information versus the amount of samples stored , Compress the memory and achieve a better use of its limited size , efficient use of limited model capacity , efficiently train the model with a limited memory , imposed memory constraints , memory constrained , Memory-Constrained Online Continual Learning (MC-OCL) , Static storage requirement , a fixed storage capacity , a limited buffer to store , constructing a fixed-size replay buffer , avoiding overwriting the previous information , Avoiding the need for removing data from the buffer , buffer can store more data for the same fixed amount of memory , consider limitations on the size of the replay buffer 

Memory management : avoid duplication , buffer deletion (and insertion) agnostic to the level of the sample , Buffering strategy , construct source buffer , different memory budgets , evaluate memory population , growth of buffer size further improve the performance of PCR , improve the informativeness of the auxiliary memory , integrate the information , Makes the external memory condensation more efficient and scalable to OCL , memory management , memory population approach , memory writing rule , controls what resolution the sample is stored , Small episodic memory , Balance diversity and purity in the episodic memory 

stability-plasticity trade-off : relieve the stability-plasticity dilemma , stability-plasticity trade-off , zero stability gap , achieve a better stability-plasticity trade-off , balance the stability and plasticity , balances the stability-plasticity , fight against the stability plasticity dilemma , make an tradeoff between the current knowledge and the previous knowledge to remove the imbalance between the current task and the previous task data , stability-plasticity trade-off online , prevent instabilities in the algorithms , plasticity , plasticity , stability , great balance between preventing catastrophic forgetting and facilitating learning , make a balance between stability and plasticity , make the right balance between plastic and stability in ocl 

Transfer learning : transfer training the whole network , transferring network weights without domain classification , better control over the negative transfer , explore the transferability of optimal learning conditions , explore transfer metalearning across modalities and datasets , forward transfer , Source Domain Pretraining , transfer knowledge , transfer of knowledge learned , transferring the knowledge between tasks , train a neural network on the source domain , communicate information forward , effective knowledge transfer , facilitating knowledge transfer , knowledge sharing across tasks , previous prediction of the model and transfer the past knowledge 

meta-learning : meta-learning fashion , based on consolidation in a meta-space , cast metalearning optimization , consolidation in a meta-space of model parameters , continual-meta learning , meta-consolidation phase , meta-distribution of model parameters , meta-learning , meta-learning perspective , explore transfer metalearning across modalities and datasets , relation between samples across tasks , Relationship matching , considerable level of generality , transfer metalearning 

single-pass learning : samples of data stream are seen only once , single data pass , the single-pass data stream fails to gradually update the prototypes and network parameters , without revisiting , a single epoch , considered each data sample only once , a single forward pass , a single pass through data , any hold-out data , dosent store previous data , one-epoch , one-pass stream of sample , online (one-epoch) task-incremental learning , good performance in the single pass setting 

Task-specific adaptation : task-aware inference , task-specific heads , learned task-specific priors , Modeling Task-specific Parameter Distributions , adaptation of new individual target , efficiently fine-tune , encourages fine-tuning , encourages the model to learn taskspecific information implicitly , focuses on the current task , learn a task-specific parameter distribution , task boundary detection , train task-specific classification models , Learn a discriminative classifier for each new task 

Concept drift adaptation : reducing representation drift of previously observed data , reduction in this representation drift , reduction Representation drift , robust to adaptation , suitable for handling concept drift problems in on-line environments , control drift , control the representation drift of old classes , cop with background concept drift , deal with the change of probability distribution , effective for representational drift , reflect the changes in data distribution , classifier cannot leverage the distribution shift , Representation drift in OCL 

incremental learning : training data available incrementally , incremental learning strategy , learned and consolidated incrementally , train neural networks incrementally , (online) incremental learning , data-incremental learning , incremental model updates , incrementally learning a deep segmentation network , intermediary classification , incremental learning environment , rehearsal-based incremental learning , careful update scheme allows for learning incrementally 

diversity preservation : Preserves a set of training examples that are diverse and pure , promote diversity , provides more advantageous and diverse cross-informatio , to address dilemma that selects the examples with low diversity , Trade-off between diversity and purity , Diversity-based update mechanism of the replay buffer , maximizing the diversity of samples , Balance diversity and purity in the episodic memory , based on rare semantic class sampling and image diversity , keeping diverse examples , maximizes image diversity 

No task boundaries : task boundaries are blurry or data are imbalanced , blurry task boundaries , task boundary detection , unknown task boundaries , learner does not have access to  the task identifier t at inference time , no assumption on task boundaries , no task boundary , no task or class boundaries , not define clear task boundaries , not rely on the oracle of task boundary , blurry task boundaries 

predictor : predict the activity labels provided in the initial data , predicted probability of the ground truth label , prediction , prediction on a small batch , previous prediction of the model and transfer the past knowledge , compute the current predictor , delivers the final predictions , imitate the predictions of the past model , learn the predictions of the previous model�s intermediate representations. , learns to scale the output feature maps predicted by the transformation function , schedule-robust predictor 

regularization : reduce the change in important weights , regularize the network parameter , regularize the parameter update , regularize the training loss , regularizes forgetting , restricting the gradient updates , avoids the high-loss ridge of past tasks , Constraint vs Regularization , construct extra constraints , control the strength of the regularization , minimize an entropy penalty 

feature replay : quantized latent rehearsal scheme , receives aggregated features , save more feature exemplars , allows to move the feature replay to lower layers , pretrained feature extractor , store feature embedding , compressed feature replay at intermediate layers , compressed feature replay , discriminator to trained to associate a data sample , Feature augmentation , maintaining the classification characteristics of the replayed features 

clustering based : clustering , Clustering , clustering approach , clustering the data for each class based on visual similarity , CNN-based , overlapping clusters of representations , clustering based exemplar selection algorithm , Exemplar Selection From Clusters , number of generated clusters are not set beforehand , based on cluster mean 

representation learning : store the most representative data , learning general representations , a better representation of the input samples where similar samples lie close , Dynamic representations , learn more generalised representations , learns more holistic/comprehensive features , obtain discrete latent representations , overlapping clusters of representations , Use the knowledge of task boundaries to select the representative examples , extract generalised useful features 

Adaptive learning : tune the learning rate as a function of time , adapting to the individual environment , fast adapt deep segmentation models , Online Adaptation , effectively learn new data , engaged in a continuous improvement process , fast adaptation , integrating both processing and learning into the networks , learns to scale the output feature maps predicted by the transformation function 

calibrating learned : optimization process to train the calibrated model , neuron calibration , task-agnostic calibrator , calibrated units under a general formulation , calibrating learned and online information , facilitate prototype refinement and calibration , feature calibration , Learning Calibration Parameters , rehearse for retaining and calibrating learned and online information 

rehearsal : performing rehearsal on the buffer , preserve the information of instances , rehearsal imbalance , rehearsal phase , rehearse for retaining , balanced rehearsal distribution , rehearse for retaining and calibrating learned and online information , rehearsal-based incremental learning , Rehearsal to mitigate forgetting in online case 

Domain Incremental Learning : domain incremental learning , domain incremental setting , Domain-Incremental setting , online continual learning on an unseen target domain , single-head classifier in the domain-incremental learning , domain adaptation , domain adaptation method , domain adaptation method 

ensemble methods : distribute training the individual modules in parallel , Voting layer , endto-end ensemble training , ensemble methods , ensembling multiple models at test time , fully differentiable ensemble method , models are ensembled , ensembling technique guided by data complexity 

handling noisy labels : outlier detection on a small batch , mitigate erroneous noisy label signals , Noise induced Amnesia , Filter clean data , learn more robust , Noisy Labeled Continual Learning , obtain the most clean samples , Reduce forgetting even with noisy labels 

pre-training  : pre-train phase , pre-train the meta-graph , pre-trained , pre-training strategies , pretrained feature extractor , Source Domain Pretraining , robust pretraining on a dedicated source dataset , discriminator to trained to associate a data sample 

on-device learning : on-device or real-world , practical continual learning , real-world applications , real-world domains , more realistic , EMBEDDED DEVICES , on-device learning 

feature alignment : align feature representations , align features of all tasks , constrain the relationship between input and synthetic images , feature calibration , generate task-invariant features , make the representations of dual view image pairs consistent 

few-shot learning : with limited training data , work well with limited data , low data regime , does not require pretraining , enhanced sample efficiency , model spatiotemporal context for online contextualized few-shot learning 

Non-stationary data streams : distributions might be non-stationary , non-stationary , Controllable non-stationarity , non-stationarity , non-stationary data streams , non-stationary distribution 

Prediction weighting strategy : prediction weighting strategy , The weights to decide the final prediction , maintain a cumulative moving average of the estimated importance weights , generate the class prediction scores , weights are automatically assigned to each image in the memory , calculate weights of the images in the buffer 

real-time learning : real-time , Real-time learning , real-time capable setting , real-time requirements , receives real-time feedback , be updated in realtime 

data augmentation : data augmentation on selected exemplar , data augmentation pipeline , knowledge distillation on augmented exemplars , without requiring to store all the augmented images , random transform , Feature augmentation 

hierarchical memory : MULTI-LEVEL STORAGE , adaptive multi-level storage mechanism , handles the information in a hierarchical manner with different stages of information , Hierarchical memory structures  , dual memory management strategy , MULTI-LEVEL RESERVOIR SAMPLING 

encoder-decoder architecture : quantize encoder outputs , shared encoder , backend to a deep encoder , encode and decode the data samples , probabilistic encoder and decoder 

Out-of-distribution (OOD) / In-distribution (ID) detection : out-of-distribution (OOD) domain , reflect the changes in data distribution , in-distribution (ID) domain , achieve correct outlier detection , memory examples remain in distribution 

self-supervised learning (SSL) : self-supervised learning(SSL) , self-supervised learning (SSL) , self-supervised replay technique , generate pseudo-labels , generate the task labels 

synaptic plasticity : parameterizes multiple synaptic plasticity mechanisms , regulates synaptic plasticity , synaptic regularization , controlling synaptic plasticity , incorporate synaptic plasticity mechanisms 

domain adaptation : online continual learning on an unseen target domain , domain adaptation , domain adaptation method , domain adaptation method , no assumption on the distribution 

complex data handling  : shuffled data , complex datasets , ensembling technique guided by data complexity , store an independent and identically distributed (iid) 

greedy-based algorithm : alternative greedy method based on heuristic , based on heuristic , greedy algorithm , greedy selection 

independent and identically distributed (IID) : store an independent and identically distributed(iid) , data arrives in an independent and identically distributed manner , independent and identically distributed(iid) , creating a buffer to store data with independent and identical distributions 

memory consolidation : expedited memory consolidation , introduces a simple mechanism for memory consolidation , offline consolidation , offline memory consolidation 

Modular network design : train parts of the model , finding better architecture for each instance , mechanism of selecting paths , training an intermediate classifier

Non-i.i.d : non i.i.d manner , non-i.i.d input distributions , Non-i.i.d settings

Privacy-preserving techniques : privacy preserving , Privacy-preserving , private data to learn 

